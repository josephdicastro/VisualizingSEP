Clinical research attempts to address a relatively straightforward,
and extremely important challenge: how do we determine whether one 
medical intervention is better than another, whether it offers greater clinical benefit and/or poses fewer risks? Clinicians may one day be
able to answer these questions by relying on computer models, thereby
avoiding reliance on clinical research and the ethical concerns it
raises. Until that day, clinical researchers begin by testing
potential new medical interventions in the laboratory, and often in
animals. While these methods can provide valuable information and, in
the case of animal research, raise important ethical issues of their
own, potential new interventions eventually must be tested in
humans. Potential new interventions which work miracles in test tubes
and in mice, often leave humans untouched, or worse off.
The human tests of a medical intervention typically pose some risks to subjects no matter how many laboratory and animal tests have preceded them. In this way, the process of collecting data to improve health and well-being exposes research subjects to risks for the benefit of future patients. These studies thus provide a clear
example of the central ethical challenge posed by clinical
research: When is it ethically permissible to expose research subjects to risks of harm for the benefit of others? The present entry focuses on this concern, and canvasses the
most prominent attempts to address it. The present entry largely
brackets the range of interesting and important ethical challenges that
arise in the course of conducting clinical research: How should it be
reviewed? Who may conduct it? What must potential subjects understand
to give valid consent? May it be conducted in countries that will not
be able to afford the intervention being tested? Do investigators have
any obligations to treat unrelated medical conditions they uncover in
the course of their research?
One might attempt to address the central ethical challenge
by limiting clinical research to the medical setting, offering
experimental interventions to patients who want to try them. This
approach, which has the virtue of evaluating interventions in the
process of trying to help individual patients, can make sense for comparisons of two or more interventions that are widely accepted and already in clinical use. In contrast, this approach poses enormous
scientific and practical problems with respect to testing new interventions. On the practical side, who would be
willing to manufacture a new intervention without knowing whether it
works? What dose should be used? How often should the new drug be
taken? More importantly, this approach might not yield reliable
information as to whether the new treatment is useful or harmful until
hundreds, perhaps thousands of people have received it. Clinical
research is designed to address these concerns by systematically
exposing a small number of individuals, including very sick ones, to
potential new treatments. Many activities, driving a car, smoking a
cigarette, flushing our waste down the drain, expose others to risks of
harm. Nonetheless, there has been surprisingly little philosophical
analysis of the conditions under which it is acceptable to do this
(Hayenhjelm and Wolff 2012). Therefore, in addition to being of value
in its own right, evaluation of the ethics of clinical research
provides an opportunity to consider one of the more fundamental
concerns in moral theory: when is it acceptable to expose some
individuals to risks of harm for the potential benefit of others?
 
1. What is Clinical Research?
Human subjects research is research which studies humans, as
opposed to animals, atoms, or asteroids. Assessment of whether humans prefer 100 dollars or a 1% chance of
10,000 dollars constitutes human subjects research. Clinical research
refers to the subset of human subjects research which focuses on interventions to
improve human health and well-being. The present analysis focuses on research that is designed to
improve human health and well-being by identifying better methods to
treat, cure or prevent illness. This focus on treating, curing and
preventing illness is intended to bracket the question of
whether research on enhancements qualifies as clinical research. Such
research has the potential to improve well-being, allowing us to remember more and worry less, without identifying methods to address illness.
We shall also bracket the question of whether quality improvement
and quality assurance projects qualify as clinical research. To
briefly consider the type of research at the heart of this debate,
consider a hospital which proposes to evaluate the impact of
checklists on the quality of patient care. Half the nurses in the
hospital are told to continue to provide care as usual; the other half
are provided with a checklist and instructed to mechanically check off
each item as they complete it when caring for their
patients. The question
of whether this activity constitutes clinical research is of
theoretical interest for clarifying the precise boundaries of the
concept. Should we say that this is not clinical research because the
checklist is used by the nurses, not administered to the patients? Or
should we say this is clinical research because it involves the
systematic testing of a hypothesis which is answered by collecting
data on patient outcomes? And this theoretical clarification has significant practical implications, determining whether these activities must satisfy existing
regulations for clinical research, including whether the clinicians
need to obtain patients’ informed consent to use the checklist.
While clinical medicine is enormously better than it was 100 or
even 50 years ago, there remain many diseases against which current
clinical medicine offers an inadequate response. To name just a few,
malaria kills over a million people, mostly children, every year;
chronic diseases, chief among them heart disease and stroke, kill
millions each year, and there currently are no effective treatments
for Alzheimer disease. The social value of clinical research lies in
its ability to collect information that might be useful to identifying
improved methods to treat these conditions.  Yet, it is the rare
clinical research study which definitively establishes that a
particular method is effective and safe for treating, curing or
preventing some illness. The success of specific research studies more
commonly lies in the gathering of information needed to inform future
studies.
Prior to establishing the efficacy of an experimental treatment for
a given condition, researchers typically need to identify the cause of
the condition, possible mechanisms for treating it, a safe and
effective dose, and ways of testing whether the drug is having an
effect on the disease.
The process of testing potential new treatments can take 10-15
years, and is standardly divided into phases. Formalized phase 0
studies are a relatively recent phenomenon involving the testing of
interventions and methods which might be used in later phase
studies. A phase 0 study might be designed to determine the mechanism
of action of a particular drug and evaluate different ways to
administer it. Phase 1 studies are the earliest tests of a new
intervention and are conducted in small numbers of individuals. Phase
1 studies are designed to evaluate the pharmacokinetics and
pharmacodynamics of new treatments, essentially evaluating how the
drug influences the human body and how the human body influences the
drug. Phase 1 studies also evaluate the risks of the treatment and
attempt to identify an appropriate dose to be used in subsequent phase
2 studies.  Phase 1 studies pose risks and frequently offer little if
any potential for clinical benefit to subjects. As a result, a
significant amount of the ethical concern over clinical research
focuses on phase 1 studies.
If phase 1 testing is successful, potential new treatments go on to
larger phase 2 studies which are designed to further assess risks and
also to evaluate whether there is any evidence that the treatment
might be beneficial. Successful phase 2 studies are followed by phase
3 studies which involve hundreds, sometimes thousands of
patients. Phase 3 studies are designed to provide a rigorous test of
the efficacy of a treatment and frequently involve randomization of
subjects to the new treatment or a control, which might be standard
existing treatment or a placebo. Finally, post-marketing or phase 4
studies evaluate the use of interventions in clinical practice.
Clinical trials of experimental treatments typically include purely
research procedures, such as blood draws, imaging scans, or biopsies,
that are performed to collect data regarding the treatment under
study.  Analysis of the ethics of clinical research thus requires
evaluation of three related risk-benefit profiles: (a) the
risk-benefit profile of the interventions(s) under
study; (b) the risk-benefit profile of the included research
procedures; and (c) the risk-benefit profile of the study as a
whole.
Potential new treatments sometimes are in the ex ante interests of
research subjects. For example, the risks posed by an experimental
cancer treatment might be justified by the possibility that it will
extend subjects’ lives. Moreover, the risk/benefit profile of the
treatment might be as favorable to subjects as the risk/benefit
profile of the available alternatives. In these cases, receipt of the
experimental intervention ex ante promotes subjects’ interests.  In
other cases, participation in research poses ‘net’ risks,
that is, risks of harm which are not, or not entirely, justified by
potential clinical benefits to individual subjects. Experimental
interventions sometimes pose net risks. A first in human trial of an
experimental treatment might involve a single dose to see whether
humans can tolerate it. And it might occur in healthy individuals who have no need of treatment. These studies pose risks to subjects and
offer essentially no chance for clinical benefit. The qualifier to ‘essentially’ no chance of clinical benefit is intended to capture the fact that the research procedures
included in clinical trials may inadvertently end up providing some clinical benefit to some subjects. For example, a biopsy that is used to collect research data may disclose a previously unidentified and treatable condition. The chance for such benefit, albeit real, is typically so
remote that it is not sufficient to compensate for the risks of the
procedure. Whether a study as a whole poses net risks depends on
whether the potential benefits of the experimental intervention
compensate for its risks plus the net risks of the research procedures
included in the study.
Clinical research which poses net risks raises important ethical
concern.  Net-risk studies raise concern that subjects are being used
as mere means to collect information to benefit future
patients. Research procedures that pose net risks may seem to raise
less concern when they are embedded within a study which offers a
favorable risk-benefit profile overall. Yet, since these procedures
pose net risks, and since the investigators could provide subjects
with the new potential treatment alone, they require justification. An
investigator who is about to insert a needle into a research subject
to obtain some blood purely for laboratory purposes faces the question
of whether doing so is ethically justified, even when the procedure is included in a study that offers subjects the potential for important medical benefit. The goal of ethical
analyses of clinical research is to provide an answer.  Clinical
research poses three types of net risks: absolute, relative, and
indirect (Rid and Wendler 2011). Absolute net risks arise when the
risks of an intervention or procedure are not justified by its
potential clinical benefits. Most commentators focus on this
possibility with respect to research procedures which pose some risks
and offer no chance of clinical benefit, such as blood draws to obtain
cells for laboratory studies.  Research with healthy volunteers is
another example which frequently offers no chance for clinical
benefit. Clinical research also poses absolute net risks when it
offers a chance for clinical benefit which is not sufficient to
justify the risks subjects face. A kidney biopsy to obtain tissue from
presumed healthy volunteers may offer some very low chance of
identifying an unrecognized and treatable pathology. This intervention
nonetheless poses net risks if the chance for clinical benefit for the subjects is not
sufficient to justify the risks of their undergoing the biopsy.
Relative net risks arise when the risks of a research intervention
are justified by its potential clinical benefits, but the
intervention’s risk-benefit profile is less favorable than the
risk-benefit profile of one or more available alternatives. Imagine
that investigators propose a randomized-controlled trial to compare an
inexpensive drug against an expensive and somewhat more effective
drug. Such trials make sense when, in the absence of a direct
comparison, it is unclear whether the increased effectiveness of the
more expensive drug justifies its costs. In this case, receipt of the
cheaper drug would be contrary to subjects’ interest in comparison to
receiving the more expensive drug. The trial thus poses relative net
risks to subjects.
Indirect net risks arise when a research intervention has a
favorable risk-benefit profile, but the intervention diminishes the
risk-benefit profile of other interventions provided as part of or in
parallel to the study. For example, an experimental drug for cancer
might undermine the effectiveness of other drugs individuals are
taking for their condition. The risks of research participation can be
compounded if the indicated response to the harm in question posse additional risks. Kidney damage suffered as the result of research participation might lead to the need for
short-term dialysis which poses additional risks to the individual; a subject who experiences a postlumbar
puncture headache might need a ‘blood patch’ which
poses some risk of blood entering the
epidural space which would call for a further response which brings with it additional risks. While commentators tend to focus on the risks of
physical harm, participation in clinical research can pose other
types of risks as well, including psychological, economic, and social
risks. Depending on the study and the circumstances, individuals who
are injured as the result of participating in research might incur
significant expenses. Most guidelines and regulations stipulate that
evaluation of the acceptability of clinical research studies should
take into account all the different risks to which subjects are
exposed.
To assess the ethics of exposing subjects to risks, one needs an account of
why exposing others to risks raises ethical concern in the first place.
Being exposed to risks obviously raises concern to the extent that the
potential harm to which the risk refers is realized: the chance of a
headache turns into an actual headache. Being exposed to risks also can
lead to negative consequences as a result of the recognition that one is at
risk of harm. Individuals who recognize that they face a risk may become
frightened; they also may take costly or burdensome measures to protect
themselves. In contrast, the literature on the ethics of clinical research
implicitly assumes that being exposed to risks is not itself harmful. The
mere fact that subjects are exposed to risks is not regarded as necessarily making them worse off
Increasingly, researchers are storing human biological samples and
using them in future research projects. These studies raise difficult
questions regarding the possibility of what might be called
‘contribution’ and ‘information’ risks. The
former question concerns the conditions under which it is acceptable
to ask individuals to contribute to answering the scientific question
posed by a given study (Jonas 1969). The frequent neglect of this
issue may trace to a narrow understanding of subjects’
interests. Individuals undoubtedly have an interest in avoiding the
kinds of physical harms they face in clinical research. It seems that
individuals’ interests also may be implicated, and possibly thwarted,
when they contribute to particular projects, activities and goals.
Imagine that an individual provides a blood sample which
investigators store and use in future research projects designed to
promote goals which conflict with the individual’s fundamental values. Can such research
harm the individual if they never learn about the results and are
never personally affected by them? Are the interests of an individual
who fundamentally opposes cloning, and constructs her life around
efforts to stop it, set back if she contributes to a research study
that identifies improved methods to clone human beings? With respect
to information risks, investigators used DNA samples obtained from
members of the Havasupai tribe to study “theories of the tribe’s
geographical origins.” The study’s conclusion that early members of
the tribe had migrated from Asia across the Bering Strait contradicted
the tribe’s own views that they originated in the Grand Canyon (Harmon
2010). Can learning the truth about the origins of one’s tribal group
harm members of the tribe?
Exposing research subjects to risks of harm is considered morally
problematic largely because it has the potential to result in their
being harmed. In addition, guidelines and regulations on clinical
research are replete with admonitions to expose subjects to risks only
when doing so is justified by the value of the study in question. This
focus reveals an important although typically implicit feature of most
analyses of the ethics of clinical research. It is often said that the
ethics of clinical research concerns the protection of research
subjects. One might conclude that exposing subjects to risks is
regarded as problematic only to the extent that it has the potential
to harm them. On this view, analysis of the appropriateness of
investigators exposing subjects to risks would be limited to the
possibility of making subjects worse off. In fact, while the
protection of research subjects is important, it does not exhaust the
ethics of clinical research. Guidelines and regulations also reflect
implicit principles regarding what constitutes appropriate
investigator behavior that are independent of the possibility of
making subjects worse off. Put generally, a full analysis of the ethics of exposing subjects to risks needs to justify both the treatment of the subjects and the behavior of the researchers.
The future oriented aspect of clinical research is worth
emphasizing. The fundamental ethical concern raised by clinical
research is whether and when it can be acceptable to expose some
individuals to risks and burdens for the benefit of others. In
general, the answer to this question depends crucially on the others
in question, and their relationship to those who are being exposed to
the risks. It is one thing to expose a consenting adult to risks to
save the health or life of an identified and present other,
particularly when the two individuals are first degree relatives. It
is another thing, or seems to many to be another thing, to expose
consenting individuals to risks to help unknown and unidentified, and
possibly future others. Almost no one objects to operating on a
healthy, consenting adult to obtain a kidney that might save an ailing
sibling, even though the operation poses some risk of serious harm and offers the donor no potential for clinical benefit. Greater
concern is raised by attempts to obtain a kidney from a healthy,
consenting adult and give it to an unidentified
individual. Commentators express even greater ethical concern as the
path from risk exposure to benefit becomes longer and more
tenuous. Many clinical research studies expose subjects to risks in
order to collect generalizable information which, if combined with the
results of other, as yet non-existent studies, may eventually benefit
future patients through the identification of a new intervention, assuming the appropriate regulatory authorities
approve it, some company or group chooses to manufacture it, and
patients can afford to purchase it. The potential benefits of clinical
research may thus be realized someday, but the risks and burdens are
clear and present.
Attempts to determine when it is acceptable to conduct clinical
research have been significantly influenced by its history, by how it
has been conducted and, in particular, by how it has been misconducted
(Lederer 1995; Beecher 1966). Thus, to understand the current state of
the ethics of clinical research, it is useful to know something of its
past.
2. Early Clinical Research


Modern clinical research may have begun on the 20th of May,
1747, aboard the HMS Salisbury. James Lind, the ship’s surgeon, was
concerned with the costs scurvy was exacting on British sailors, and
was skeptical of some of the interventions, cider, elixir of vitriol,
vinegar, sea-water, being used to treat it. Unlike other clinicians of his day, Lind did not simply assume that he was correct and treat his
patients accordingly. He designed a study to test whether he
was right. He chose 12 sailors from among the 30 or so Salisbury’s
crew members who were suffering from scurvy, and divided them into six
groups of 2 sailors each. Lind assigned a different intervention to
each of the groups, including two sailors turned research subjects who
received 2 oranges and 1 lemon each day. Within a week these two were
sailors again; the others remained patients, and several were dying.

The ethics of clinical research begins by asking how we should think
about the fate of these latter sailors. Do they have a moral claim
against Lind? Did Lind treat them appropriately? It is widely assumed
that physicians should do what they think is best for the patient in
front of them. Lind, despite being a physician, did not follow this
maxim. He felt strongly that giving sea water to individuals with
scurvy was a bad idea, but he gave sea water to 2 of the sailors in
his study to test whether he, or others, were right. To put the
fundamental concern raised by clinical research in its simplest form:
Did Lind sacrifice these two sailors, patients under his care, for the
benefit of others?


Lind’s experiments represent perhaps the first modern clinical trial
because he attempted to address one of the primary challenges facing
those who set out to evaluate medical treatments. How does one show
that any differences in the outcomes of the treatments under study are a result of
the treatments themselves, and not a result of the patients who
received them, or other differences in the patients’ environment or diet? How
could Lind be confident that the improvements in the two
sailors were the result of the oranges and lemons, and
not a result of the fact that he happened to give this particular
treatment to the two patients who occupied the most salutary rooms on the ship?
Lind tried to address the challenge of confounding variables by beginning with patients who
were as similar as possible. He carefully chose the 12 subjects for
his experiment from a much larger pool of ailing sailors; he also
tried to ensure that all 12 received the same rations each day, apart
from the treatments provided as part of his study. It is also worth
noting that Lind’s dramatic results were largely ignored for decades,
leading to uncounted and unnecessary deaths, and highlighting the
importance of combining clinical research with effective promulgation and 
implementation. The Royal Navy did not adopt citrus rations for another 50 years
(Sutton 2003), at which point scurvy essentially disappeared from the
Royal Navy.


Lind’s experiments, despite controlling for a number of factors, did
not exclude the possibility that his own choices of which sailors got
which treatment influenced the results. More recent experiments,
including the first modern randomized, placebo controlled trial of
Streptomycin for TB in 1948 (D’Arcy Hart 1999), attempt to address
this concern by assigning treatments to patients using a random
selection process. By randomly assigning patients to treatment groups
these studies ushered in the modern era of controlled, clinical
trials. And, by taking the choice of which treatment a given patient
receives out of the hands of the treating clinician, these trials
underscore and, some argue, exacerbate the ethical concerns raised by
clinical research (Hellman and Hellman 1991). A foundational principle
of clinical medicine is the importance of individual judgment. A
physician who decides which treatments her patients receive by
flipping a coin is guilty of malpractice. A clinical investigator who
relies on the same methods receives awards and gets published in elite journals. One might conclude that sacrifice of the interests of
some, often sick patients, for the benefit of future patients, is essentially mandated by the scientific method (Miller & Weijer 2006; Rothman
2000). The history of clinical research seems to provide tragic support for this view.
3. Abuses and Guidelines


The history of clinical research is littered with abuses. Indeed, one account maintains that the history of
pediatric research is “largely one of child abuse”
(Lederer and Grodin 1994, 19; also see Lederer 2003). This history has had a significant influence on how research ethicists understand
the concerns raised by clinical research and on how
policy makers attempt to address them. In particular, policy makers have responded to previous abuses by developing guidelines
intended to prevent their recurrence.


The most influential abuses in this regard were the horrific experiments conducted by Nazi physicians
during WW II (abuses perpetrated by Japanese physicians were equally horrific, but have received significantly less attention). Response to these abuses led to the Nuremberg Code (Grodin & Annas 1996;
Shuster 1997), which is frequently regarded as the
first set of formal guidelines for clinical research, an ironic claim
on two counts. First, there is some debate over whether the Nuremberg
Code was intended to apply generally to clinical research or whether,
as a legal ruling in a specific trial, it was intended to address only
the cases before the court (Katz 1996). Second, the Germans themselves had
developed systematic research guidelines as early as 1931 (Vollmann & Winau
1996). These guidelines were still legally in force at the time of the
Nazi atrocities and clearly prohibited a great deal of what the Nazi
doctors did.


In addition to being ignored by practicing researchers, wide consensus
developed by the end of the 1950s that the Nuremberg Code was
inadequate to the ethics of clinical research. Specifically, the
Nuremberg Code did not include a requirement that clinical research
receive independent ethics review and approval. In addition, the first
and longest principle in the Nuremberg Code states that informed
consent is “essential” to ethical clinical research
(Nuremberg Military Tribunal 1947). This requirement provides a
powerful safeguard against the abuse of research subjects. It also
appears to preclude clinical research with individuals who cannot
consent.


One could simply insist that the informed consent of subjects is necessary
to ethical clinical research and accept the opportunity costs thus
incurred. the impossibility of conducting clinical research on conditions unique to children or unconscious adults. Representatives of the World Medical Association, who hoped
to avoid these costs, began meeting in the early 1960s to develop
guidelines, which would become known as the Declaration of Helsinki,
to address the perceived shortcomings of the Nuremberg Code (Goodyear,
Krleza-Jeric, and Lemmens 2007). They recognized that insisting on
informed consent as a necessary condition for clinical research would
preclude a good deal of research designed to find better ways to treat
dementia and conditions affecting children, as well as research in
emergency situations. Regarding consent as necessary precludes such
research even when it poses only minimal risks or offers subjects a
compensating potential for important clinical benefit. The challenge,
still facing us today, is to identify protections for research
subjects which are sufficient to protect them without being so strict
as to preclude appropriate research designed to benefit the groups to
which they belong.


The Declaration of Helsinki (World Medical Organization 1996) allows
individuals who cannot consent to be enrolled in clinical research
based on the permission of the subject’s representative. The U.S.
federal regulations governing clinical research take a similar
approach.  These regulations are not laws in the strict sense of being
passed by Congress and applying to all research conducted on
U.S. soil. Instead, the regulations represent administrative laws
which effectively attach to clinical research at the beginning and the
end. Research conducted using U.S. federal monies, for instance,
research funded by the NIH, or research involving NIH researchers,
must follow the U.S. regulations (Department of Health and Human
Services 2005). Research that is included as part of an application for approval from the U.S. FDA
also must have been conducted according to FDA regulations which,
except for a few exceptions, are essentially the same. Although many
countries now have their own national regulations (Brody 1998), the
U.S. regulations continue to exert enormous influence around the world
because so much clinical research is conducted using U.S. federal
money and U.S. federal investigators, and the developers of medical
treatments often want to obtain approval for the U.S. market.


The abuses perpetrated as part of the infamous Tuskegee syphilis study
were made public in 1972, 40 years after the study was initiated. The
resulting outcry led to the formation of the U.S. National Commission,
which was charged with evaluating the ethics of clinical research with
humans and developing recommendations for appropriate
safeguards. These deliberations resulted in a series of
recommendations for the conduct of clinical research, which became the
framework for existing U.S. regulations. The U.S. regulations, like
many regulations, place no clear limits on the risks to which
competent and consenting adults may be exposed. In contrast, strict
limits are placed on the level of research risks to which those unable
to consent may be exposed, particularly children. In the case of
pediatric research, the standard process for review and approval is
limited to studies that offer a ‘prospect of direct’
benefit and research that poses minimal risk or a minor increase over
minimal risk. Studies that do not qualify in one of these categories
must be reviewed by an expert panel and approved by a high government
official. While this 4th
category offers important flexibility, it implies that, at least in principle, U.S. regulations do not
mandate a ceiling on the risks to which pediatric research subjects
may be exposed for the benefit of others. This reinforces the
importance of considering how we might justify exposing subjects to
research risks, both minimal and greater than minimal, for the benefit
of others. 
4. Clinical Research and Clinical Care


Several attempts have been made to justify exposing research subjects
to risks for the benefit of future patients. Lind’s experiments on
scurvy exemplify the fact that clinical research is often conducted by
clinicians and often is conducted on patients. Many commentators have
thus assumed that the ethics of clinical research should be governed
by the ethics of clinical care, and the methods of research should not
diverge from the methods that are acceptable in clinical care. On
this approach, subjects should not be denied any beneficial treatments
available in the clinical setting and they should not be exposed to
any risks not present in the clinical setting.


Some proponents (Rothman 2000) argue that this approach is implied by
the kind of treatment that patients, understood as individuals who
have a condition or illness needing treatment, are owed. Such
individuals are owed treatment that promotes, or at least is
consistent with their medical interests. Others (Miller & Weijer
2006) argue that the norms of clinical research derive largely from
the obligations that bear on clinicians. These commentators argue that
it is unacceptable for a physician to participate in, or even support
the participation of her patients in a clinical trial unless that
trial is consistent with the patients’ medical interests. To do less
is to provide substandard medical treatment and to violate one’s
obligations as a clinician.


The claim that the treatment of research subjects should be consistent
with the norms which govern clinical care has been applied most
prominently to the ethics of randomized clinical trials (Hellman &
Hellman 1991). Randomized trials determine which treatment a given
research subject receives based on a random process, not based on
clinical judgment of which treatment would be best for that
patient. Lind assigned the different existing treatments for scurvy to
the sailors in his study based not on what he thought was best for
them, but based on what he thought would yield an effective
comparative test. Lind did not give each intervention to the same
number of sailors because he thought that all the interventions had an
equal chance of being effective. To the contrary, he did this because
he was confident that several of the interventions were harmful and
this design was the best way to prove it. Contemporary clinical
researchers go even further, assigning subjects to treatments 
randomly. Because this aspect of
clinical research represents a clear departure from the practice of
clinical medicine it appears to sacrifice the interests of subjects in
order to collect valid data.


One of the most influential responses to this concern(Freedman 1987) argues that randomization is
acceptable when the study in question satisfies what has come to be
known as ‘clinical equipoise.’ Clinical equipoise obtains
when, for the population of patients from which subjects will be
selected, the available clinical evidence does not favor one of the
treatments being used over the others. In addition, it must be the
case that there are no treatments available outside the trial that are
better than those used in the trial. Satisfaction of these conditions
seems to imply that the interests of research subjects will not be
undermined in the service of collecting scientific information.  If
the available data do not favor any of the treatments being used,
randomizing subjects seems as good a process as any other for choosing
which treatment they receive.

Proponents of clinical equipoise as an ethical requirement for clinical research determine whether equipoise obtains not by appeal to the
belief states of individual clinicians, but based on whether there is
consensus among the community of experts regarding which treatment is
best. Lind believed that sea water was ineffective for the treatment
of scurvy. Yet, in the absence of agreement among the community of
experts, this view essentially constituted an individual preference
rather than a clinical norm. This suggests that it was acceptable
for Lind to randomly assign sailors under his care to the prevailing
treatments in order to test, in essence, whose preferred treatment was
the best. In this way, the existence of uncertainty within the
community of experts seems to offer a way to reconcile the methods of
clinical research with the norms of clinical medicine.


Critics respond that even when clinical equipoise obtains for the
population of patients, the specific circumstances of individual
patients within that population may imply that one of the treatments
under investigation is better for them (Gifford 2007). A specific
patient may have reduced liver function which places her at greater
risk of harm if she receives a treatment metabolized by the
liver. And some patients may have personal preferences which incline
them toward one treatment rather than another (e.g., they may prefer a
one-time riskier procedure to multiple, lower risk procedures which
pose the same collective risk). Current debate focuses on whether
randomized clinical trials can take these possibilities into account
in a way that is consistent with the norms of clinical medicine.


Even if the existence of clinical equipoise can justify some
randomized trials, a significant problem remains, namely, many studies and procedures which are
crucial to the identification and development of improved methods for
protecting and advancing health and well-being are 
inconsistent with subjects’ medical interests. This concern
arises for many phase 1 studies which offer essentially no chance for
medical benefit and pose at least some risks, and to that extent are
inconsistent with the subjects’ medical interests.


Phase 3 studies which randomize subjects to a potential new treatment
or existing standard treatment, and satisfy clinical equipoise,
typically include non-beneficial procedures, such as additional blood
draws, to evaluate the drugs being tested. Enrollment in these studies may be consistent with
subjects’ medical interests in the sense that the overall risk-benefit
ratio that the study offers is at least as favorable as the available
alternatives. Yet, evaluation of the overall risk-benefit profile of the study masks the
fact that it includes individual procedures which are contrary
to subjects’ medical interests, and contrary to the norms of clinical
medicine.

The attempt to protect research subjects by appeal to the obligations
clinicians have to promote the medical interests of their patients
also seems to leave healthy volunteers unprotected. Alternatively,
proponents might characterize this position in terms of clinicians’
obligations to others in general: clinicians should not perform
procedures on others unless doing so promotes the individual’s
clinical interests. This approach seems to preclude essentially all
research with healthy volunteers. For example, many phase 1 studies
are conducted in healthy volunteers to determine a safe dose of the
drug under study. These studies, vital to drug development, are
inconsistent with the principle that clinicians should expose
individuals to risks only when doing so is consistent with their
clinical interests. It follows that appeal to clinical equipoise alone
cannot render clinical research consistent with the norms of clinical
practice.


Commentators sometimes attempt to justify net-risk procedures that are
included within studies, and studies that overall pose net risks by
distinguishing between ‘therapeutic’ and
‘non-therapeutic’ research (Miller and Weijer 2006). The claim is that the demand
of consistency with subjects’ medical interests applies only to
therapeutic research; non-therapeutic research studies and procedures
may diverge from these norms to a certain extent, provided subjects’
medical interests are not significantly compromised. The distinction
between therapeutic and non-therapeutic research is sometimes based on
the design of the studies in question, and sometimes based on the intentions of
the investigators. Studies designed to benefit subjects, or
investigators who intend to benefit subjects are conducting
therapeutic studies. Those designed to collect generalizable knowledge
or in which the investigators intend to do so constitute
non-therapeutic research.


The problem with the distinction between therapeutic and
non-therapeutic research so defined is that research itself often is
defined as a practice designed to collect generalizable knowledge and
conducted by investigators who intend to achieve this end (Levine
1988). On this definition, all research qualifies as
non-therapeutic. Conversely, most investigators intend to benefit
their subjects in some way. Perhaps they design the study in a way
that provides subjects with clinically useful findings, or they
provide minor care not required for research purposes, or referrals to
colleagues. Even if one can make good on the distinction between
therapeutic and non-therapeutic research in theory, these practices
appear to render it irrelevant to the practice of clinical
research. More importantly, it is not clear why investigators’
responsibilities to patients, or patients’ claims on investigators,
should vary as a function of this distinction. Why think
that investigators are allowed to expose patients to some risks for
the benefit of others, but only in the context of research that is not
designed to benefit the subjects? To apply this proposed resolution to pediatric
research, why might it be acceptable to
expose infants to risks for the benefit of others, but only in the
context of studies which offer the infants no chance for personal
benefit?


To take one possibility, it is not clear that this view can be
defended by appeal to physicians’ role responsibilities. A prima facie
plausible view holds that physicians’ role responsibilities apply to
all encounters between physicians and patients who need medical
treatment. This view would imply that physicians may not compromise
patients’ medical interests when conducting therapeutic studies, but
also seems to prohibit non-therapeutic research procedures with
patients. Alternatively, one might argue that physicians’ role
responsibilities apply only in the context of clinical care and so do
not apply in the context of clinical research at all. This
articulation yields a more plausible view, but does not support the
use of the therapeutic/ non-therapeutic distinction. It provides no
reason to think that physicians’ obligations differ based on the type
of research in question.

Recent critics argue that these problems highlight the fundamental
confusion that results when one attempts to evaluate clinical research
based on norms appropriate to clinical medicine. They instead
distinguish between the ethics of clinical research and the ethics of
clinical care, arguing that it is inappropriate to assume that
investigators are subject to the claims and obligations which apply to
physicians, despite the fact that the individuals who conduct clinical
research often are physicians (Miller and Brody 2007).


The claim that clinical research should satisfy the norms of clinical
medicine has this strong virtue: it provides a clear method to
protect individual research subjects and reassure the public that they
are being so protected. If research subjects must be treated
consistent with their medical interests, we can be reasonably
confident that improvements in clinical medicine will not be won at
the expense of exploiting them. Most accounts of the ethics of
clinical research now recognize the limitations of this approach and
struggle to find ways to ensure that research subjects are not exposed
to excessive risks without assuming that the claims of clinical
medicine apply to clinical researchers (Emanuel, Wendler, and Grady
2000; CIOMS 2002). Dismissal of the distinction between therapeutic
and non-therapeutic research thus yields an increase in both
conceptual clarity and concern regarding the potential for
abuse of research subjects.


Clinicians, first trained as physicians taught to act in the best
interests of the patient in front of them, often struggle with the
process of exposing some patients to risky procedures for the benefit
of others. It is one thing for philosophers to insist, no matter how
accurately, that research subjects are not patients and need not be
treated according to the norms of clinical medicine. It is another
thing for clinical researchers to regard research subjects who are
suffering from disease and illness as anything other than
patients. These clinical instincts, while understandable and laudable,
have the potential to obscure the true nature of clinical research, as
investigators and subjects alike try to convince themselves that
clinical research involves nothing more than the provision of clinical
care. One way to try to address this collective and often willful
confusion would be to identify a justification for exposing research
subjects to net risks for the benefit of others. 
5. A Libertarian Analysis


It is often said that those working in bioethics are obsessed with the
principle of respect for individual autonomy. Advocates of this view of bioethicists
cite the high esteem accorded in the field to the requirement of obtaining
individual informed consent and the frequent attempts to resolve
bioethical challenges by citing its satisfaction. One might assume
that this view within bioethics traces to implicit endorsement of a libertarian analysis according to
which it is permissible for competent and informed individuals to do
whatever they prefer, provided those with whom they interact are
competent, informed and in agreement. In the words of Mill,
investigators should be permitted to conduct research and expose
subjects to risks provided they obtain subjects’ “free,
voluntary, and undeceived consent and participation” (On
Liberty, page 11). Setting aside the question of whether this view accurately characterizes bioethics and bioethicists generally, it does not apply to the vast majority of work done on the
ethics of clinical research. Almost no one in the field
argues that it is permissible for investigators to conduct any
research they want provided they obtain the free and informed consent
of the subjects they enroll.


Current research ethics does place significant weight on informed
consent and many regulations and guidelines devote much of their
length to articulating the requirement for informed consent. Yet, as
exemplified by the response to the Nuremberg Code, almost no one
regards informed consent as necessary and sufficient for ethical
research.  Most regulations and guidelines, beginning with the 
Declaration of Helsinki, first adopted in 1964 (World Medical Organization 1996), allow
investigators to conduct research on human subjects only when it has
been approved by an independent group charged with ensuring that the
study is ethically acceptable. Most regulations further place
limitations on the types of research that independent ethics
committees may approve. They must find that the research has important
social value and the risks have been minimized before approving it,
thereby restricting the types of research to which even competent
adults may consent. Are these requirements justified, or are they
inappropriate infringements on the free actions of competent
individuals?  The importance of answering this question goes beyond
its relevance to debates over Libertarianism. Presumably, the
requirements placed on clinical research have the effect of reducing
to some extent the number of research studies that get conducted. The
fact that at least some of the prohibited studies likely would have
important social value, helping to identify better ways to promote
health and well-being, provides a normative reason to eliminate the
restrictions, unless there is some compelling reason to retain
them.


One might regard the limitations as betraying the paternalism embedded
in most approaches to the ethics of clinical research (Miller and Wertheimer 2007). Although the
charge of paternalism often carries with it some degree of
condemnation, there is a strong history of what is regarded as
appropriate paternalism in the context of clinical research. This too
may have evolved from clinical medicine. Clinicians are charged with
protecting and promoting the interests of the patient “in front of
them”. Clinician researchers, who frequently begin their careers as
clinicians, may regard themselves as similarly charged. However, if we
accept the thesis that clinical research is normatively distinct from
clinical care, we need some reason to think that the norms for
clinical care are relevant to clinical research. The fact that these
restrictions on the options available to competent adults in the
context of clinical research trace to its close relationship with
clinical care does not constitute a justification for applying the
restrictions to this normatively distinct context. As noted, this is
especially important given that the restrictions at least sometimes
block otherwise socially valuable research.


The libertarian claim that valid informed consent is necessary and
sufficient to justify exposing research subjects to risks for the
benefit of others seems to imply, consistent with the first principle
of the Nuremberg Code, that research with individuals who cannot
consent is unethical. This plausible and tempting claim commits one to
the view that research with children, research in many emergency
situations, and research with the demented elderly all are ethically
impermissible. One could consistently maintain such a view but the
social costs of adopting it would be great. It is estimated, for
example, that approximately 70% of medications provided to children
have not been tested in children, even for basic safety and efficacy
(Roberts, Rodriquez, Murphy, Crescenzi 2003; Field & Behrman 2004;
Caldwell, Murphy, Butow, and Craig 2004). Absent clinical research with
children, pediatricians will be forced to continue to provide
sometimes inappropriate treatment, leading to significant harms that
could have been avoided by pursuing clinical research to identify
better approaches.


One response would be to argue that the Libertarian analysis is not
intended as an analysis of the conditions under which clinical
research is acceptable. Instead, the claim might be that it provides
an analysis of the conditions under which it is acceptable to conduct
clinical research with competent adults. Informed consent is necessary
and sufficient for enrolling competent adults in research. While this
view does not imply that research with subjects who cannot consent is
impermissible, it faces the not insignificant challenge of providing
an account for why such research might be acceptable.

Bracketing the question of individuals who cannot consent, many of the
limitations on clinical research apply to research with competent
adults. How might these limitations be justified? One approach would be to essentially grant the Libertarian analysis on theoretical
grounds, but then argue that the conditions for its implementation are
rarely realized in practice. In particular, there are good reasons,
and significant empirical data, to question how often clinical
research actually involves subjects who are sufficiently informed to
provide valid consent. Even otherwise competent adults often fail to
understand clinical research sufficiently to make their own informed
decisions regarding whether to enroll (Flory and Emanuel 2004).

To consider an example which is much discussed in the research ethics
literature, it is commonly assumed that valid consent for randomized
clinical trials requires individuals to understand randomization. It
requires individuals to understand that the treatment they will
receive, if they enroll in the study, will be determined by a process
which does not take into account which of the treatments is better for
them (Kupst 2003). There is an impressive wealth of data which
suggests that many, perhaps most individuals who participate in
clinical research do not understand this (Snowden 1997; Featherstone
and Donovan 2002; Appelbaum 2004). The data also suggest that these
failures of understanding often are resistant to educational
interventions.


It is sometimes argued that the restrictions placed on clinical
research studies, such as the requirements for independent review and
minimizing risks, are justified on the grounds of soft
paternalism. Paternalism involves interfering with the liberty of
agents for their own benefit (Feinberg 1986; see also entry
on paternalism). As the terms are used
in the present debate, ‘soft’ paternalism involves
interfering with the liberty of an individual in order to promote
their interests on the grounds that the action being interfered with
is the result of impaired decision-making: “A
freedom-restricting intervention is based on soft paternalism only
when the target’s decision-making is substantially impaired, when the
agent lacks (or we have reason to suspect that he lacks) the
information or capacity to protect his own interests—as
when A prevents B from drinking the liquid in a glass
because A knows it contains poison but B does not”
(Miller & Wertheimer 2007). ‘Hard’ paternalism, in
contrast, involves interfering with the liberty of an individual in
order to promote their interests, despite the fact that the action
being interfered with is the result of an informed and voluntary
choice by a competent individual.


If the myriad restrictions on clinical research were justified on the
basis of hard paternalism they would represent restrictions on
individuals’ autonomous actions. However, the data on the extent to
which otherwise competent adults fail to understand what they need to
understand to provide valid consent suggests that the limitations can
instead be justified on the grounds of soft paternalism. This suggests that while
the restrictions may limit the liberty of adult research subjects, they
do not limit their autonomy. In this way, one may regard
many of the regulations on clinical research not as inconsistent with
the libertarian ideal, but instead as starting from that ideal and
recognizing that otherwise competent adults often fail to attain
it.


Even if most research participants have sufficient understanding to provide valid consent, it would not
follow that there should be no limitations on
research with competent adults. The conditions on what one individual may do to another
are not exhausted by what the second individual consents to.  Perhaps
some individuals may choose for themselves to be treated with a lack
of respect, even tortured. It does not follow that it is acceptable
for me or you to treat them accordingly. As independent moral agents
we need sufficient reason to believe that our actions, especially the
ways in which we treat others, are appropriate, and this evaluation
concerns, in typical cases, more than just the fact that the affected
individuals consented to them.


Understood in this way, many of the limitations on the kinds of
research to which competent adults may consent are not justified, or
at least not solely justified, on paternalistic grounds. Instead,
these limitations point to a crucial and often overlooked concern in
research ethics. The regulations for clinical research often are
characterized as protecting the subjects of research from
harm. Although this undoubtedly is an important and perhaps primary
function of the regulations, they also have an important role in
limiting the extent to which investigators harm research subjects, and
limiting the extent to which society supports and benefits from a
process which inappropriately harms others. It is not just that research subjects
should not be exposed to risk of harm without compelling
reason. Investigators should not expose them to such risks without
compelling reason, and society should not support and benefit from their doing so.


This aspect of the ethics of clinical research has strong connections
with the view that the obligations of clinicians restrict what sort of
clinical research they may conduct. On that view, it is the fact that
one is a physician and is obligated to promote the best interests of
those with whom one interacts professionally which determines what one
is allowed to do to subjects. This connection highlights the pressing
questions that arise once we attempt to move beyond the view that
clinical research is subject to the norms of clinical medicine. There
is a certain plausibility to the claim that a researcher is not acting
as a clinician and so may not be subject to the obligations that bear
on clinicians. Or perhaps we might say that the researcher/subject
dyad is distinct from the physician/patient dyad and is not
necessarily subject to the same norms. But, once one concludes that we
need an account of the ethics of clinical research, distinct from
the ethics of clinical care, one is left with the question of which
limitations apply to what researchers may do to research subjects.


It seems clear that researchers may not expose research subjects to
risks without sufficient justification, and also clear that this claim
applies even to those who provide free and informed consent. The
current challenge then is to develop an analysis of the conditions
under which it is acceptable for investigators to expose subjects to
risks and determine to what extent current regulations need to be
modified to reflect this analysis. To consider briefly the extent of
this challenge, and to underscore and clarify the claim that the
ethics of clinical research go beyond the protection of research
subjects to include the independent consideration of what constitutes
appropriate behavior on the part of investigators, consider an
example.


Physical and emotional abuse cause enormous suffering, and a good deal
of research is designed to study various methods to reduce instances
of abuse and also to help victims recover from being abused. Imagine
that a team of investigators establishes a laboratory to promote the
latter line of research. The investigators will enroll consenting
adults and, to mimic the experience of extended periods of abuse in
real life, they will abuse their subjects emotionally and physically
for a week. The abused subjects will then be used in studies to
evaluate the efficacy of different methods for helping victims to cope
with the effects of abuse.
 

The proper response to this proposal is to point out that the fact the
subjects are competent and give informed consent does not establish
that it is ethically acceptable. One needs to consider many other
things. Is the experiment sufficiently similar to real life abuse that
its results will have external validity? Are there less risky ways to
obtain the same results? Finally, even if these questions are
answered in a way which supports the research, the question remains 
whether investigators may treat their subjects in this way. The
fact that essentially everyone working in research ethics would hold
that this study is unethical—investigators are not permitted to
treat subjects in this way—suggests that research ethics, both
in terms of how it is practiced and how it should be practiced, goes
beyond respect for individual autonomy to include independent
standards on investigator behavior. Defining those standards
represents one of the more important challenges for research
ethics.
 

As exemplified by Lind’s experiments on treatments for scurvy,
clinical research studies were first conducted by clinicians wondering
whether the methods they were using were effective. To answer this
question, the clinicians altered the ways in which they treated their
patients in order to yield information that would allow them to assess
their methods. In this way, clinical research studies intially were
part of, but an exception to standard clinical practice. As a result,
clinical research came to be seen as an essentially unique
activity. And widespread recognition of clinical research’s scandals
and abuses led to the view that this activity needed its own extensive
regulations.

More recently, some commentators have come to question the view that
clinical research is a unique human activity, as well as the
regulations and guidelines which result from this view. In particular,
it has been argued that this view has led to overly restrictive
requirements on clinical research, requirements that hinder
scientists’ ability to improve medical care for future patients, and
also fail to respect the liberty of potential research subjects. This
view is often described in terms of the claim that many regulations
and guidelines for clinical research are based on an unjustified
‘research exceptionalism’ (Wertheimer 2010).


The central ethical concern raised by clinical research involves the
practice of exposing subjects to risks for the benefit of others. Yet,
our everday activities frequently expose some to risks for the benefit
of others. When you drive to the store, you expose your neighbors to
some increased risk of pollution for the benefits you derive from
shopping; speeding ambulances expose pedestrians to risks for the
benefit of the patients they carry; factories expose their workers to
risks for the benefit of their customers; charities expose volunteers
to risks for the benefit of recipients. Despite this similarity,
non-beneficial clinical research is widely regarded as ethically
problematic and is subject to significantly greater regulation,
review, and oversight (Wilson and Hunter 2010). Almost no one regards
driving, ambulances, charities, or factories as inherently
problematic. Even those who are not great supporters of a given
charity do not argue that it treats its volunteers as guinea pigs. And
no one argues that charitable activities should satisfy the
requirements that are routinely applied to clinical research, such as
the requirements for independent review and written consent based on
an exhaustive description of the risks and potential benefits of the
activity, its purpose, duration, scope, and procedures.


Given that many activities expose some to risks for the benefit of
others, yet are not subject to such extensive regulation, some
commentators conclude that many of the requirements for clinical
research are unjustified (Sachs 2010, Stewart et al. 2008, and
Sullivan 2008). This work is based on the assumption that, when it
comes to regulation and ethical analysis, we should treat clinical
research the way we treat other activities in daily life which involve
exposing some to risks for the benefit of others. And this assumption
leads to a straightforward solution to our central ethical problem of
justifying the practice of exposing research subjects to risks for the
benefit of others.


Exposing factory workers to risks for the benefit of others is deemed ethically 
acceptable when they agree to do the work and are paid a fair
wage. The solution suggested for the ethical concern of non-beneficial
research is to obtain consent and pay research
subjects a sufficient wage for their efforts. This view is much less
restrictive than current regulations for clinical research, but seems
to be less permissive than a Libertarian analysis. The latter
difference is evident in claims that research studies should treat
subjects fairly and not exploit them, even if individuals consent to
being so treated.


The gap between this approach and the traditional view of research
ethics is evident in the fact that advocates of the traditional view
tend to regard payment of research subjects as exacerbating rather
than resolving its ethical concerns, raising, among others, worries of
undue inducement and commodification. Those who are concerned about
research exceptionalism, in contrast, tend to regard payment as it is
regarded in most other contexts in daily life: some is good and more
is better.


The claims of research exceptionalism have led to valuable discussion
of the extent to which clinical research differs from other activities
which pose risks to participants for the benefit of others and whether
any of the differences justify the extensive regulations and
guidelines standardly applied to clinical research. Proponents of
research exceptionalism who regard many of the existing regulations as
unjustified face the challenge of articulating an appropriate set of
regulations for clinical research. While comparisons to factory work
provide a useful lens for thinking about the ethics of clinical
research, it is not immediately obvious what positive recommendations
follow from this perspective. After all, it is not as if there is
general consensus regarding the regulations to which industry should
be subject. Some endorse minimum wage laws; others oppose them. There
are further arguments over whether workers should be able to unionize;
whether governments should set safety standards for industry; whether
there should be rules protecting workers against discrimination.
6. Contract Theory


A few commentators (Caplan 1984; Harris 2005; Heyd 1996) have
considered the possibility of justifying the exposure of 
subjects to risks for the benefit of others on the grounds that there
is an obligation to participate in clinical research. One might
try to ground this obligation in the fact that current individuals have
benefited from clinical research conducted on individuals in the past.
At least all individuals who have access to medical care have benefited
from the efforts of previous research subjects in the form of effective
vaccines and better medical treatment.


Current participation in clinical research typically benefits future
patients. However, if we incur an obligation for the benefits we have
received from previous research studies, we presumably are obligated
to the patients who participated in those studies, an obligation we
cannot discharge by participating in current studies. This approach
also does not provide a way to justify the very first clinical trials,
such as Lind’s, which of necessity enrolled subjects who had never
benefitted from previous clinical research.


Alternatively, one might argue that the obligation to participate does
not trace to benefits the individuals in fact received from the
efforts of previous research participants. Rather, the obligation is
to the overall social system of which clinical research is a part
(Brock 1994). For example, one might argue that individuals acquire
this obligation as the result of being raised in the context of a
cooperative scheme or society. We are obligated to do our part because
of the many benefits we have enjoyed as a result of being born within
such a scheme.


The first challenge for this view is to explain why the mere enjoyment
of benefits, without some prospective agreement to respond in kind,
obligates individuals to help others. Presumably, your doing a nice
thing for me yesterday, without my knowledge or invitation, does not
obligate me to do you a good turn today. This concern seems even
greater with respect to pediatric research. Children certainly benefit
from previous research studies, but typically do so unknowingly and
often with vigorous opposition. The example of pediatric research
makes the further point that justification of non-beneficial research
on straightforward contractualist grounds will be difficult at best.
Contract theories have difficulties with those groups, such as
children, who do not accept in any meaningful way the benefits of the
social system under which they live (Gauthier 1990).


In a Rawlsian vein, one might try to establish an obligation to
participate in non-beneficial research based on the choices
individuals would make regarding the structure of society from a
position of ignorance regarding their own place within that society,
from behind a veil of ignorance (Rawls 1999). To make this argument,
one would have to modify the Rawlsian argument in several
respects. The knowledge that one is currently living could well bias
one’s decision against the conduct of clinical research. Those who
know they are alive at the time the decision is being made have
already reaped many of the benefits they will receive from the conduct
of clinical research.


To avoid these biases, we might stretch the veil of ignorance to
obscure the generation to which one belongs—past, present or
future (Brock 1994). Under a veil of ignorance so stretched,
individuals might choose to participate in clinical research,
including non-beneficial research as long as the benefits of the
practice exceed its overall burdens. One could then argue that justice
as fairness gives all individuals an obligation to participate in
clinical research when their turn comes.  This approach seems to have
the advantage of explaining why we can expose even children to some
risks for the benefit of others, and why parents can give permission
for their children to participate in such research. This argument also
seems to imply not simply that clinical research is acceptable, but
that, in a range of cases, individuals have an obligation to participate in it. It implies
that adults whose turn has come are obligated to participate in clinical research,
although for practical reasons we might refrain from forcing them to
do so.


This justification for clinical research faces several challenges. First,
Rawlsian arguments typically are used to determine the basic structure
of society, that is, to determine a fair arrangement of the basic
institutions within the society (Rawls 1999). If the structure
of society meets these basic conditions, members of the society cannot
argue that the resulting distribution of benefits and burdens is
unfair. Yet, even when the structure of society meets the conditions
for fairness, it does not follow that individuals are obligated to
participate in the society so structured. Competent adults can decide
to leave a society that meets these conditions rather than enjoy its benefits (whether they have any
better places to go is another question). The right of exit suggests
that the fairness of the system does not generate an obligation to
participate, but rather defends the system against those
who would argue that it is unfair to some of the participants over
others. At most, then, the present argument can show that it is not
unfair to enroll a given individual in a research study, that this is
a reasonable thing for all individuals, including those who are unable
to consent.


Second, it is important to ask on what grounds individuals behind
the veil of ignorance make their decisions. In particular: are these
decisions constrained or guided by moral considerations? (Dworkin 1989;
Stark 2000). It seems plausible to think that they would
be. After all, we are seeking the
ethical approach or policy with respect to clinical research. The
problem, then, is that the answer we get in this case may depend
significantly on which ethical constraints are built into the system,
rendering the approach question begging. Most importantly, we are considering whether it is ethical to expose subjects who cannot consent to risks for the benefit of others. If it isn’t, then it seems that this should be a limitation on the choices individuals can make from behind the veil of ignorance, in which case appeal to those choices will not be able to justify non-beneficial pediatric research, nor non-beneficial
research with incompetent adults. And if this research is ethical it is unclear why we need this mechanism to justify it. 


Proponents might avoid this dilemma by assuming that individuals
behind the veil of ignorance will make decisions based purely on
self-interest, unconstrained by moral limits or
considerations. Presumably, many different systems would satisfy this
requirement. In particular, the system that produces the greatest
amount of benefits overall may well be one that we regard as
unethical. Many endorse the view that clinical research studies which
offer no potential benefit to subjects and pose a high chance of
serious risk, such as death, are unethical, independent of the
magnitude of the social value to be gained. For example, almost all research ethicists would
regard as unethical a study which intentionally infects a few subjects
with the HIV virus, even when the study offers the potential to identify
a cure for AIDS. Yet, individuals behind the veil of ignorance who
make decisions based solely on self-interest might well allow this
study on the grounds that it offers a positive cost-benefit ratio
overall: the high risks to a few subjects are clearly outweighed by
the potential to save the lives of millions.


The question here is not whether a
reasonable person would choose to make the poor even worse off in
order to elevate the status of those more privileged. Rather, both
options involve some individuals being in unfortunate circumstances,
namely, infected with the HIV virus. The difference is that the one
option (not conducting the study) involves many more individuals
becoming infected over time, whereas the other option involves
significantly fewer individuals being infected, but some as the result
of being injected in the process of identifying an effective
vaccine. Since the least desirable circumstances (being infected with
HIV) are the same in both cases, the reasonable choice, even if one
endorses the maximin strategy, seems to be whichever option reduces
the total number of individuals who are in those circumstances,
revealing that, in the present case at least, the Rawlsian approach
seems not to take into account the way in which individuals end up in
the positions they occupy.
7. Minimal Risks


Limits on risks are a central part of almost all
current research regulations and guidelines. With respect to those who can
consent, there is an essentially implicit agreement that the risks
should not be too high in the context of non-beneficial research (as
noted some argue that there should not be any net risks to even
competent adults in the context of so-called therapeutic
research). However, there is no consensus regarding how to determine
which risks are acceptable in this context. With respect to those who cannot consent, many commentators argue that
non-beneficial research is acceptable provided that the net risks are
very low. The challenge, currently faced by many in clinical research,
is to identify a standard, and find a reliable way to implement it,
for what constitutes a sufficiently low risk in this context. An
interesting and important question in this regard is whether the level
of acceptable risks varies depending on the particular class of
individuals who cannot consent. Is the level of acceptable risks the
same for individuals who were once competent, such as previously
competent adults with Alzheimer disease, individuals who are not now
but are expected to become competent, such as healthy children, and
individuals who are not now and likely never will be competent, such
as individuals born with severe cognitive disabilities?


Some argue that the risks of clinical research qualify as sufficiently
low when they are ‘negligible’, understood as risks that
do not pose any chance of serious harm (Nicholson 1986). Researchers
who ask children a few questions for research purposes may expose them
to risks no more worrisome than that of being mildly upset for a few
minutes. It seems not implausible that exposing subjects to a risk of minor harm for the benefit of others does not raise ethical concern. Or one might argue that the ethical concerns it raises do not merit serious ethical concern. Despite
the plausibility of these views, very few studies
satisfy the negligible risk standard. Even routine procedures that are
widely accepted in pediatric research, such as single blood draws,
pose some, typically very low risk of more than negligible harm.  


Others (Kopelman 2000; Resnik 2005) define risks as sufficiently low
or ‘minimal’ when they do not exceed the risks individuals
face during the performance of routine examinations. This standard
provides a clear and quantifiable threshold for acceptable risks. Yet, the risks of routine medical procedures for healthy
individuals are so low that this standard seems to prohibit intuitively acceptable research. This approach faces the additional
problem that, as the techniques of clinical medicine become safer and
less invasive, increasing numbers of procedures used in non-beneficial
research would be deemed excessively risky. And, at a theoretical
level, one might wonder why we should think that the risks we
currently happen to accept in the context of clinical care for healthy children should
define the level of risk that is acceptable in clinical research. Why think that the ethical acceptability of a non-beneficial blood draw in pediatric research depends on whether clinicians still use blood draws as part of clinical screening for healthy children?


Many guidelines (U.S. Department of Health and Human Services 2005;
Australian National Health and Medical Research Council 1999) and
commentators take the view that non-beneficial research is ethically
acceptable as long as the risks do not exceed the risks individuals face
in daily life. Many of those
involved in clinical research implicitly assume that this minimal risk
standard is essentially equivalent to the negligible risk standard. If
the risks of research are no greater than the risks individuals face
in daily life, then the research does not pose risk of any serious
harm. As an attitude toward many of the risks we face in daily life,
this view makes sense.  We could not get through our daily lives if we
were conscious of all the risks we face. Crossing the street poses
more risks than one can catalog, much less process readily. When these
risks are sufficiently low, psychologically healthy individuals place
them in the cognitive background, ignoring them unless the
circumstances provide reason for special concern (e.g.  one hears a
siren, or sees a large gap in the sidewalk).


Paul Ramsey reports that during the deliberations of the National
Commission on pediatric research, members often used the terms minimal
and negligible risks in a way which seemed to imply that they were
willing to allow minimal risk research, even with children, on the
grounds that it poses no chance of serious harm (Ramsey 1978).  The
members then went on to argue that an additional ethical requirement
for such research is a guarantee of compensation for any serious
research injuries. This approach to minimal risk pediatric research
highlights nicely the somewhat confused attitudes we often have toward
risks, especially those of daily life.


We go about our daily lives as though harms with very low probability are not going to
occur, effectively treating low probability harms as zero probability
events. To this extent, we are not Bayesians about the risks of daily
life. We treat some possible harms as impossible for the purposes of
getting through the day. This attitude, crucial to living our lives,
does not imply that there are no serious risks in daily life. The fact
that our attitude toward the risks of everyday life is justified by
its ability to help us to get through the day undermines its ability
to provide an ethical justification for exposing research subjects to
the same risks in the context of non-beneficial research (Ross &
Nelson 2006).


First, the extent to which we ignore the risks of daily life is not
a fully rational process. In many cases, our attitude regarding risks
is a function of features of the situation that are not correlated
directly with the risk level, such as our perceived level of control
and our familiarity with the activity (Tversky, Kahneman 1974; Tversky,
Kahneman 1981; Slovic 1987; Weinstein 1989). Second, to the extent that
the process of ignoring some risks is rational, we are involved in a
process of determining which risks are worth paying attention to.
Some risks are so low that they are not worth paying attention to.
Consideration of them would be more harmful (would cost us more)
than the expected value of being aware of them in the first place.


To some extent, then, our attitudes in this regard are based on a
rational cost/benefit analysis. To that extent, these attitudes do not
provide an ethical argument for exposing research subjects to risks
for the benefit of others. The fact that the costs to an individual of
paying attention to a given risk in daily life are greater than the
benefits to that individual does not seem to have any relevance for
what risks we may expose them to for the benefit of others. Finally,
there is a chance of serious harm from many of the activities of daily
life.  This reveals that the ‘risks of daily life’
standard does not preclude the chance of some subjects experiencing
serious harm.  Indeed, one could put the point in a much stronger
way. Probabilities being what they are, the risks of daily life
standard implies that if we conduct enough minimal risk research
eventually a few subjects will die and scores will suffer permanent
disability.


As suggested above, a more plausible line of argument would be to
defend clinical research that poses minimal risks on the grounds that
it does not increase the risks to which subjects are exposed.
It seems plausible to assume that at any given time an individual will
either be participating in research or involved in the activities of
daily life. But, by assumption, the risks of the two activities are
essentially equivalent, implying that enrollment in the study, as
opposed to allowing the subject to continue to participate in the
activities of daily life does not increase the risks to which he is
exposed.


The problem with this argument is that the risks of research
often are additive rather than substitutive. For example, participation in a study might require the subject to drive to the
clinic for a research visit. The present defense succeeds to the
extent that this trip replaces another trip in the car, or some
similarly risky activity in which the subject would have been
otherwise involved. In practice, this often is not the case. The
subject instead may simply put off the trip to the mall until
after the research visit. In that case, the subject’s risk of serious
injury from a car trip may be doubled as a result of her participation
in research. Moreover, we accept many risks in daily life because the
relevant activities offer those who pursue them a chance of personal
benefit. We allow children to take the bus because we assume that the
benefits of receiving an education justify the risks. The fact that we
accept these risks given the potential benefits provides no reason to
think that the same risks or even the same level of risk would be
acceptable in the context of an activity, including a non-beneficial
research study, which offers no chance of medical benefit. Finally,
and strictly speaking, this justification seems to imply that
investigators should evaluate what risks individuals would face if
they did not enroll in the research, and enroll only those who would
otherwise face similar or greater levels of risk.
8. Goals and Interests


In one of the most influential papers in the history of research
ethics, Hans Jonas (1969) argues that the progress clinical
research offers is normatively optional, whereas the need to protect
individuals from the harms to which clinical research exposes them is
mandatory. He writes:
… unless the present state is intolerable, the
melioristic goal [of biomedical research] is in a sense gratuitous,
and this is not only from the vantage point of the present. Our
descendants have a right to be left an unplundered planet; they do not
have a right to new miracle cures. We have sinned against them if by
our doing, we have destroyed their inheritance not if by the time they
come around arthritis has not yet been conquered (unless by sheer
neglect). (Jonas 1969, 230–231)


Jonas’s view does not imply that clinical research is necessarily
unethical, but the conditions on when it may be conducted are very
strict. This argument may seem plausible to the extent that one
regards, as Jonas does, the benefits of clinical research to be ones
that make an acceptable state in life even better. The example of
arthritis cited by Jonas characterizes this view. Curing arthritis,
like curing dyspepsia, baldness, and the minor aches and pains of
living and aging, may be nice, but may be thought to address no profound
problem in our lives. If this were all that clinical research had to
offer, we might be reluctant to accept many risks in order to
achieve its goals. We should not, in particular, take much chance of
wronging individuals, or exploiting them to realize these goals.


This argument makes sense to the extent that one regards the status
quo as acceptable. Yet, without further argument, it is not clear why
one should accept this view; it seems almost certain that those
suffering from serious illness that might be addressed by future
research will not accept it. Judgments regarding the present state of
society concern very general level considerations and a determination
that society overall is doing fairly well is consistent with many
individuals suffering terrible diseases. Presumably, the suffering of
these individuals provides some reason to conduct clinical
research. In response, one might understand Jonas to be arguing that
the present state of affairs involves sufficiently good medicine and
adequately flourishing lives such that the needs which could now be
addressed by additional clinical research are not of sufficient
importance to justify the risks raised by conducting it. It might have
been the case, at some point in the past, that life was sufficiently
nasty, brutish and short to justify running the risk of exploiting
research subjects in the process of identifying through clinical
research ways to improve the human lot. But, we have advanced, in part
thanks to the conduct of clinical research, well beyond that
point. This reading need not interpret Jonas as ignoring the fact that
there remain serious ills to be cured. Instead, he might be arguing
that these ills, while real and unfortunate, are not of sufficient
gravity, or perhaps prevalence to justify the risks of conducting
clinical research. 


This view implicitly expands the ethical concerns raised by clinical
research. We have been focusing on the importance of protecting
individual research subjects. However, Jonas assumes that clinical
research also threatens society in some sense. There are at least two
possibilities here. First, it might be thought that the conduct of
unethical research reaches beyond individual investigators to taint
society as a whole. This does not seem unreasonable given that
clinical research typically is conducted in the name of and often for
the benefit of society. Second, one might be concerned that allowing
investigators to expose research subjects to some risks for the
benefit of others might put us on a slippery slope that ends with
serious abuses throughout society.


An alternative reading would be to interpret Jonas as arguing from
a version of the active-passive distinction. It is often claimed that
there is a profound moral difference between actively causing harm
versus merely allowing harm to occur, between killing
someone versus allowing them to die, for example. Jonas often seems to
appeal to this distinction when evaluating the ethics of clinical
research. The idea is that conducting clinical research involves
investigators actively exposing individuals to risks of harm and, when
those harms are realized, it involves investigators actively harming
them. The investigator who injects a subject with an experimental
medication in the context of a non-beneficial study actively exposes
the individual to risks for the benefit of others and actively harms,
perhaps even kills those who suffer harm as a result. And, to the
extent that clinical research is conducted in the name of and for the
benefit of society in general, one can say without too much difficulty
that society is complicit in causing these harms. Not conducting clinical
research, in contrast, involves our allowing individuals to be subject
to diseases that we might otherwise have been able to avoid or cure.
And this situation, albeit tragic and unfortunate, has the virtue of
not involving clear moral wrongdoing.


The problem with at least this version of the argument is that the
benefits of clinical research often involve finding safer ways to
treat disease. The benefits of this type of clinical research, to the
extent they are realized, involve clinicians being able to provide
less harmful, less toxic medications to patients. Put differently,
many types of clinical research offer the potential to identify
medical treatments which harm patients less than current ones. This
not an idle goal. One study found that the incidence of serious
adverse events from the appropriate use of clinical medications
(i.e. excluding such things as errors in drug administration,
noncompliance, overdose, and drug abuse) in hospitalized patients was
6.7%. The same study, using data from 1994, concludes that the
approved and properly prescribed use of medications is likely the
5th leading cause of death in the US (Lazarou, Pomeranz,
and Corey 1998).


These data suggest that the normative calculus is significantly more
complicated than the present reading of Jonas suggests. The question
is not whether it is permissible to risk harming some individuals in
order to make other individuals slightly better off. Instead, we have
to decide how to trade off the possibility of clinicians exposing
patients to greater risks of harm (albeit with a still favorable risk-benefit ratio) in the process of treating them
versus clinical researchers exposing subjects to risk of harm in the
process of trying to identify improved methods to treat others. This
is not to say that there is no normative difference between these two
activities, only that that difference is not accurately described as
the difference between harming individuals versus improving their lot
beyond some already acceptable status quo. It is not even a difference
between harming some individuals versus allowing other individuals to
suffer harms. The argument that needs to be made is that harming
individuals in the process of conducting clinical research potentially
involves a significant moral wrong not present when clinicians harm
patients in the process of treating them.

The primary concern here is that, by exposing subjects to risks
of harm, the process of conducting clinical research involves the
threat of exploitation of a particular kind. It runs the risk of
investigators treating persons as things, devoid of any interests of
their own. The worry here is not so much that investigators and
subjects enter together into the shared activity of clinical research
with different, perhaps even conflicting goals. The concern is rather
that, in the process of conducting clinical research, investigators
treat subjects as if they had no goals at all or, perhaps, that any
goals they might have are normatively irrelevant.


Jonas argues that this concern can be addressed, and the process of
experimenting on some to benefit others made ethically acceptable,
only when the research subjects share the goals of the research
study. Ethically appropriate research, on Jonas’s
view, is marked by: “appropriation of the research purpose into
the person’s own scheme of ends” (Jonas 1969, 236). And assuming
that it is in one’s interests to achieve one’s, at least, proper
goals, it follows that, by participating in research, subjects will be
acting in their own interests, despite the fact that they are thereby
being exposed to risky procedures which are performed to collect
information to benefit others.


Jonas claims in some passages that research subjects, at least those
with an illness, can share the goals of a clinical research study only
when they have the condition or illness under study (Jonas
1969). These passages reveal something of the account of human
interests on which Jonas’s arguments rely. On standard preference
satisfaction accounts of human interests, what is in a given
individual’s interests depends on what the individual happens to want
or prefer, or the goals the individual happens to endorse, or the
goals the individual would endorse in some idealized state scrubbed
clean of the delusions, misconceptions and confusion which inform our
actual preferences (Griffin 1986). On this view, participation in
clinical research would promote an individual’s interests as long as
she was well informed and wanted to participate. This would be so
whether or not she had the condition being studied. Jonas’s view, in
contrast, seems to be that there are objective conditions under which
individuals can share the goals of a given research study. They can
endorse the cause of curing or at least finding treatments for
Alzheimer disease only if they suffer from the disease
themselves. 


One possible objection would be to argue that there are many reasons
why an individual might endorse the goals of a given study, apart from
the fact of having the disease themselves. One might have family
members with the disease, or co-religionists, or have adopted improved
treatment of the disease as an important personal goal.

The larger question is whether subjects endorsing the goals of a
clinical research study is a necessary condition on its
acceptability. Recent commentators and guidelines rarely, if ever,
adopt this condition, although at least some of them might be assuming
that the requirement to obtain free and informed consent will ensure its
satisfaction. It might be assumed, that is, that competent, informed, and free
individuals will enroll in research only when they share the goals of
the study in question.


Jonas was cognizant of the extent to which the normative concerns
raised by clinical research are not exhausted by the risks to which
subjects are exposed, but also include the extent to which
investigators and by implication society are the agents of their 
exposure to risks. For this reason, he recognized that the libertarian response
is inadequate, even with respect to competent adults who truly
understand. Finally, to the extent Jonas’s claims rely on an objective
account of human interests, one may wonder whether he adopts an overly
restrictive one. Why should we think, on an objective account, that
individuals will have an interest in contributing to the goals of a
given study only when they have the disease it addresses? Moreover,
although we will not pursue the point here, appeal to an objective
account of human interests raises the possibility of justifying the
process of exposing research subjects to risks for the benefit of
others on the grounds that contributing to valuable projects,
including presumably some clinical research studies, is objectively in
(most) individuals’ interests (Wendler 2010).
9. Industry Sponsored Research


The fundamental ethical challenge posed by clinical research is
whether it is acceptable to expose some to research risks for the
benefit of others. In the standard formulation, the one we have been
considering to this point, the benefits that others enjoy as the
result of subjects’ participation in clinical research are medical and
health benefits, better treatments for disease, better methods to
prevent disease.


Industry funded research introduces the potential for a very different
sort of benefit and thereby potentially alters, in a fundamental way,
the moral concerns raised by clinical research.  Pharmaceutical
companies typically focus on generating profit and increasing stock
price and market share. Indeed, it is sometimes argued that
corporations have an obligation to their shareholders to pursue
increased market share and share price (Friedman 1970). This approach
may well lead companies to pursue new medical treatments which have
little or no potential to improve overall health and well-being
(Huskamp 2006; Croghan and Pittman 2004).  “Me-too” drugs
are the classic example here. These are drugs identical in all
clinically relevant respects to approved drugs already in use. The
development of a me-too drug offers the potential to redistribute
market share without increasing overall health and well-being.


There is considerable debate regarding how many me-too drugs there
really are and what is required for a drug to qualify as effectively
identical (Garattini 1997). For example, if the existing treatment needs to be
taken with meals, but a new treatment need not, is that a clinically
relevant advance? Bracketing these questions, a drug company may well
be interested in a drug which clearly qualifies as a me-too drug. The
company may be able, by relying on a savvy marketing department, to
convince physicians to prescribe, and consumers to request the new
one, thus increasing profit for the company without advancing health
and well-being.


The majority of clinical research was once conducted by governmental
agencies. For example, the US NIH is likely the largest governmental sponsor of clinical research in the world. However, its research budget has declined over the past 20 years (Mervis 2004, 2008), and it is estimated that a majority,
perhaps a significant majority of clinical research studies are
now conducted by industry: “as recently as 1991 eighty per cent of
industry-sponsored trials were conducted in academic health
centers…Impatient with the slow pace of academic bureaucracies,
pharmaceutical companies have moved trials to the private sector,
where more than seventy per cent of them are now conducted”
(Elliott 2008, Angell 2008, Miller and Brody 2005).


In addition to transforming the fundamental ethical challenge posed by
clinical research, industry sponsored research has the potential to
transform the way that many of the specific ethical concerns are
addressed within that context.  For example, the possibility that investigators and
funders may earn significant amounts of money from their participation
in clinical research might, it is thought, warp their judgment in ways
that conflict with appropriate protection of research subjects
(Fontanarosa, Flanagin, and DeAngelis 2005). When applied to
investigators and funders this concern calls into question the very
significant percentage of research funded by and often conducted by
for-profit organizations. Skeptics might wonder whether the goal of
making money has any greater potential to influence judgment
inappropriately compared to many other motivations that are widely
accepted, even esteemed in the context of clinical research, such as
gaining tenure and fame, impressing one’s colleagues, or winning the
Nobel Prize.


Financial conflicts of interest in clinical research point to a
tension between relying on profits to motivate research versus
insulating drug development and testing from the profit motive as a
way of protecting research subjects and future patients (Psaty and
Kronmal 2008). Finally, if industry can make billions of dollars from the development
of a single drug one wonders what constitutes an appropriate response
to the subjects who were vital to the development of the drug in
question. On a standard definition, whether a given transaction is fair depends on the risks and burdens that each party to the transaction bears and the extent to which others benefit from the party’s
participation in the transaction (see entry on
 exploitation). A series of clinical
research studies can result in a company earning tens of billions of dollars
in profits. Recognizing that a fair level of benefit is a complex
function of participants’ inputs compared to the inputs of others, and
the extent to which third parties benefit from those inputs, it is
difficult to see how one might fill in the details of this scenario to
show that the typically minimal, or non-existent compensation offered
to research participants is fair.


At the same time, addressing this potential for exploitation by
offering substantial payments to research participants who contribute to especially lucrative studies would introduce its own set
of ethical concerns: is payment an appropriate response to the kind of
contribution made by research participants; might payment constitute
an undue inducement to participate; will payment undermine other
participants’ altruistic motivations; to what extent does payment
encourage research subject to provide misleading or false information
to investigators in order to enroll and remain in research studies? In
the end, then, as commentators struggle to address the existing
ethical concerns raised by clinical research, its conduct in the real
world raises new ethical concerns and, thereby, offers opportunities
for philosophers looking for interesting, not to mention practically
very important issues in need of analysis and resolution.
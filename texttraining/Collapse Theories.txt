Quantum mechanics, with its revolutionary implications, has posed
innumerable problems to philosophers of science. In particular, it has
suggested reconsidering basic concepts such as the existence of a
world that is, at least to some extent, independent of the observer,
the possibility of getting reliable and objective knowledge about it,
and the possibility of taking (under appropriate circumstances)
certain properties to be objectively possessed by physical systems. It
has also raised many others questions which are well known to those
involved in the debate on the interpretation of this pillar of modern
science. One can argue that most of the problems are not only due to
the intrinsic revolutionary nature of the phenomena which have led to
the development of the theory. They are also related to the fact that,
in its standard formulation and interpretation, quantum mechanics is a
theory which is excellent (in fact it has met with a success
unprecedented in the history of science) in telling us everything
about what we observe, but it meets with serious difficulties
in telling us what is. We are making here specific reference
to the central problem of the theory, usually referred to as the
measurement problem, or, with a more appropriate term, as the
macro-objectification problem. It is just one of the many
attempts to overcome the difficulties posed by this problem that has
led to the development of Collapse Theories, i.e., to the
Dynamical Reduction Program (DRP). As we shall see, this
approach consists in accepting that the dynamical equation of the
standard theory should be modified by the addition of stochastic and
nonlinear terms. The nice fact is that the resulting theory is
capable, on the basis of a single dynamics which is assumed to govern
all natural processes, to account at the same time for all
well-established facts about microscopic systems as described by the
standard theory as well as for the so-called postulate of wave packet
reduction (WPR). As is well known, such a postulate is assumed in the
standard scheme just in order to guarantee that measurements have
outcomes but, as we shall discuss below, it meets with
insurmountable difficulties if one takes the measurement itself to be
a process governed by the linear laws of the theory. Finally, the
collapse theories account in a completely satisfactory way for the
classical behavior of macroscopic systems. 

Two specifications are necessary in order to make clear from the
beginning what are the limitations and the merits of the program. The
only satisfactory explicit models of this type (which are essentially
variations and refinements of the one proposed in 
Ghirardi, Rimini, and Weber (1986), and usually referred to as
the GRW theory) are phenomenological attempts to solve a foundational
problem. At present, they involve phenomenological parameters which,
if the theory is taken seriously, acquire the status of new constants
of nature. Moreover, the problem of building satisfactory relativistic
generalizations of these models which seemed extremely difficult up to
few years ago, has seen some significant improvements. More important,
such improvements have elucidated some crucial points and have made
clear that there is no reason of principle preventing to reach this
goal.

In spite of their phenomenological character, we think that Collapse
Theories have a remarkable relevance, since they have made clear that
there are new ways to overcome the difficulties of the formalism, to
close the circle in the precise sense defined by Abner
Shimony (1989), which until a few years ago were considered
impracticable, and which, on the contrary, have been shown to be
perfectly viable. Moreover, they have allowed a clear identification
of the formal features which should characterize any unified theory of
micro and macro processes. Last but not least, Collapse theories
qualify themselves as rival theories of quantum mechanics and one can
easily identify some of their physical implications which, in
principle, would allow crucial tests discriminating between the two.
To get really stringent indications from such tests requires
experiments involving technological techniques which have been
developed only very recently. Actually, it is just due to remarkable
improvements in dealing with mesoscopic systems and to important
practical steps forward, that some specific bounds have already been
obtained for the parameters characterizing the theories under
investigation, and, more important, precise families of physical
processes in which a violation of the linear nature of the standard
formalism might emerge have been clearly identified and are the
subject of systematic investigations which might lead, in the end, to
relevant discoveries.
 
1. General Considerations

As stated already, a very natural question which all scientists who
are concerned about the meaning and the value of science have to face,
is whether one can develop a coherent worldview that can accommodate
our knowledge concerning natural phenomena as it is embodied in our
best theories. Such a program meets serious difficulties with quantum
mechanics, essentially because of two formal aspects of the theory
which are common to all of its versions, from the original
nonrelativistic formulations of the 1920s, to the quantum field
theories of recent years: the linear nature of the state space and of
the evolution equation, i.e., the validity of the superposition
principle and the related phenomenon of entanglement, which, in
Schrödinger’s words: 

is not one but the characteristic trait of quantum mechanics, the one
that enforces its entire departure from classical lines of thought
(Schrödinger, 1935, p. 807).


These two formal features have embarrassing consequences, since they
imply 

objective chance in natural processes, i.e., the nonepistemic
nature of quantum probabilities;
objective indefiniteness of physical properties both at the micro
and macro level;
objective entanglement between spatially separated and
non-interacting constituents of a composite system, entailing a sort
of holism and a precise kind of nonlocality.


For the sake of generality, we shall first of all present a very
concise sketch of ‘the rules of the quantum game’.
2. The Formalism: A Concise Sketch

Let us recall the axiomatic structure of quantum theory: 



States of physical systems are associated with normalized vectors in a
Hilbert space, a complex, infinite-dimensional, complete and separable
linear vector space equipped with a scalar product. Linearity implies
that the superposition principle holds: if |f⟩|f⟩\ket{f} is a state and
|g⟩|g⟩\ket{g} is a state, then (for aaa and bbb arbitrary complex
numbers) also
|K⟩=a|f⟩+b|g⟩|K⟩=a|f⟩+b|g⟩
\ket{K} = a\ket{f} + b\ket{g}


is a state. Moreover, the state evolution is linear, i.e., it
preserves superpositions: if |f,t⟩|f,t⟩\ket{f,t} and |g,t⟩|g,t⟩\ket{g,t} are the
states obtained by evolving the states |f,0⟩|f,0⟩\ket{f,0} and
|g,0⟩|g,0⟩\ket{g,0}, respectively, from the initial time t=0t=0t=0 to the time
ttt, then a|f,t⟩+b|g,t⟩a|f,t⟩+b|g,t⟩a\ket{f,t} + b\ket{g,t} is the state obtained by the
evolution of a|f,0⟩+b|g,0⟩a|f,0⟩+b|g,0⟩a\ket{f,0} + b\ket{g,0}. Finally, the completeness
assumption is made, i.e., that the knowledge of its statevector
represents, in principle, the most accurate information one can have
about the state of an individual physical system.


The observable quantities are represented by self-adjoint operators
BBB on the Hilbert space. The associated eigenvalue equations
B|bk⟩=bk|bk⟩B|bk⟩=bk|bk⟩B\ket{b_k} = b_k \ket{b_k} and the corresponding eigenmanifolds
(the linear manifolds spanned by the eigenvectors associated to a
given eigenvalue, also called eigenspaces) play a basic role for the
predictive content of the theory. In fact:

The eigenvalues bkbkb_k of an operator BBB represent the only
possible outcomes in a measurement of the corresponding
observable.
The square of the norm (i.e., the length) of the projection of the
normalized vector (i.e., of length 1) describing the state of the
system onto the eigenmanifold associated to a given eigenvalue gives
the probability of obtaining the corresponding eigenvalue as the
outcome of the measurement. In particular, it is useful to recall that
when one is interested in the probability of finding a particle at a
given place, one has to resort to the so-called configuration space
representation of the statevector. In such a case the statevector
becomes a square-integrable function of the position variables of the
particles of the system, whose modulus squared yields the probability
density for the outcomes of position measurements.



We stress that, according to the above scheme, quantum mechanics makes
only conditional probabilistic predictions (conditional on the
measurement being actually performed) for the outcomes of prospective
(and in general incompatible) measurement processes. Only if a state
belongs already before the act of measurement to an eigenmanifold of
the observable which is going to be measured, can one predict the
outcome with certainty. In all other cases—if the completeness
assumption is made—one has objective nonepistemic probabilities
for different outcomes.

The orthodox position gives a very simple answer to the question: what
determines the outcome when different outcomes are possible?
Nothing—the theory is complete and, as a consequence, it is
illegitimate to raise any question about possessed properties
referring to observables for which different outcomes have
non-vanishing probabilities of being obtained. Correspondingly, the
referent of the theory are the results of measurement procedures.
These are to be described in classical terms and involve in general
mutually exclusive physical conditions.

As regards the legitimacy of attributing properties to physical
systems, one could say that quantum mechanics warns us against
requiring too many properties to be actually possessed by physical
systems. However—with Einstein—one can adopt as a
sufficient condition for the existence of an objective individual
property that one be able (without in any way disturbing the system)
to predict with certainty the outcome of a measurement. This implies
that, whenever the overall statevector factorizes into the product of
a state of the Hilbert space of the physical system SSS and of the
rest of the world, SSS does possess some properties (actually a
complete set of properties, i.e., those associated to appropriate
maximal sets of commuting observables).

Before concluding this section we must add some comments about the
measurement process. Quantum theory was created to deal with
microscopic phenomena. In order to obtain information about them one
must be able to establish strict correlations between the states of
the microscopic systems and the states of objects we can perceive.
Within the formalism, this is described by considering appropriate
micro-macro interactions. The fact that when the measurement is
completed one can make statements about the outcome is accounted for
by the already mentioned WPR postulate (Dirac 1948): a measurement
always causes a system to jump in an eigenstate of the observed
quantity. Correspondingly, also the statevector of the apparatus
‘jumps’ into the manifold associated to the recorded
outcome.
3. The Macro-Objectification Problem

In this section we shall clarify why the formalism we have just
presented gives rise to the measurement or macro-objectification
problem. To this purpose we shall, first of all, discuss the standard
oversimplified argument based on the so-called von Neumann ideal
measurement scheme. 

Let us begin by recalling the basic points of the standard
argument:

Suppose that a microsystem SSS, just before the measurement of an
observable BBB, is in the eigenstate |bj⟩|bj⟩\ket{b_j} of the
corresponding operator. The apparatus (a macrosystem) used to gain
information about BBB is initially assumed to be in a precise
macroscopic state, its ready state, corresponding to a definite macro
property—e.g., its pointer points at 0 on a scale. Since the
apparatus AAA is made of elementary particles, atoms and so on, it
must be described by quantum mechanics, which will associate to it the
state vector |A0⟩|A0⟩\ket{A_0}. One then assumes that there is an
appropriate system-apparatus interaction lasting for a finite time,
such that when the initial apparatus state is triggered by the state
|bj⟩|bj⟩\ket{b_j} it ends up in a final configuration |Aj⟩|Aj⟩\ket{A_j}, which
is macroscopically distinguishable from the initial one and from the
other configurations |Ak⟩|Ak⟩\ket{A_k} in which it would end up if
triggered by a different eigenstate |bk⟩|bk⟩\ket{b_k}. Moreover, one
assumes that the system is left in its initial state. In brief, one
assumes that one can dispose things in such a way that the
system-apparatus interaction can be described as:
(initial state): (final state): |bk⟩|A0⟩|bk⟩|Ak⟩(1)(1)(initial state): |bk⟩|A0⟩(final state): |bk⟩|Ak⟩\begin{align}
\tag{1}
\textit{(initial state)}{:}\ &amp; \ket{b_k} \ket{A_0} \\
\textit{(final state)}{:}\ &amp; \ket{b_k} \ket{A_k} 
\end{align}

Equation (1) and the hypothesis that the superposition principle
governs all natural processes tell us that, if the initial state of
the microsystem is a linear superposition of different eigenstates
(for simplicity we will consider only two of them), one has:
(initial state): (final state): (a|bk⟩+b|bj⟩)|A0⟩(a|bk⟩|Ak⟩+b|bj⟩|Aj⟩).(2)(2)(initial state): (a|bk⟩+b|bj⟩)|A0⟩(final state): (a|bk⟩|Ak⟩+b|bj⟩|Aj⟩).\begin{align}
\tag{2}
\textit{(initial state)}{:}\ &amp; (a\ket{b_k} + b\ket{b_j})\ket{A_0 } \\
\textit{(final state)}{:}\ &amp; (a\ket{b_k} \ket{A_k} + b\ket{b_j} \ket{A_j}).
\end{align}

Some remarks about this are in order:

The scheme is highly idealized, both because it takes for granted
that one can prepare the apparatus in a precise state, which is
impossible since we cannot have control over all its degrees of
freedom, and because it assumes that the apparatus registers the
outcome without altering the state of the measured system. However, as
we shall discuss below, these assumptions are by no means essential to
derive the embarrassing conclusion we have to face, i.e., that the
final state is a linear superposition of two states corresponding to
two macroscopically different states of the apparatus. Since we know
that the + representing linear superpositions cannot be replaced by
the logical alternative either … or, the measurement
problem arises: what meaning can one attach to a state of affairs in
which two macroscopically and perceptively different states occur
simultaneously?
As already mentioned, the standard solution to this problem is
given by the WPR postulate: in a measurement process reduction occurs:
the final state is not the one appearing in the second line of
equation (2) but, since macro-objectification takes place, it is

either or |bk⟩|Ak⟩ with probability |a|2|bj⟩|Aj⟩ with probability |b|2.(3)(3)either |bk⟩|Ak⟩ with probability |a|2or |bj⟩|Aj⟩ with probability |b|2.
\begin{align}
\tag{3}
\text{either } &amp;\ket{b_k} \ket{A_k} \text{ with probability } \lvert a\rvert^2 \\
\text{or } &amp;\ket{b_j} \ket{A_j} \text{ with probability } \lvert b\rvert^2.
\end{align}

Nowadays, there is a general consensus that this solution is
absolutely unacceptable for two basic reasons:

It corresponds to assuming that the linear nature of the theory is
broken at a certain level. Thus, quantum theory is unable to explain
how it can happen that the apparata behave as required by the WPR
postulate (which is one of the axioms of the theory).
Even if one were to accept that quantum mechanics has a limited
field of applicability, so that it does not account for all natural
processes and, in particular, it breaks down at the macrolevel, it is
clear that the theory does not contain any precise criterion for
identifying the borderline between micro and macro, linear and
nonlinear, deterministic and stochastic, reversible and irreversible.
To use J.S. Bell’s words, there is nothing in the theory fixing
such a borderline and the split between the two above types
of processes is fundamentally shifty. As a matter of fact, if
one looks at the historical debate on this problem, one can easily see
that it is precisely by continuously resorting to this ambiguity about
the split that adherents of the Copenhagen orthodoxy or easy
solvers (Bell 1990) of the measurement problem have rejected the
criticism of the heretics (Gottfried 2000). For instance,
Bohr succeeded in rejecting Einstein’s criticisms at the Solvay
Conferences by stressing that some macroscopic parts of the apparatus
had to be treated fully quantum mechanically; von Neumann and Wigner
displaced the split by locating it between the physical and the
conscious (but what is a conscious being?), and so on. Also other
proposed solutions to the problem, notably certain versions of
many-worlds interpretations, suffer from analogous ambiguities.


It is not our task to review here the various attempts to solve the
above difficulties. One can find many exhaustive treatments of this
problem in the literature. On the contrary, we would like to discuss
how the macro-objectification problem is indeed a consequence of very
general, in fact unavoidable, assumptions on the nature of
measurements, and not specifically of the assumptions of von
Neumann’s model. This was established in a series of theorems of
increasing generality, notably the ones by Fine (1970),
d’Espagnat (1971), Shimony (1974), Brown (1986) and Busch and
Shimony (1996). Possibly the most general and direct proof is given by
Bassi and Ghirardi (2000), whose results we briefly summarize. The
assumptions of the theorem are:

that a microsystem can be prepared in two different eigenstates of
an observable (such as, e.g., the spin component along the z-axis) and
in a superposition of two such states;
that one has a sufficiently reliable way of
‘measuring’ such an observable, meaning that when the
measurement is triggered by each of the two above eigenstates, the
process leads in the vast majority of cases to
macroscopically and perceptually different situations of the universe.
This requirement allows for cases in which the experimenter does not
have perfect control of the apparatus, the apparatus is entangled with
the rest of the universe, the apparatus makes mistakes, or the
measured system is altered or even destroyed in the measurement
process;
that all natural processes obey the linear laws of the
theory.


From these very general assumptions one can show that, repeating the
measurement on systems prepared in the superposition of the two given
eigenstates, in the great majority of cases one ends up in a
superposition of macroscopically and perceptually different situations
of the whole universe. If one wishes to have an acceptable final
situation, one mirroring the fact that we have definite perceptions,
one is arguably compelled to break the linearity of the theory at an
appropriate stage.
4. The Birth of Collapse Theories

The debate on the macro-objectification problem continued for many
years after the early days of quantum mechanics. In the early 1950s an
important step was taken by D. Bohm who presented (Bohm 1952) a
mathematically precise deterministic completion of quantum mechanics
(see the entry on Bohmian Mechanics). In the area of Collapse
Theories, one should mention the contribution by Bohm and Bub (1966),
which was based on the interaction of the statevector with
Wiener-Siegel hidden variables. But let us come to Collapse Theories
in the sense currently attached to this expression. 

Various investigations during the 1970s can be considered as
preliminary steps for the subsequent developments. In the years
1970 we were
seriously concerned with quantum decay processes and in particular
with the possibility of deriving, within a quantum context, the
exponential decay law. For an exhaustive review of our approach see (Fonda,
Ghirardi, and Rimini 1978). Some features of this approach are
extremely relevant for the DRP. Let us list them:

One deals with individual physical systems;
The statevector is supposed to undergo random processes at random
times, inducing sudden changes driving it either within the linear
manifold of the unstable state or within the one of the decay
products;
To make the treatment quite general (the apparatus does not know
which kind of unstable system it is testing) one is led to identify
the random processes with localization processes of the relative
coordinates of the decay fragments. Such an assumption, combined with
the peculiar resonant dynamics characterizing an unstable system,
yields, completely in general, the desired result. The ‘relative
position basis’ is the preferred basis of this theory;
Analogous ideas have been applied to measurement processes;
The final equation for the evolution at the ensemble level is of
the quantum dynamical semigroup type and has a structure extremely
similar to the final one of the GRW theory.


Obviously, in these papers the reduction processes which are involved
were not assumed to be ‘spontaneous and fundamental’
natural processes, but due to system-environment interactions.
Accordingly, these attempts did not represent original proposals for
solving the macro-objectification problem but they have paved the way
for the elaboration of the GRW theory. 

Almost in the same years, P. Pearle (1976, 1979), and subsequently N.
Gisin (1984) and others, had entertained the idea of accounting for
the reduction process in terms of a stochastic differential equation.
These authors were really looking for a new dynamical equation and for
a solution to the macro-objectification problem. Unfortunately, they
were unable to give any precise suggestion about how to identify the
states to which the dynamical equation should lead. Indeed, these
states were assumed to depend on the particular measurement process
one was considering. Without a clear indication on this point there
was no way to identify a mechanism whose effect could be negligible
for microsystems but extremely relevant for all the macroscopic ones.
N. Gisin gave subsequently an interesting (though not uncontroversial)
argument (Gisin 1989) that nonlinear modifications of the standard
equation without stochasticity are unacceptable since they imply the
possibility of sending superluminal signals. Soon afterwards, G. C.
Ghirardi and R. Grassi  proved that stochastic modifications
without nonlinearity can at most induce ensemble and not individual
reductions, i.e., they do not guarantee that the state vector of each
individual physical system is driven in a manifold corresponding to
definite properties.
5. The Original Collapse Model

As already mentioned, the Collapse Theory  we are going to describe amounts to accepting a modification of
the standard evolution law of the theory such that microprocesses and
macroprocesses are governed by a single dynamics. Such a dynamics must
imply that the micro-macro interaction in a measurement process leads
to WPR. Bearing this in mind, recall that the characteristic feature
distinguishing quantum evolution from WPR is that, while
Schrödinger’s equation is linear and deterministic (at the
wave function level), WPR is nonlinear and stochastic. It is then
natural to consider, as was suggested for the first time in the above
quoted papers by P. Pearle, the possibility of nonlinear and
stochastic modifications of the standard Schrödinger dynamics.
However, the initial attempts to implement this idea were
unsatisfactory for various reasons. The first, which we have already
discussed, concerns the choice of the preferred basis: if one wants to
have a universal mechanism leading to reductions, to which linear
manifolds should the reduction mechanism drive the statevector? Or,
equivalently, which of the (generally) incompatible
‘potentialities’ of the standard theory should we choose
to make actual? The second, referred to as the trigger problem by
Pearle (1989), is the problem of how the reduction mechanism can
become more and more effective in going from the micro to the macro
domain. The solution to this problem constitutes the central feature
of the Collapse Theories of the GRW type. To discuss these points, let
us briefly review the first consistent Collapse model to appear in the literature.

Within such a model, originally referred to as QMSL (Quantum Mechanics
with Spontaneous Localizations), the problem of the choice of the
preferred basis is solved by noting that the most embarrassing
superpositions, at the macroscopic level, are those involving
different spatial locations of macroscopic objects. Actually, as
Einstein has stressed, this is a crucial point which has to be faced
by anybody aiming to take a macro-objective position about natural
phenomena: ‘A macro-body must always have a quasi-sharply
defined position in the objective description of reality’ (Born,
1971, p. 223). Accordingly, QMSL considers the possibility of
spontaneous processes, which are assumed to occur instantaneously and
at the microscopic level, which tend to suppress the linear
superpositions of differently localized states. The required trigger
mechanism must then follow consistently.

The key assumption of QMSL is the following: each elementary
constituent of any physical system is subjected, at random times, to
random and spontaneous localization processes (which we will call
hittings) around appropriate positions. To have a precise mathematical
model one has to be very specific about the above assumptions; in
particular one has to make explicit HOW the process works, i.e., which
modifications of the wave function are induced by the localizations,
WHERE it occurs, i.e., what determines the occurrence of a
localization at a certain position rather than at another one, and
finally WHEN, i.e., at what times, it occurs. The answers to these
questions are as follows.

Let us consider a system of NNN distinguishable particles and let us
denote by F(q1,q2,…,qN)F(q1,q2,…,qN)F(\boldsymbol{q}_1, \boldsymbol{q}_2 , \ldots
,\boldsymbol{q}_N ) the coordinate representation (wave function) of
the state vector (we disregard spin variables since hittings are
assumed not to act on them).

The answer to the question HOW is then: if a hitting occurs for
the iii-th particle at point xx\boldsymbol{x}, the wave function
is instantaneously multiplied by a Gaussian function (appropriately
normalized) 

G(qi,x)=Kexp[−{1/(2d2)}(qi−x)2],G(qi,x)=Kexp⁡[−{1/(2d2)}(qi−x)2], G(\boldsymbol{q}_i, \boldsymbol{x}) = K
\exp[-\{1/(2d^2)\}(\boldsymbol{q}_i -\boldsymbol{x})^2], 

where ddd represents the localization accuracy. Let us denote as
Li(q1,q2,…,qN;x)=F(q1,q2,…,qN)G(qi,x)Li(q1,q2,…,qN;x)=F(q1,q2,…,qN)G(qi,x) L_i (\boldsymbol{q}_1, \boldsymbol{q}_2, \ldots, \boldsymbol{q}_N ;
\boldsymbol{x}) = F(\boldsymbol{q}_1, \boldsymbol{q}_2, \ldots,
\boldsymbol{q}_N) G(\boldsymbol{q}_i, \boldsymbol{x}) 

the wave function immediately after the localization, as yet
unnormalized. 
As concerns the specification of WHERE the localization occurs, it
is assumed that the probability density P(x)P(x)P(\boldsymbol{x}) of its
taking place at the point xx\boldsymbol{x} is given by the square of
the norm of the state LiLiL_i (the length, or to be more precise, the
integral of the modulus squared of the function LiLiL_i over the
3N3N3N-dimensional space). This implies that hittings occur with
higher probability at those places where, in the standard quantum
description, there is a higher probability of finding the particle.
Note that the above prescription introduces nonlinear and stochastic
elements in the dynamics. The constant KKK appearing in the
expression of G(qi,x)G(qi,x)G(\boldsymbol{q}_i, \boldsymbol{x}) is chosen in
such a way that the integral of P(x)P(x)P(\boldsymbol{x}) over the whole
space equals 1.
Finally, the question WHEN is answered by assuming that the
hittings occur at randomly distributed times, according to a Poisson
distribution, with mean frequency fff.


It is straightforward to convince oneself that the hitting process
leads, when it occurs, to the suppression of the linear superpositions
of states in which the same particle is well localized at different
positions separated by a distance greater than ddd. As a simple
example we can consider a single particle whose wavefunction is
different from zero only in two small and far apart regions hhh and
ttt. Suppose that a localization occurs around hhh; the state
after the hitting is then appreciably different from zero only in a
region around hhh itself. A completely analogous argument holds for
the case in which the hitting takes place around ttt. As concerns
points which are far from both hhh and ttt, one easily sees that
the probability density for such hittings , according to the
multiplication rule determining LiLiL_i, turns out to be practically
zero, and moreover, that if such a hitting were to occur, after the
wave function is normalized, the wave function of the system would
remain almost unchanged.

We can now discuss the most important feature of the theory, i.e., the
Trigger Mechanism. To understand the way in which the spontaneous
localization mechanism is enhanced by increasing the number of
particles which are in far apart spatial regions (as compared to
d)d)d), one can consider, for simplicity, the superposition
|S⟩|S⟩\ket{S}, with equal weights, of two macroscopic pointer states
|H⟩|H⟩\ket{H} and |T⟩|T⟩\ket{T}, corresponding to two different pointer
positions HHH and TTT, respectively. Taking into account that the
pointer is ‘almost rigid’ and contains a macroscopic
number NNN of microscopic constituents, the state can be written, in
obvious notation, as:
|S⟩=[|1 near h1⟩…|N near hN⟩+|1 near t1⟩…|N near tN⟩],(4)(4)|S⟩=[|1 near h1⟩…|N near hN⟩+|1 near t1⟩…|N near tN⟩],\tag{4} \ket{S} = [\ket{1 \near h_1} \ldots \ket{N \near h_N} +
\ket{1 \near t_1} \ldots \ket{N \near t_N}], 

where hihih_i is near HHH, and titit_i is near TTT. The states
appearing in first term on the right-hand side of equation (4) have
coordinate representations which are different from zero only when
their arguments (1,…,N)(1,…,N)(1,\ldots ,N) are all near HHH, while those of
the second term are different from zero only when they are all near
TTT. It is now evident that if any of the particles (say, the
iii-th particle) undergoes a hitting process, e.g., near the point
hihih_i, the multiplication prescription leads practically to the
suppression of the second term in (4). Thus any spontaneous
localization of any of the constituents amounts to a localization of
the pointer. The hitting frequency is therefore effectively amplified
proportionally to the number of constituents. Notice that, for
simplicity, the argument makes reference to an almost rigid body,
i.e., to one for which all particles are around HHH in one of the
states of the superposition and around TTT in the other. It should
however be obvious that what really matters in amplifying the
reductions is the number of particles which are in different positions
in the two states appearing in the superposition itself.

Under these premises we can now proceed to choose the parameters ddd
and fff of the theory, i.e., the localization accuracy and the mean
localization frequency. The argument just given allows one to
understand how one can choose the parameters in such a way that the
quantum predictions for microscopic systems remain fully valid while
the embarrassing macroscopic superpositions in measurement-like
situations are suppressed in very short times. Accordingly, as a
consequence of the unified dynamics governing all physical processes,
individual macroscopic objects acquire definite macroscopic
properties. The choice suggested in the GRW-model is:
fd=10−16 s−1=10−5 cm(5)(5)f=10−16 s−1d=10−5 cm\begin{align}
\tag{5}
f &amp;= 10^{-16} \text{ s}^{-1} \\
d &amp;= 10^{-5} \text{ cm} 
\end{align}

It follows that a microscopic system undergoes a localization, on
average, every hundred million years, while a macroscopic one
undergoes a localization every 10−710−710^{-7} seconds. With reference to
the challenging version of the macro-objectification problem presented
by Schrödinger with the famous example of his cat, J.S. Bell
comments (1987, p.44): [within QMSL] the cat is not both dead and
alive for more than a split second. Besides the extremely low
frequency of the hittings for microscopic systems, also the fact that
the localization width is large compared to the dimensions of atoms
(so that even when a localization occurs it does very little violence
to the internal economy of an atom) plays an important role in
guaranteeing that no violation of well-tested quantum mechanical
predictions is implied by the modified dynamics.

Some remarks are appropriate. QMSL, being precisely formulated, allows
to locate precisely the ‘split’ between micro and macro,
reversible and irreversible, quantum and classical. The transition
between the two types of ‘regimes’ is governed by the
number of particles which are well localized at positions further
apart than 10−510−510^{-5} cm in the two states whose coherence is going
to be dynamically suppressed. In principle, the model is testable
against quantum mechanics. However, for the above choice of the values
of the parameters, its predictions do not contradict any already
established fact about microsystems and macrosystems.

Concerning the choice of the parameters of the model, it has to be
stressed that, as it is obvious, the just mentioned quantum to
classical transition region depends crucially on their values. The
situation concerning the two parameters is rather different; in fact
ddd cannot be made smaller than 10−510−510^{-5} cm without inducing
unacceptable effects on the internal dynamics, e.g., of solids, and it
cannot be made much larger if one wants macrosystems to end up being
rather accurately localized. On the contrary, an appreciable variation
of fff turns out to be possible. With reference to this point we
would like to mention that Adler (2003) has suggested to change its
value by a factor of the order of 10910910^9. The reasons for this
derive from pretending that the latent image formation in photography
occurs immediately after a grain of the emulsion has been excited, and
that when a human eye is hit by few photons (the perceptual threshold
being very low) reduction takes place in the rods of the eye. As we
will discuss in what follows, if one takes the original GRW value for
fff, reduction cannot occur in the rods (because a relatively small
number of molecules—less than 10510510^5—are affected), but
only during the transmission along the nervous signal within the
brain, a process which involves the displacement of a number of ions
of the order of 1012101210^{12}.

It is interesting to remark that the drastic change suggested by Adler
(2003) has physical implications which have already been
experimentally falsified, see Curceanu et al. 2015, Bassi
et al. 2010, Vinante et al. 2015 (Other Internet
Resources), and Toros & Bassi 2016 (Other Internet Ressources).

6. The Continuous Spontaneous Localization Model (CSL)

The model just presented (QMSL) has a serious drawback: it does not
allow to deal with systems containing identical constituents because
it does not respect the symmetry or antisymmetry requirements for such
particles. A quite natural idea to overcome this difficulty would be
that of relating the hitting process not to the individual particles
but to the particle number density averaged over an appropriate
volume. This can be done by introducing a new phenomenological
parameter in the theory which however can be eliminated by an
appropriate limiting procedure (see below).

Another way to overcome this problem derives from injecting the
physically appropriate principles of the GRW model within the original
approach of P. Pearle. This line of thought has led to a quite elegant
formulation of a dynamical reduction model, usually referred to as CSL
(Pearle 1989; Ghirardi, Pearle, and Rimini 1990) in which the
discontinuous jumps which characterize QMSL are replaced by a
continuous stochastic evolution in the Hilbert space (a sort of
Brownian motion of the statevector).

We will not enter into the rather technical details of this
interesting development of the original GRW proposal, since the basic
ideas and physical implications are precisely the same as those of the
original formulation. Actually, one could argue that the above idea of
tackling the problem of identical particles by considering the average
particle number within an appropriate volume is correct. In fact it
has been proved (Ghirardi, Pearle, and Rimini 1990) that for any CSL
dynamics there is a hitting dynamics which, from a physical point of
view, is ‘as close to it as one wants’. Instead of
entering into the details of the CSL formalism, it is useful, for the
discussion below, to analyze a simplified version of it.
7. A Simplified Version of CSL

With the aim of understanding the physical implications of the CSL
model, such as the rate of suppression of coherence, we make now some
simplifying assumptions. First, we assume that we are dealing with
only one kind of particles (e.g., the nucleons), secondly, we
disregard the standard Schrödinger term in the evolution and,
finally, we divide the whole space in cells of volume d3d3d^3. We
denote by |n1,n2,…⟩|n1,n2,…⟩\ket{n_1, n_2 ,\ldots} a Fock state in which there are
ninin_i particles in cell iii, and we consider a superposition of
two states |n1,n2,…⟩|n1,n2,…⟩\ket{n_1, n_2 , \ldots} and |m1,m2,…⟩|m1,m2,…⟩\ket{m_1, m_2 , \ldots}
which differ in the occupation numbers of the various cells of the
universe. With these assumptions it is quite easy to prove that the
rate of suppression of the coherence between the two states (so that
the final state is one of the two and not their superposition) is
governed by the quantity: 
exp{−f[(n1−m1)2+(n2−m2)2+…]t},(6)(6)exp⁡{−f[(n1−m1)2+(n2−m2)2+…]t},\tag{6}
\exp\{-f [(n_1 - m_1)^2 + (n_2 - m_2)^2 +\ldots]t\},


all cells of the universe appearing in the sum within the square
brackets in the exponent. Apart from differences relating to the
identity of the constituents, the overall physics is quite similar to
that implied by QMSL. 

Equation 6 offers the opportunity of discussing the possibility of
relating the suppression of coherence to gravitational effects. In
fact, with reference to this equation we notice that the worst case
scenario (from the point of view of the time necessary to suppress
coherence) is the one corresponding to the superposition of two states
for which the occupation numbers of the individual cells differ only
by one unit. Indeed, in this case the amplifying effect of taking the
square of the differences disappears. Let us then raise the question:
how many nucleons (at worst) should occupy different cells, in order
for the given superposition to be dynamically suppressed within the
time which characterizes human perceptual processes? Since such a time
is of the order of 10−210−210^{-2} sec and f=10−16f=10−16f = 10^{-16} sec−1−1^{-1},
the number of displaced nucleons must be of the order of 1018101810^{18},
which corresponds, to a remarkable accuracy, to a Planck mass. This
figure seems to point in the same direction as Penrose’s
attempts to relate reduction mechanisms to quantum gravitational
effects (Penrose 1989).

Obviously, the model theory we are discussing implies various further
physical effects which deserve to be discussed since they might allow
a test of the theory with respect to standard quantum mechanics. For
review, see (Bassi and Ghirardi 2003; Adler 2007, Bassi et
al. 2013). We briefly list the most promising type of experiments
which in the future might allow such a crucial test.

Effects in superconducting devices. A detailed analysis has been
presented in (Ghirardi and Rimini 1990). As shown there and as follows
from estimates about possible effects for superconducting devices (Rae
1990; Gallis and Fleming 1990; Rimini 1995), and for the excitation of
atoms (Squires 1991), it turns out not to be possible, with present
technology, to perform clear-cut experiments allowing to discriminate
the model from standard quantum mechanics.
Loss of coherence in diffraction experiments with macromolecules.
The group of Arndt and Zeilinger in Vienna has performed several
diffraction experiments involving macromolecules.The most well known
include C6060_{60}, (720 nucleons) (Arndt et al. 1999), C7070_{70},
(840 nucleons) (Hackermueller et al. 2004) and
C3030_{30}H1212_{12}F3030_{30}N22_2O44_4, (1030 nucleons) (Gerlich
et al. 2007). These experiments aim at testing the validity of the
superposition principle towards the macroscopic scale. The challenge
is very exciting and near-future technology will probably allow to
perform experiments with systems containing up to 10610610 ^6 nucleons
and, accordingly, they will represent those imposing most severe
limitations to the parameters of Collapse theories. 
Loss of coherence in opto-mechanical interferometers. Recently, an
interesting proposal of testing the superposition principle by
resorting to an experimental set-up involving a (mesoscopic) mirror
has been advanced (Marshall et al. 2003). This stimulating proposal
has led a group of scientists directly interested in Collapse Theories
(Bassi et al. 2005) to check whether the proposed experiment might be
a crucial one for testing dynamical reduction models versus quantum
mechanics. The problem is extremely subtle because the extension of
the oscillations of the mirror is much smaller than the localization
accuracy of GRW, so that the localizations processes become almost
ineffective. However, quite recently a detailed reconsideration of the
physics of such systems has been performed and it has allowed to draw
the relevant conclusion that the proposal by Adler (2007) of changing
the frequency of the GRW theory of a factor like the one he has
considered is untenable. 
Spontaneous X-ray emission from Germanium. Collapse models not
only forbid macroscopic superpositions to be stable, they share
several other features which are forbidden by the standard theory. One
of these is the spontaneous emission of radiation from otherwise
stable systems, like atoms. While the standard theory predicts that
such systems—if not excited—do not emit radiation,
collapse models allow for radiation to be produced. The emission rate
has been computed both for free charged particles (Fu 1997) and for
hydrogenic atoms (Adler et al. 2007). The theoretical predictions were
compatible with current experimental data (Fu 1997). At any rate, the
importance of such experiments lies in the fact that—so
far—they provide the strongest upper bounds on the collapse
parameters (Adler et al. 2007). But this is not the whole story: very
recently Curceanu et al, 2015, following this line of
research, have been able to prove experimentally that the proposal by
Adler (2007) of a drastic change of the frequency of the localizations
with respect to those of the original GRW paper is definitely
incompatible with the experimental data. 


In the recent years, another line of research has been proposed, one
which makes direct reference to the way, which we will discuss in
Section 10, in which collapse models account for the psycho-physical
correspondence. The suggested approach might lead to completely new
and fundamentally different practical tests of Collapse theories. The
basic facts concerning the proposal deserve to be mentioned. In almost
all physical situations we have analyzed, the appreciable dynamical
changes of the system (tipically, the spreading of the center-of-mass
position of a macroscopic object) take a time (years) which is
enormously longer than the one between two localizations (10−7(10−7(10^{-7}
sec). On the contrary, as we will discuss below, in the case of
conscious perceptions, the collapse time of two brain states in a
superposition and the time which is necessary for the emergence of a
definite perception, are quite similar, and this has some (small but
significant) implications concerning the probabilities of the
outcomes. This point has been analyzed in detail and explicitly
evaluated by resorting to a simple model of a quantum system subjected
to reduction processes (Ghirardi et al., 2014). The idea is to
consider a spin 1/2 particle whose spin rotates around the xxx-axis
with a frequency of about one hundreth of the one of the random
measurements ascertaining whether its spin is UP or DOWN with respect
to the zzz-axis. It turns out that for a superposition with
amplitudes aaa and bbb of the two eigenstates of Szz_z, the
probability of the two supervening perceptions associated to the two
outcomes will differ of about 1% from those predicted by quantum
mechanics, i.e. |a|2|a|2\lvert a\rvert^2 and |b|2|b|2\lvert b\rvert^2,
respectively.

The test would be quite interesting also for the general meaning of
collapse theories because it will give some practical evidence
concerning the fact that, in the case in which a superposition of two
microscopic different states which are able to trigger two precise
(and different) perceptions, the brain actually collapses the
wavefunction yielding only one perception, an clear-cut indication
that the standard theory cannot run the whole process. 


Summarizing, we stress that, due to recent technological improvements,
experiments in which one might test the deviations from Standard
Quantum Theory implied by Collapse Models, seems to have become more
feasible. Actually, lot of work has been done and it is still going on
in this direction. The subject is developing rapidly and important
papers have appeared and interesting experimental work has been and it
is being performed. For a detailed technical analysis and for a
precise specification of the limits for the parameters ddd and fff
which have been derived, we refer the reader to the papers by Bassi
et al. (2013), Donadi et al. (2013 a,b), Baharami
et al. (2014), Großardt et al. (2015, Other
Internet Resources), Vinante et al. (2015). 
8. Some remarks about Collapse Theories

A. Pais famously recalls in his biography of Einstein: 

We often discussed his notions on objective reality. I recall that
during one walk Einstein suddenly stopped, turned to me and asked
whether I really believed that the moon exists only when I look at it
(Pais 1982, p. 5).


In the context of Einstein’s remarks in Albert Einstein,
Philosopher-Scientist (Schilpp 1949), we can regard this
reference to the moon as an extreme example of ‘a fact that
belongs entirely within the sphere of macroscopic concepts’, as
is also a mark on a strip of paper that is used to register the
outcome of a decay experiment, so that 

as a consequence, there is hardly likely to be anyone who would be
inclined to consider seriously […] that the existence of the
location is essentially dependent upon the carrying out of an
observation made on the registration strip. For, in the macroscopic
sphere it simply is considered certain that one must adhere to the
program of a realistic description in space and time; whereas in the
sphere of microscopic situations one is more readily inclined to give
up, or at least to modify, this program (p. 671).


However, 

the ‘macroscopic’ and the ‘microscopic’ are so
inter-related that it appears impracticable to give up this program in
the ‘microscopic’ alone (p. 674).


One might speculate that Einstein would not have taken the DRP
seriously, given that it is a fundamentally indeterministic program.
On the other hand, the DRP allows precisely for this middle ground,
between giving up a ‘classical description in space and
time’ altogether (the moon is not there when nobody looks), and
requiring that it be applicable also at the microscopic level (as
within some kind of ‘hidden variables’ theory). It would
seem that the pursuit of ‘realism’ for Einstein was more a
program that had been very successful rather than an a priori
commitment, and that in principle he would have accepted attempts
requiring a radical change in our classical conceptions concerning
microsystems, provided they would nevertheless allow to take a
macrorealist position matching our definite perceptions at this
scale.

In the DRP, we can say of an electron in an EPR-Bohm situation that
‘when nobody looks’, it has no definite spin in any
direction , and in particular that when it is in a superposition of
two states localised far away from each other, it cannot be thought to
be at a definite place (see, however, the remarks in Section 11). In
the macrorealm, however, objects do have definite positions and are
generally describable in classical terms. That is, in spite of the
fact that the DRP program is not adding ‘hidden variables’
to the theory, it implies that the moon is definitely there even if no
sentient being has ever looked at it. In the words of J. S. Bell, the
DRP

allows electrons (in general microsystems) to enjoy the cloudiness of
waves, while allowing tables and chairs, and ourselves, and black
marks on photographs, to be rather definitely in one place rather than
another, and to be described in classical terms (Bell 1986, p. 364).


Such a program, as we have seen, is implemented by assuming only the
existence of wave functions, and by proposing a unified dynamics that
governs both microscopic processes and ‘measurements’. As
regards the latter, no vague definitions are needed. The new dynamical
equations govern the unfolding of any physical process, and the
macroscopic ambiguities that would arise from the linear evolution are
theoretically possible, but only of momentary duration, of no
practical importance and no source of embarrassment.

We have not yet analyzed the implications about locality, but since in
the DRP program no hidden variables are introduced, the situation can
be no worse than in ordinary quantum mechanics: ‘by adding
mathematical precision to the jumps in the wave function’,
the GRW theory ‘simply makes precise the action at a
distance of ordinary quantum mechanics’ (Bell 1987, p. 46).
Indeed, a detailed investigation of the locality properties of the
theory becomes possible as shown by Bell himself (Bell 1987, p. 47).
Moreover, as it will become clear when we will discuss the
interpretation of the theory in terms of mass density, the QMSL and
CSL theories lead in a natural way to account for a behaviour of
macroscopic objects corresponding to our definite perceptions about
them, the main objective of Einstein’s requirements.

The achievements of the DRP which are relevant for the debate about
the foundations of quantum mechanics can also be concisely summarized
in the words of H.P. Stapp:

The collapse mechanisms so far proposed could, on the one hand, be
viewed as ad hoc mutilations designed to force ontology to kneel to
prejudice. On the other hand, these proposals show that one can
certainly erect a coherent quantum ontology that generally conforms to
ordinary ideas at the macroscopic level (Stapp 1989, p. 157).

9. Relativistic Dynamical Reduction Models

As soon as the GRW proposal appeared and attracted the attention of
J.S. Bell it also stimulated him to look at it from the point of view
of relativity theory. As he stated subsequently (Bell 1989a):

When I saw this theory first, I thought that I could blow it out of
the water, by showing that it was grossly in violation of
Lorentz invariance. That’s connected with the problem of
‘quantum entanglement’, the EPR paradox.


Actually, he had already investigated this point by studying the
effect on the theory of a transformation mimicking a nonrelativistic
approximation of a Lorentz transformation and he arrived (Bell 1987)
at a surprising conclusion:

… the model is as Lorentz invariant as it could be in its
nonrelativistic version. It takes away the ground of my fear that any
exact formulation of quantum mechanics must conflict with fundamental
Lorentz invariance.


What Bell had actually proved by resorting to a two-times formulation
of the Schrödinger equation is that the model violates locality
by violating outcome independence and not, as deterministic hidden
variable theories do, parameter independence.

Indeed, with reference to this point we recall that, as is well known,
(Suppes and Zanotti 1976; van Fraassen 1982; Jarrett 1984; Shimony
1983; see also the entry on
 Bell’s Theorem),
 Bell’s locality assumption is equivalent to the conjunction of
two other assumptions, viz., in Shimony’s terminology, parameter
independence and outcome independence. In view of the experimental
violation of Bell’s inequality, one has to give up either or
both of these assumptions. The above splitting of the locality
requirement into two logically independent conditions is particularly
useful in discussing the different status of CSL and deterministic
hidden variable theories with respect to relativistic requirements.
Actually, as proved by Jarrett himself, when parameter independence is
violated, if one had access to the variables which specify completely
the state of individual physical systems, one could send
faster-than-light signals from one wing of the apparatus to the other.
Moreover, in Ghirardi and Grassi (1996) it has been proved that
it is impossible to build a genuinely relativistically
invariant theory which, in its nonrelativistic limit, exhibits
parameter dependence. Here we use the term genuinely
invariant to denote a theory for which there is no (hidden)
preferred reference frame. On the other hand, if locality is violated
only by the occurrence of outcome dependence then faster-than-light
signaling cannot be achieved (Eberhard 1978; Ghirardi, Rimini, and
Weber 1980). Few years after
the just mentioned proof by Bell, it has been shown in complete
generality (Ghirardi, Grassi, Butterfield, and Fleming 1993) that the GRW and CSL theories, just as
standard quantum mechanics, exhibit only outcome dependence. This is
to some extent encouraging and shows that there are no reasons of
principle making unviable the project of building a relativistically
invariant DRM.

Let us be more specific about this crucial problem. P. Pearle was the
first to propose (Pearle 1990) a relativistic generalization of CSL to
a quantum field theory describing a fermion field coupled to a meson
scalar field enriched with the introduction of stochastic and
nonlinear terms. A quite detailed discussion of this proposal was
presented in (Ghirardi et al. 1990a) where it was shown that the
theory enjoys of all properties which are necessary in order to meet
the relativistic constraints. Pearle’s approach requires the
precise formulation of the idea of stochastic Lorentz invariance. The
proposal can be summarized in the following terms: 

One considers a fermion field coupled to a meson field and puts
forward the idea of inducing localizations for the fermions through
their coupling to the mesons and a stochastic dynamical reduction
mechanism acting on the meson variables. In practice, one considers
Heisenberg evolution equations for the coupled fields and a
Tomonaga-Schwinger CSL-type evolution equation with a skew-hermitian
coupling to a c-number stochastic potential for the state vector. This
approach has been systematically investigated by Ghirardi, Grassi, and
Pearle (1990), to which we refer the reader for a detailed
discussion. Here we limit ourselves to stressing that, under certain
approximations, one obtains in the non-relativistic limit a CSL-type
equation inducing spatial localization. However, due to the white
noise nature of the stochastic potential, novel renormalization
problems arise: the increase per unit time and per unit volume of the
energy of the meson field is infinite due to the fact that infinitely
many mesons are created. This point has also been lucidly discussed by
Bell (1989b) in the talk he delivered at Trieste on the occasion of
the 25th anniversary of the International Centre for Theoretical
Physics. This talk appeared under the title The Trieste Lecture of
John Stewart Bell. For these
reasons one cannot consider this as a satisfactory example of a
relativistic reduction model.

In the years following the just mentioned attempts there has been a
flourishing of researches aimed at getting the desired result. Let us
briefly comment about them. As already mentioned, the source of the
divergences is the assumption of point interactions between the
quantum field operators in the dynamical equation for the statevector,
or, equivalently, the white character of the stochastic noise. Having
this aspect in mind P. Pearle (1989), L. Diosi (1990) and A. Bassi and
G.C. Ghirardi (2002) reconsidered the problem from the beginning by
investigating nonrelativistic theories with nonwhite Gaussian noises.
The problem turns out to be very difficult from the mathematical point
of view, but steps forward have been made. In recent years, a precise
formulation of the nonwhite generalization (Bassi and Ferialdi 2009)
of the so-called QMUPL model, which represents a simplified version of
GRW and CSL, has been proposed. Moreover, a perturbative approach for
the CSL model has been worked out (Adler and Bassi 2007, 2008).
Further work is necessary. This line of thought is very interesting at
the nonrelativistic level; however, it is not yet clear whether it
will lead to a real step forward in the development of relativistic
theories of spontaneous collapse. 

In the same spirit, Nicrosini and Rimini (Nicrosini 2003) tried to
smear out the point interactions without success because, in their
approach, a preferred reference frame had to be chosen in order to
circumvent the nonintegrability of the Tomonaga-Schwinger equation


Also other interesting and different approaches have been suggested.
Among them we mention the one by Dove and Squires (Dove 1996) based on
discrete rather than continuous stochastic processes and those by
Dawker and Herbauts (Dawker 2004a) and Dawker and Henson (Dawker
2004b) formulated on a discrete space-time.

Before going on we consider it important to call attention to the fact
that precisely in the same years similar attempts to get a
relativistic generalization of the other existing ‘exact’
theory, i.e., Bohmian Mechanics, were going on and that they too have
encountered some difficulties. Relevant steps are represented by a
paper (Dürr 1999) resorting to a preferred spacetime slicing, by
the investigations of Goldstein and Tumulka (Goldstein 2003) and by
other scientists (Berndl et. al 1996). However, we must recognize that
no one of these attempts has led to a fully satisfactory solution of
the problem of having a theory without observers, like Bohmian
mechanics, which is perfectly satisfactory from the relativistic point
of view, precisely due to the fact that they are not genuinely
Lorentz invariant in the sense we have made precise before.
Mention should be made also of the attempt by Dewdney and Horton
(Dewdney 2001) to build a relativistically invariant model based on
particle trajectories. 

Let us come back to the relativistic DRP. Some important changes have
occurred quite recently. Tumulka (2006a) succeeded in proposing a
relativistic version of the GRW theory for N non-interacting
distinguishable particles, based on the consideration of a multi-time
wavefunction whose evolution is governed by Dirac like equations and
adopts as its Primitive Ontology (see the next section) the one which
attaches a primary role to the space and time points at which
spontaneous localizations occur, as originally suggested by Bell
(1987). To my knowledge this represents the first proposal of a
relativistic dynamical reduction mechanism which satisfies all
relativistic requirements. In particular it is divergence free and
foliation independent. However it can deal only with systems
containing a fixed number of noninteracting fermions.

At this point explicit mention should be made of the most recent steps
which concern our problem. D. Bedingham (2011) following strictly the
original proposal by Pearle (1990) of a quantum field theory inducing
reductions based on a Tomonaga-Schwinger equation, has worked out an
analogous model which, however, overcomes the difficulties of the
original model. In fact, Bedingham has circumvented the crucial
problems deriving from point interactions by (paying the price of)
introducing, besides the fields characterizing the Quantum Field
Theories he is interested in, an auxiliary relativistic field that
amounts to a smearing of the interactions whilst preserving Lorentz
invariance and frame independence. Adopting this point of view and
taking advantage also of the proposal by Ghirardi (2000) concerning
the appropriate way to define objective properties at any space-time
point xxx, he has been able to work out a fully satisfactory and
consistent relativistic scheme for quantum field theories in which
reduction processes may occur.

It has also to be mentioned that, taking once more advantage of the
ideas of the paper by Ghirardi (2000), various of the just quoted
authors (see Bedingham et al. 2013), have been able to prove
that it is possible to work out a relativistic generalization of
Collapse models when their primitive ontology is taken to be the one
given by the mass density interpretation for the nonrelativistic case
we will present in what follows.

In view of these results and taking into account the interesting
investigations concerning relativistic Bohmian-like theories,the
conclusions that Tumulka has drawn concerning the status of attempts
to account for the macro-objectification process from a relativistic
perspective are well-founded:

A somewhat surprising feature of the present situation is that we seem
to arrive at the following alternative: Bohmian mechanics shows that
one can explain quantum mechanics, exactly and completely, if one is
willing to pay with using a preferred slicing of spacetime; our model
suggests that one should be able to avoid a preferred slicing of
spacetime if one is willing to pay with a certain deviation from
quantum mechanics,


a conclusion that he has rephrased and reinforced in (Tumulka
2006c):

Thus, with the presently available models we have the alternative:
either the conventional understanding of relativity is not right, or
quantum mechanics is not exact.


Very recently, a thorough and illuminating discussion of the important
approach by Tumulka has been presented by Tim Maudlin (2011) in the
third revised edition of his book Quantum Non-Locality and
Relativity. Tumulka’s position is perfectly consistent with
the present ideas concerning the attempts to transform relativistic
standard quantum mechanics into an ‘exact’ theory in the
sense which has been made precise by J. Bell. Since the only unified,
mathematically precise and formally consistent formulations of the
quantum description of natural processes are Bohmian mechanics and
GRW-like theories, if one chooses the first alternative one has to
accept the existence of a preferred reference frame, while in the
second case one is not led to such a drastic change of position with
respect to relativistic concepts but must accept that the ensuing
theory disagrees with the predictions of quantum mechanics and
acquires the status of a rival theory with respect to it.

In spite of the fact that the situation is, to some extent, still open
and requires further investigations, it has to be recognized that the
efforts which have been spent on such a program have made possible a
better understanding of some crucial points and have thrown light on
some important conceptual issues. First, they have led to a completely
general and rigorous formulation of the concept of stochastic
invariance. Second, they have
prompted a critical reconsideration, based on the discussion of
smeared observables with compact support, of the problem of locality
at the individual level. This analysis has brought out the necessity
of reconsidering the criteria for the attribution of objective local
properties to physical systems. In specific situations, one cannot
attribute any local property to a microsystem: any attempt to do so
gives rise to ambiguities. However, in the case of macroscopic
systems, the impossibility of attributing to them local properties
(or, equivalently, the ambiguity associated to such properties) lasts
only for time intervals of the order of those necessary for the
dynamical reduction to take place. Moreover, no objective property
corresponding to a local observable, even for microsystems, can emerge
as a consequence of a measurement-like event occurring in a space-like
separated region: such properties emerge only in the future light cone
of the considered macroscopic event. Finally, recent investigations
(Ghirardi and Grassi 1996; Ghirardi 2000) have shown that
the very formal structure of the theory is such that it does not
allow, even conceptually, to establish cause-effect relations between
space-like events.

The conclusion of this section, is that the question of whether a
relativistic dynamical reduction program can find a satisfactory
formulation seems to admit a positive answer.

A last comment. Recently, a paper by Conway and Kochen (Conway 2006,
2006b), which has raised a lot of interest, has been published. A few
words about it are in order, to clarify possible misunderstandings.
The first and most important aim of the paper is the derivation of
what the authors have called The Free Will Theorem, putting
forward the provocative idea that if human beings are free to make
their choices about the measurements they will perform on one of a
pair of far-away entangled particles, then one must admit that also
the elementary particles involved in the experiment have free will.
One might make several comments on this statement. For what concerns
us here the relevant fact is that the authors claim that their theorem
implies, as a byproduct, the impossibility of elaborating a
relativistically invariant dynamical reduction model. A lively debate
has arisen. At the end, Goldstein et al (Goldstein
2010) have made clear why the argument of Conway and Kochen is not
pertinent. We may conclude that nothing in principle forbids a
perfectly satisfactory relativistic generalization of the GRW theory,
and, actually, as repeatedly stressed, there are many elements which
indicate that this is actually feasible.
10. Collapse Theories and Definite Perceptions

Some authors (Albert and Vaidman 1989; Albert 1990, 1992) have raised
an interesting objection concerning the emergence of definite
perceptions within Collapse Theories. The objection is based on the
fact that one can easily imagine situations leading to definite
perceptions, that nevertheless do not involve the displacement of a
large number of particles up to the stage of the perception itself.
These cases would then constitute actual measurement situations which
cannot be described by the GRW theory, contrary to what happens for
the idealized (according to the authors) situations considered in many
presentations of it, i.e., those involving the displacement of some
sort of pointer. To be more specific, the above papers consider a
‘measurement-like’ process whose output is the emission of
a burst of few photons triggered by the position in which a particle
hits a screen. This can easily be devised by considering, e.g., a
Stern-Gerlach set-up in which a spin 1/2 microsystem, according to the
value of its spin component hits a fluorescent screen in different
places and excites a small number of atoms which subsequently decay,
emitting a small number of photons. The argument goes as follows: if
one triggers the apparatus with a superposition of two spin states,
since only a few atoms are excited, since the excitations involve
displacements which are smaller than the characteristic localization
distance of GRW, since GRW does not induce reductions on photon states
and, finally, since the photon states immediately overlap, there is no
way for the spontaneous localization mechanism to become effective in
suppressing the ensuing superposition of the states ‘photons
emerging from point AAA of the screen’ and ‘photons
emerging from point BBB of the screen’. On the other hand,
since the visual perception threshold is quite low (about 6-7
photons), there is no doubt that the naked eye of a human observer is
sufficient to detect whether the luminous spot on the screen is at
AAA or at BBB. The conclusion follows: in the case under
consideration no dynamical reduction can take place and as a
consequence no measurement is over, no outcome is definite, up to the
moment in which a conscious observer perceives the spot.

Aicardi et al. (1991) have presented a detailed answer to this
criticism. The crucial points of the argument are the following: it is
agreed that in the case considered the superposition persists for long
times (actually the superposition must persist, since, the system
under consideration being microscopic, one could perform interference
experiments which everybody would expect to confirm quantum
mechanics). However, to deal in the appropriate and correct way with
such a criticism, one has to consider all the systems which enter into
play (electron, screen, photons and brain) and the universal dynamics
governing all relevant physical processes. A simple estimate of the
number of ions which are involved in the transmission of the nervous
signal up to the higher virtual cortex makes perfectly plausible that,
in the process, a sufficient number of particles are displaced by a
sufficient spatial amount to satisfy the conditions under which,
according to the GRW theory, the suppression of the superposition of
the two nervous signals will take place within the time scale of
perception.

To avoid misunderstandings, this analysis by no means amounts to
attributing a special role to the conscious observer or to perception.
The observer’s brain is the only system present in the set-up in
which a superposition of two states involving different locations of a
large number of particles occurs. As such it is the only place where
the reduction can and actually must take place according to the
theory. It is extremely important to stress that if in place of the
eye of a human being one puts in front of the photon beams a spark
chamber or a device leading to the displacement of a macroscopic
pointer, or producing ink spots on a computer output, reduction will
equally take place. In the given example, the human nervous system is
simply a physical system, a specific assembly of particles, which
performs the same function as one of these devices, if no other such
device interacts with the photons before the human observer does. It
follows that it is incorrect and seriously misleading to claim that
the GRW theory requires a conscious observer in order that
measurements have a definite outcome.

A further remark may be appropriate. The above analysis could be taken
by the reader as indicating a very naive and oversimplified attitude
towards the deep problem of the mind-brain correspondence. There is no
claim and no presumption that GRW allows a physicalist explanation of
conscious perception. It is only pointed out that, for what we know
about the purely physical aspects of the process, one can state that
before the nervous pulses reach the higher visual cortex, the
conditions guaranteeing the suppression of one of the two signals are
verified. In brief, a consistent use of the dynamical reduction
mechanism in the above situation accounts for the definiteness of the
conscious perception, even in the extremely peculiar situation devised
by Albert and Vaidman.
11. The Interpretation of the Theory and its Primitive Ontologies 

As stressed in the opening sentences of this contribution, the most
serious problem of standard quantum mechanics lies in its being
extremely successful in telling us about what we observe, but
being basically silent on what is. This specific feature is
closely related to the probabilistic interpretation of the
statevector, combined with the completeness assumption of the theory.
Notice that what is under discussion is the probabilistic
interpretation, not the probabilistic character, of the theory. Also
collapse theories have a fundamentally stochastic character, but, due
to their most specific feature, i.e., that of driving the statevector
of any individual physical system into appropriate and physically
meaningful manifolds, they allow for a different interpretation. One
could even say (if one wants to avoid that they too, as the standard
theory, speak only of what we find) that they
require a different interpretation, one that accounts for our
perceptions at the appropriate, i.e., macroscopic, level.

We must admit that this opinion is not universally shared. According
to various authors, the ‘rules of the game’ embodied in
the precise formulation of the GRW and CSL theories represent all
there is to say about them. However, this cannot be the whole story:
stricter and more precise requirements than the purely formal ones
must be imposed for a theory to be taken seriously as a fundamental
description of natural processes (an opinion shared by J. Bell). This
request of going beyond the purely formal aspects of a theoretical
scheme has been denoted as (the necessity of specifying) the Primitive
Ontology (PO) of the theory in an extremely interesting recent paper
(Allori et al. 2008). The
fundamental requisite of the PO is that it should make absolutely
precise what the theory is fundamentally about.

This is not a new problem; as already mentioned it has been raised by
J. Bell since his first presentation of the GRW theory. Let me
summarize the terms of the debate. Given that the wavefunction of a
many-particle system lives in a (high-dimensional) configuration
space, which is not endowed with a direct physical meaning connected
to our experience of the world around us, Bell wanted to identify the
‘local beables’ of the theory, the quantities on which one
could base a description of the perceived reality in ordinary
three-dimensional space. In the specific context of QMSL, he (Bell
1987 p. 45) suggested that the ‘GRW jumps’, which we
called ‘hittings’, could play this role. In fact they
occur at precise times in precise positions of the three-dimensional
space. As suggested in (Allori et al. 2008) we will denote
this position concerning the PO of the GRW theory as the
‘flashes ontology.’

However, later, Bell himself suggested that the most natural
interpretation of the wavefunction in the context of a collapse theory
would be that it describes the ‘density […] of
stuff’ in the 3N-dimensional configuration space (Bell 1990, p.
30), the natural mathematical framework for describing a system of
NNN particles. Allori et al. (2008) appropriately have
pointed out that this position amounts to avoiding commitment about
the PO ontology of the theory and, consequently, to leaving vague the
precise and meaningful connections it permits to be established
between the mathematical description of the unfolding of physical
processes and our perception of them.

The interpretation which, in the opinion of the present writer, is
most appropriate for collapse theories, has been proposed in
(Ghirardi, Grassi and Benatti 1995) and has been referred in
Allori et al. 2008 as ‘the mass density
ontology’. Let us briefly describe it.

First of all, various investigations (Pearle and Squires 1994) had
made clear that QMSL and CSL needed a modification, i.e., the
characteristic localization frequency of the elementary constituents
of matter had to be made proportional to the mass characterizing the
particle under consideration. In particular, the original frequency
for the hitting processes f=10−16f=10−16f = 10^{-16} sec−1−1^{-1} is the one
characterizing the nucleons, while, e.g., electrons would suffer
hittings with a frequency reduced by about 2000 times. Unfortunately
we have no space to discuss here the physical reasons which make this
choice appropriate; we refer the reader to the above paper, as well as
to the recent detailed analysis by Peruzzi and Rimini (2000). With
this modification, what the nonlinear dynamics strives to make
‘objectively definite’ is the mass distribution in the
whole universe. Second, a deep critical reconsideration (Ghirardi,
Grassi, and Benatti 1995) has made evident how the concept of
‘distance’ that characterizes the Hilbert space is
inappropriate in accounting for the similarity or difference between
macroscopic situations. Just to give a convincing example, consider
three states |h⟩,|h∗⟩|h⟩,|h∗⟩\ket{h} , \ket{h^*} and |t⟩|t⟩\ket{t} of a macrosystem
(let us say a massive macroscopic bulk of matter), the first
corresponding to its being located here, the second to its having the
same location but one of its atoms (or molecules) being in a state
orthogonal to the corresponding state in |h⟩|h⟩\ket{h}, and the third
having exactly the same internal state of the first but being
differently located (there). Then, despite the fact that the first two
states are indistinguishable from each other at the macrolevel, while
the first and the third correspond to completely different and
directly perceivable situations, the Hilbert space distance between
|h⟩|h⟩\ket{h} and |h∗⟩|h∗⟩\ket{h^*}, is equal to that between |h⟩|h⟩\ket{h}
and |t⟩|t⟩\ket{t}.

When the localization frequency is related to the mass of the
constituents, then, in completely generality (i.e., even when one is
dealing with a body which is not almost rigid, such as a gas or a
cloud), the mechanism leading to the suppression of the superpositions
of macroscopically different states is fundamentally governed by the
the integral of the squared differences of the mass densities
associated to the two superposed states. Actually, in the original
paper  the mass density at a point
was identified with its average over the characteristic volume of the
theory, i.e., 10−1510−1510^{-15} cm33^3 around that point. It is however
easy to convince oneself that there is no need to do so  and that the mass density at any point, directly identified by
the statevector (see below), is the appropriate quantity on which to
base an appropriate ontology. Accordingly, we take the following
attitude: what the theory is about, what is real ‘out
there’ at a given space point xx\boldsymbol{x}, is just a
field, i.e., a variable m(x,t)m(x,t)m(\mathbf{x},t) given by the expectation
value of the mass density operator M(x)M(x)M(\boldsymbol{x}) at
xx\boldsymbol{x} obtained by multiplying the mass of any kind of
particle times the number density operator for the considered type of
particle and summing over all possible types of particles which can be
present:
m(x,t)M(x)=⟨F,t∣M(x)∣F,t⟩;=∑(k)m(k)a∗(k)(x)a(k)(x).(7)(7)m(x,t)=⟨F,t∣M(x)∣F,t⟩;M(x)=∑(k)m(k)a(k)∗(x)a(k)(x).\begin{align}
\tag{7}
m(\boldsymbol{x},t) &amp;= \langle F,t \mid M(\boldsymbol{x}) \mid F,t \rangle; \\
M(\boldsymbol{x}) &amp;= {\sum}_{(k)} m_{(k)}a^*_{(k)}(\boldsymbol{x})a_{(k)}(\boldsymbol{x}).
\end{align}

Here |F,t⟩|F,t⟩\ket{F,t} is the statevector characterizing the system at the
given time, and a∗(k)(x)a(k)∗(x)a^*_{(k)}(\boldsymbol{x}) and
a(k)(x)a(k)(x)a_{(k)}(\boldsymbol{x}) are the creation and annihilation
operators for a particle of type kkk at point xx\boldsymbol{x}. It
is obvious that within standard quantum mechanics such a function
cannot be endowed with any objective physical meaning due to the
occurrence of linear superpositions which give rise to values that do
not correspond to what we find in a measurement process or what we
perceive. In the case of GRW or CSL theories, if one considers only
the states allowed by the dynamics one can give a description of the
world in terms of m(x,t)m(x,t)m(\boldsymbol{x},t), i.e., one recovers a
physically meaningful account of physical reality in the usual
3-dimensional space and time. To illustrate this crucial point we
consider, first of all, the embarrassing situation of a macroscopic
object in the superposition of two differently located position
states. We have then simply to recall that in a collapse model
relating reductions to mass density differences, the dynamics
suppresses in extremely short times the embarrassing superpositions of
such states to recover the mass distribution corresponding to our
perceptions. Let us come now to a microsystem and let us consider the
equal weight superposition of two states |h⟩|h⟩\ket{h} and |t⟩|t⟩\ket{t}
describing a microscopic particle in two different locations. Such a
state gives rise to a mass distribution corresponding to 1/2 of the
mass of the particle in the two considered space regions. This seems,
at first sight, to contradict what is revealed by any measurement
process. But in such a case we know that the theory implies that the
dynamics running all natural processes within GRW ensures that
whenever one tries to locate the particle he will always find it in a
definite position, e.g., one and only one of the Geiger counters which
might be triggered by the passage of the proton will fire, just
because a superposition of ‘a counter which has fired’ and
‘one which has not fired’ is dynamically forbidden.

This analysis shows that one can consider at all levels (the micro and
the macroscopic ones) the field m(x,t)m(x,t)m(\mathbf{x},t) as accounting for
‘what is out there’, as originally suggested by
Schrödinger with his realistic interpretation of the square of
the wave function of a particle as representing the
‘fuzzy’ character of the mass (or charge) of the particle.
Obviously, within standard quantum mechanics such a position cannot be
maintained because ‘wavepackets diffuse, and with the passage of
time become infinitely extended … but however far the
wavefunction has extended, the reaction of a detector … remains
spotty’, as appropriately remarked in (Bell 1990). As we hope to
have made clear, the picture is radically different when one takes
into account the new dynamics which succeeds perfectly in reconciling
the spread and sharp features of the wavefunction and of the detection
process, respectively.

It is also extremely important to stress that, by resorting to the
quantity (7) one can define an appropriate ‘distance’
between two states as the integral over the whole 3-dimensional space
of the square of the difference of m(x,t)m(x,t)m(\boldsymbol{x},t) for the two
given states, a quantity which turns out to be perfectly appropriate
to ground the concept of macroscopically similar or distinguishable
Hilbert space states. In turn, this distance can be used as a basis to
define a sensible psychophysical correspondence within the theory.
12. The Problem of the Tails of the Wave Function

In recent years, there has been a lively debate around a problem which
has its origin, according to some of the authors which have raised it,
in the fact that even though the localization process which
corresponds to multiplying the wave function times a Gaussian and thus
lead to wave functions strongly peaked around the position of the
hitting, they allow nevertheless the final wavefuntion to be different
from zero over the whole of space. The first criticism of this kind
was raised by A. Shimony (1990) and can be summarized by his
sentence,

one should not tolerate tails in wave functions which are so broad
that their different parts can be discriminated by the senses, even if
very low probability amplitude is assigned to them.


After a localization of a macroscopic system, typically the pointer of
the apparatus, its centre of mass will be associated to a wave
function which is different from zero over the whole space. If one
adopts the probabilistic interpretation of the standard theory, this
means that even when the measurement process is over, there is a
nonzero (even though extremely small) probability of finding its
pointer in an arbitrary position, instead of the one corresponding to
the registered outcome. This is taken as unacceptable, as indicating
that the DRP does not actually overcome the macro-objectification
problem.

Let us state immediately that the (alleged) problem arises entirely
from keeping the standard interpretation of the wave function
unchanged, in particular assuming that its modulus squared gives the
probability density of the position variable. However, as we have
discussed in the previous section, there are much more serious reasons
of principle which require to abandon the probabilistic interpretation
and replace it either with the ‘flash ontology’, or with
the ‘ mass density ontology’ which we have discussed
above.

Before entering into a detailed discussion of this subtle point we
need to focus better the problem. We cannot avoid making two remarks.
Suppose one adopts, for the moment, the conventional quantum position.
We agree that, within such a framework, the fact that wave functions
never have strictly compact spatial support can be considered
puzzling. However this is an unavoidable problem arising directly from
the mathematical features (spreading of wave functions) and from the
probabilistic interpretation of the theory, and not at all a problem
peculiar to the dynamical reduction models. Indeed, the fact that,
e.g., the wave function of the center of mass of a pointer or of a
table has not a compact support has never been taken to be a problem
for standard quantum mechanics. When, e.g., the center of mass of a
table is extremely well peaked around a given point in space, it has
always been accepted that it describes a table located at a certain
position, and that this corresponds in some way to our perception of
it. It is obviously true that, for the given wave function, the
quantum rules entail that if a measurement were performed the table
could be found (with an extremely small probability) to be kilometers
far away, but this is not the measurement or the
macro-objectification problem of the standard theory. The latter
concerns a completely different situation, i.e., that in which one is
confronted with a superposition with comparable weights of two
macroscopically separated wave functions, both of which possess tails
(i.e., have non-compact support) but are appreciably different from
zero only in far-away narrow intervals. This is the really
embarrassing situation which conventional quantum mechanics is unable
to make understandable. To which perception of the position of the
pointer (of the table) does this wave function correspond?

The implications for this problem of the adoption of the QMSL theory
should be obvious. Within GRW, the superposition of two states which,
when considered individually, are assumed to lead to different and
definite perceptions of macroscopic locations, are dynamically
forbidden. If some process tends to produce such superpositions, then
the reducing dynamics induces the localization of the centre of mass
(the associated wave function being appreciably different from zero
only in a narrow and precise interval). Correspondingly, the
possibility arises of attributing to the system the property of being
in a definite place and thus of accounting for our definite perception
of it. Summarizing, we stress once more that the criticism about the
tails as well as the requirement that the appearance of
macroscopically extended (even though extremely small) tails be
strictly forbidden is exclusively motivated by uncritically committing
oneself to the probabilistic interpretation of the theory, even for
what concerns the psycho-physical correspondence: when this position
is taken, states assigning non-exactly vanishing probabilities to
different outcomes of position measurements should correspond to
ambiguous perceptions about these positions. Since neither within the
standard formalism nor within the framework of dynamical reduction
models a wave function can have compact support, taking such a
position leads to conclude that it is just the linear character of the
Hilbert space description of physical systems which has to be given
up.

It ought to be stressed that there is nothing in the GRW theory which
forbids or makes problematic to assume that the localization function
has compact support, but it also has to be noted that following this
line would be totally useless: since the evolution equation contains
the kinetic energy term, any function, even if it has compact support
at a given time, will instantaneously spread acquiring a tail
extending over the whole of space. If one sticks to the probabilistic
interpretation and one accepts the completeness of the description of
the states of physical systems in terms of the wave function, the tail
problem cannot be avoided.

The solution to the tails problem can only derive from abandoning
completely the probabilistic interpretation and from adopting a more
physical and realistic interpretation relating ‘what is out
there’ to, e.g., the mass density distribution over the whole
universe. In this connection, the following example will be
instructive. Take a massive sphere
of normal density and mass of about 1 kg. Classically, the mass of
this body would be totally concentrated within the radius of the
sphere, call it rrr. In QMSL, after the extremely short time
interval in which the collapse dynamics leads to a
‘regime’ situation, and if one considers a sphere with
radius r+10−5r+10−5r + 10^{-5} cm, the integral of the mass density over the
rest of space turns out to be an incredibly small fraction (of the
order of 1 over 10 to the power 1015)1015)10^{15}) of the mass of a single
proton. In such conditions, it seems quite legitimate to claim that
the macroscopic body is localised within the sphere.

However, also this quite reasonable conclusion has been questioned and
it has been claimed (Lewis 1997), that the very existence of the tails
implies that the enumeration principle (i.e., the fact that the claim
‘particle 1 is within this box & particle 2 is within this
box & … & particle nnn is within this box &
no other particle is within this box’ implies the claim
‘there are nnn particles within this box’) does not
hold, if one takes seriously the mass density interpretation of
collapse theories. This paper has given rise to a long debate which
would be inappropriate to reproduce here. 

We conclude this brief analysis by stressing once more that, in the
opinion of the present writer, all the disagreements and the
misunderstandings concerning this problem have their origin in the
fact that the idea that the probabilistic interpretation of the wave
function must be abandoned has not been fully accepted by the authors
who find some difficulties in the proposed mass density interpretation
of the Collapse Theories. For a recent reconsideration of the problem
we refer the reader to the paper by Lewis (2003).
13. The Status of Collapse Models and Recent Positions about them

We recall that, as stated in Section 3, the macro-objectification
problem has been at the centre of the most lively and most challenging
debate originated by the quantum view of natural processes. According
to the majority of those who adhere to the orthodox position such a
problem does not deserve a particular attention: classical concepts
are a logical prerequisite for the very formulation of quantum
mechanics and, consequently, the measurement process itself, the
dividing line between the quantum and the classical world, cannot and
must not be investigated, but simply accepted. This position has been
lucidly summarized by J. Bell himself (1981):

Making a virtue of necessity and influenced by positivistic and
instrumentalist philosophies, many came to hold not only that it is
difficult to find a coherent picture but that it is wrong to look for
one—if not actually immoral then certainly unprofessional


The situation has seen many changes in the course of time, and the
necessity of making a clear distinction between what is quantum and
what is classical has given rise to many proposals for ‘easy
solutions’ to the problem which are based on the possibility,
for all practical purposes (FAPP), of locating the splitting
between these two faces of reality at different levels.

Then came Bohmian mechanics, a theory which has made clear, in a lucid
and perfectly consistent way, that there is no reason of principle
requiring a dichotomic description of the world. A universal dynamical
principle runs all physical processes and even though ‘it
completely agrees with standard quantum predictions’ it implies
wave-packet reduction in micro-macro interactions and the classical
behaviour of classical objects.

As we have mentioned, the other consistent proposal, at the
nonrelativistic level, of a conceptually satisfactory solution of the
macro-objectification problem is represented by the Collapse Theories
which are the subject of these pages. Contrary to bohmian mechanics,
they are rival theory of quantum mechanics, since they make different
predictions (even though quite difficult to put into evidence)
concerning various physical processes.

Let us now analyze other recent critical positions concerning the two
just mentioned approaches (in what follows I will take advantage of
the nice analysis of a paper which I have been asked to referee and of
which I do not know the author). Various physicists have criticized
Bohm approach on the basis that, being empirically indistinguishable
from quantum mechanics, such an approach is an example of ‘bad
science’ or of ‘a degenerate research program’.
Useless to say, I do not consider such criticisms as appropriate; the
conceptual advantages and the internal consistency of the approach
render it an extremely appealing theoretical scheme (incidentally, one
should not forget that it has been just the critical investigations on
such a theory which have led Bell to derive his famous and
conceptually extremely relevant inequality). On the contrary, I am
fully convinced that to consider as acceptable a theory like the
standard one, which is incapable of accounting for the way in which it
assumes the measurement apparatuses to work, and to deal with them
introduces a postulate which plainly contradicts the other assumption
of the theory, is not a scientifically tenable position. 

This being the situation, one would think that theories like the GRW
model would be exempt from an analogous charge, since they actually
are (in principle) empirically different from the standard theory. For
instance they disagree from such a theory since they forbid the
occurrence of macroscopic massive entangled states. In spite of this,
they have been the object of an analogous attack by the adherents to
the ‘new orthodoxy’ (Bub 1997; Joos et al. 1996; Zurek,
1993) pointing out that environmental induced decoherence shows that,
FAPP, collapse theories are simply phenomenological accounts of the
reduced state to which one has to resort since one has no control of
the degrees of freedom of the environment. When one takes such a
position, one is claiming that, essentially, GRW cannot be taken as a
fundamental description of nature, mainly because it suffers from the
limitation of being empirically indistinguishable from the standard
theory, provided such a theory is correctly applied taking into
account the actual physical situation. Also in this case, and even at
the level at which such an analysis is performed, the practical
indistinguishability from the standard approach should not be regarded
as a sufficient reason to not take seriously collapse models. In fact,
there are many very well known and compelling reasons (see, e.g.,
Bassi and Ghirardi 2000; Adler 2003) to prefer a logically consistent
unified theory to one which makes sense only due to the alleged
practical impossibility of detecting the superpositions of
macroscopically distinguishable states. At any rate, in principle,
such theories can be tested against the standard one and it seems that
such a challenge is already under investigation. .

But this is not the whole story. Another criticism, aimed to
‘deny’ the potential interest of collapse theories makes
reference to the fact that within any such theory the ensuing dynamics
for the statistical operator can be considered as the reduced dynamics
deriving from a unitary (and, consequently, essentially a standard
quantum) dynamics for the states of an enlarged Hilbert space of a
composite quantum system S+ES+ES+E involving, besides the physical
system SSS of interest, an ancilla EEE whose degrees of freedom
are completely unaccessible: due to the quantum dynamical semigroup
nature of the evolution equation for the statistical operator, any
GRW-like model can always be seen as a phenomenological model deriving
from a standard quantum evolution on a larger Hilbert space. In this
way, the unitary deterministic evolution characterizing quantum
mechanics would be fully restored.

Apart from the obvious remark that such a critical attitude completely
fails to grasp—and indeed, purposefully ignores—the most
important feature of collapse theories, i.e., of dealing with
individual quantum systems and not with statistical ensembles and of
yielding a perfectly satisfactory description, matching our
perceptions concerning individual macroscopic systems,
invoking an unaccessible ancilla to account for the nonlinear and
stochastic character of GRW-type theories is once more a purely verbal
way of avoiding facing the real puzzling aspects of the quantum
description of macroscopic systems. This is not the only negative
aspect of such a position; any attempt considering legitimate to
introduce unaccessible entities in the theory, when one takes into
consideration that there are infinitely possible and inequivalent ways
of doing this, amounts really to embarking oneself in a
‘degenerate research program’.

Other reasons for ignoring the dynamical reduction program have been
put forward recently by the community of scientists involved in the
interesting and exciting field of quantum information. We will not
spend too much time in analyzing and discussing the new position about
the foundational issues which have motivated the elaboration of
collapse theories. The crucial fact is that, from this perspective,
one takes the theory not to be about something real ‘occurring
out there’ in a real word, but simply about information. This
point is made extremely explicit in a recent paper (Zeilinger
2005):

information is the most basic notion of quantum mechanics, and it is
information about possible measurement results that is represented in
the quantum state. Measurement results are nothing more than states of
the classical apparatus used by the experimentalist. The quantum
system then is nothing other than the consistently constructed
referent of the information represented in the quantum state.


It is clear that if one takes such a position almost all motivations
to be worried by the measurement problem disappear, and with them the
reasons to work out what Bell has denoted as ‘an exact version
of quantum mechanics’. The most appropriate reply to this type
of criticisms is to recall that J. Bell (1990) has included
‘information’ among the words which must have no place in
a formulation with any pretension to physical precision. In particular
he has stressed that one cannot even mention information unless one
has given a precise answer to the two following questions: Whose
information? and Information about what?

A much more serious attitude is to call attention, as many serious
authors do, to the fact that since collapse theories represent rival
theories with respect to standard quantum mechanics they lead to the
identification of experimental situations which would allow, in
principle, crucial tests to discriminate between the two. As we have
discussed above, presently, fully discriminating tests seem not to be
completely out of reach.
14. Summary

We hope to have succeeded in giving a clear picture of the ideas, the
implications, the achievements and the problems of the DRP. We
conclude by stressing once more our position with respect to the
Collapse Theories. Their interest derives entirely from the fact that
they have given some hints about a possible way out from the
difficulties characterizing standard quantum mechanics, by proving
that explicit and precise models can be worked out which agree with
all known predictions of the theory and nevertheless allow, on the
basis of a universal dynamics governing all natural processes, to
overcome in a mathematically clean and precise way the basic problems
of the standard theory. In particular, the Collapse Models show how
one can work out a theory that makes perfectly legitimate to take a
macrorealistic position about natural processes, without contradicting
any of the experimentally tested predictions of standard quantum
mechanics. Finally, they might give precise hints about where to look
in order to put into evidence, experimentally, possible violations of
the superposition principle.
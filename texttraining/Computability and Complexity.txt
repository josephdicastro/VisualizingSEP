A mathematical problem is computable if it can be solved in
principle by a computing device. Some common synonyms for
“computable” are “solvable”,
“decidable”, and “recursive”. Hilbert believed
that all mathematical problems were solvable, but in the 1930’s
Gödel, Turing, and Church showed that this is not the case. There
is an extensive study and classification of which mathematical
problems are computable and which are not. In addition, there is an
extensive classification of computable problems into computational
complexity classes according to how much computation—as a
function of the size of the problem instance—is needed to
answer that instance. It is striking how clearly, elegantly, and
precisely these classifications have been drawn. 
 
1. What can be computed in principle? Introduction and History


In the 1930’s, well before there were computers, various
mathematicians from around the world invented precise, independent
definitions of what it means to be computable. Alonzo Church defined
the Lambda calculus, Kurt Gödel defined Recursive functions,
Stephen Kleene defined Formal systems, Markov defined what became known
as Markov algorithms, Emil Post and Alan Turing defined abstract
machines now known as Post machines and Turing machines.


Surprisingly, all of these models are exactly equivalent: anything
computable in the lambda calculus is computable by a Turing machine and
similarly for any other pairs of the above computational systems. After
this was proved, Church expressed the belief that the intuitive notion
of “computable in principle” is identical to the above precise notions.
This belief, now called the “Church-Turing Thesis”, is uniformly
accepted by mathematicians.


Part of the impetus for the drive to codify what is computable came
from the mathematician David Hilbert. Hilbert believed that all of
mathematics could be precisely axiomatized. He felt that once this was
done, there would be an “effective procedure”, i.e., an algorithm that
would take as input any precise mathematical statement, and, after a
finite number of steps, decide whether the statement was true or false.
Hilbert was asking for what would now be called a decision
procedure for all of mathematics.


As a special case of this decision problem, Hilbert considered the
validity problem for first-order logic. First-order logic is a
mathematical language in which most mathematical statements can be
formulated. Every statement in first-order logic has a precise meaning
in every appropriate logical structure, i.e., it is true or false in
each such structure. Those statements that are true in every
appropriate structure are called valid. Those statements that
are true in some structure are called satisfiable. Notice that
a formula, φφ\varphi, is valid iff its negation, ¬φ¬φ\neg \varphi, is not
satisfiable.


Hilbert called the validity problem for first-order logic, the
entscheidungsproblem. In a textbook, Principles of
Mathematical Logic by Hilbert and Ackermann, the authors wrote,
“The Entscheidungsproblem is solved when we know a procedure that
allows for any given logical expression to decide by finitely many
operations its validity or satisfiability.… The
entscheidungsproblem must be considered the main problem of
mathematical logic.” (Böerger, Grädel, & Gurevich
1997).


In his 1930 Ph.D. thesis, Gödel presented a complete
axiomatization of first-order logic, based on the Principia
Mathematica by Whitehead and Russell (Gödel 1930). Gödel proved his
Completeness Theorem, namely that a formula is provable from the axioms
if and only if it is valid. Gödel’s Completeness theorem was a
step towards the resolution of Hilbert’s
entscheidungsproblem.


In particular, since the axioms are easily recognizable, and rules
of inference very simple, there is a mechanical procedure that can list
out all proofs. Note that each line in a proof is either an axiom, or
follows from previous lines by one of the simple rules. For any given
string of characters, we can tell if it is a proof. Thus we can
systematically list all strings of characters of length 1, 2, 3, and so
on, and check whether each of these is a proof. If so, then we can add
the proof’s last line to our list of theorems. In this way, we can list
out all theorems, i.e., exactly all the valid formulas of first-order
logic, can be listed out by a simple mechanical procedure. More
precisely, the set of valid formulas is the range of a computable
function. In modern terminology we say that the set of valid formulas
of first-order logic is recursively enumerable (r.e.).


Gödel’s Completeness theorem was not sufficient, however, to
give a positive solution to the entscheidungsproblem. Given a
formula, φφ\varphi, if φφ\varphi is valid then the above procedure would
eventually list it out and thus could answer, “Yes, φφ\varphi is valid.”
However, if φφ\varphi were not valid then we might never find this fact
out. What was missing was a procedure to list out all the non-valid
formulas, or equivalently to list out all satisfiable formulas.


A year later, in 1931, Gödel shocked the mathematical world by
proving his Incompleteness Theorem: there is no complete and computable
axiomatization of the first-order theory of the natural numbers. That
is, there is no reasonable list of axioms from which we can prove
exactly all true statements of number theory (Gödel 1931).


A few years later, Church and Turing independently proved that the
entscheidungsproblem is unsolvable. Church did this by using
the methods of Gödel’s Incompleteness Theorem to show that the set
of satisfiable formulas of first-order logic is not r.e., i.e., they
cannot be systematically listed out by a function computable by the
lambda calculus. Turing introduced his machines and proved many
interesting theorems some of which we will discuss in the next section.
In particular, he proved the unsolvability of the halting
problem. He obtained the unsolvability of the
entscheidungsproblem as a corollary.


Hilbert was very disappointed because his program towards a decision
procedure for all of mathematics was proved impossible. However, as we
will see in more detail in the rest of this article, a vast amount was
learned about the fundamental nature of computation.
2. Turing Machines


In his 1936 paper, “On Computable Numbers, with an Application to
the Entscheidungsproblem”, Alan Turing introduced his machines and
established their basic properties.


He thought clearly and abstractly about what it would mean for a
machine to perform a computational task. Turing defined his machines to
consist of the following:

a finite set, QQQ, of possible states, because any device
must be in one of finitely many possible states;
a potentially infinite tape, consisting of consecutive cells,
σ1,σ2,σ3σ1,σ2,σ3\sigma_{1}, \sigma_{2}, \sigma_{3}, from
some finite alphabet, ΣΣ\Sigma;

(Σ(Σ(\Sigma may be any finite set containing at least two symbols. It is
convenient to fix Σ={0,1,b}Σ={0,1,b}\Sigma = \{0, 1, b\} consisting of the binary
alphabet plus the blank cell symbol. We usually assume that a finite
initial segment of the tape contains binary symbols, and the rest is
blank.)
a read/write tape head, h≥1h≥1h \ge 1, scanning tape cell
σhσh\sigma_{h}; and finally,
a transition function, δ:Q×Σ→Q×Σ×{−1,0,1}δ:Q×Σ→Q×Σ×{−1,0,1}\delta : Q \times \Sigma \rightarrow Q
\times \Sigma \times \{-1,0,1\}.   (The meaning of the
transition function is that from any given state, qqq, looking at
any given symbol, σh,δσh,δ\sigma_{h}, \delta tells us the new state the
machine should enter, the new symbol that should be written in the
current square, and the new head position, h′=h+dh′=h+dh' = h + d, where d∈{−1,0,1}d∈{−1,0,1}d
\in \{-1,0,1\} is the displacement given by δδ\delta.)



The linear nature of its memory tape, as opposed to random access
memory, is a limitation on computation speed but not power: a Turing
machine can find any memory location, i.e., tape cell, but this may be
time consuming because it has to move its head step by step along its
tape.


The beauty of Turing machines is that the model is extremely simple,
yet nonetheless, extremely powerful. A Turing machine has potentially
infinite work space so that it can process arbitrarily large inputs,
e.g., multiply two huge numbers, but it can only read or write a
bounded amount of information, i.e., one symbol, per step. Even before
Turing machines and all the other mathematical models of computation
were proved equivalent, and before any statement of the Church-Turing
thesis, Turing argued convincingly that his machines were as powerful
as any possible computing device.
2.1 Universal Machines


Each Turing machine can be uniquely described by its transition
table: for each state, qqq, and each symbol, σ,δ(q,σ)σ,δ(q,σ)\sigma ,
\delta(q,\sigma) is the new state, the new symbol, and the
head displacement. These transition tables, can be written as a finite
string of symbols, giving the complete set of instructions of each
Turing machine. Furthermore, these strings of symbols can be listed in
lexicographic order as follows: M1,M2,M3,…M1,M2,M3,…M_{1},
M_{2}, M_{3},\ldots, where
MiMiM_{i} is the transition table, i.e., the complete set
of instructions, for Turing machine number iii. The transition
table for MiMiM_{i} is the program for Turing machine
iii, or more simply, the iiith program.


Turing showed that he could build a Turing machine, UUU, that
was universal, in the sense that it could run the program of
any other Turing machine. More explicitly, for any iii, and any
input w,Uw,Uw, U on inputs iii and www would
do exactly what MiMiM_{i} would do on input www, in
symbols,
U(i,w)=Mi(w)U(i,w)=Mi(w)
U(i,w) = M_{i}(w)



Turing’s construction of a universal machine gives the most
fundamental insight into computation: one machine can run any program
whatsoever. No matter what computational tasks we may need to perform
in the future, a single machine can perform them all. This is the
insight that makes it feasible to build and sell computers. One
computer can run any program. We don’t need to buy a new computer every
time we have a new problem to solve. Of course, in the age of personal
computers, this fact is such a basic assumption that it may be
difficult to step back and appreciate it.
2.2 The Halting Problem


Because they were designed to embody all possible computations,
Turing machines have an inescapable flaw: some Turing machines on
certain inputs never halt. Some Turing machines do not halt for silly
reasons, for example, we can mis-program a Turing machine so that it
gets into a tight loop, for example, in state 17 looking at a 1 it
might go to state 17, write a 1 and displace its head by 0. Slightly
less silly, we can reach a blank symbol, having only blank symbols to
the right, and yet keep staying in the same state, moving one step to
the right, and looking for a “1”. Both of those cases of non-halting
could be easily detected and repaired by a decent compiler. However,
consider the Turing machine MFMFM_{F}, which on
input “0”, systematically searches for the first counter-example to
Fermat’s last theorem, and upon finding it outputs the counter-example
and halts. Until Andrew Wiles relatively recently proved Fermat’s Last
Theorem, all the mathematicians in the world, working for over three
centuries, were unable to decide whether or not
MFMFM_{F} on input “0” eventually halts. Now we
know that it never does.
2.3 Computable Functions and Enumerability


Since a Turing machine might not halt on certain inputs, we have to
be careful in how we define functions computable by Turing machines.
Let the natural numbers, NN\mathbf{N}, be the set
{0,1,2,…}{0,1,2,…}\{0,1,2,\ldots \} and let us consider Turing machines as partial
functions from NN\mathbf{N} to NN\mathbf{N}.


Let MMM be a Turing machine and nnn a natural number.
We say that MMM’s tape contains the number
nnn, if MMM’s tape begins with a binary
representation of the number nnn (with no unnecessary leading
0’s) followed by just blank symbols from there on.


If we start the Turing machine MMM on a tape containing
nnn and it eventually halts with its tape containing mmm,
then we say that MMM on input nnn, computes m:M(n)=mm:M(n)=mm:
M(n) = m. If, when we start MMM on
input nnn, it either never halts, or when it halts, its tape
does not contain a natural number, e.g., because it has leading 0’s, or
digits interspersed with blank symbols, then we say that
M(n)M(n)M(n) is undefined, in symbols: M(n)=↗M(n)=↗M(n)
=\nearrow.
 We can thus
associate with each Turing machine, MMM, a partial
function, M:N→N∪{↗}M:N→N∪{↗}M: \mathbf{N} \rightarrow \mathbf{N} \cup \{\nearrow\}. We say that
the function MMM is total if for all
n∈Nn∈Nn\in \mathbf{N}, M(n)∈NM(n)∈NM(n) \in \mathbf{N}, i.e., M(n)(n)(n) is always defined.


Now we can formally define what it means for a set to be
recursively enumerable (r.e.) which we earlier described
informally. Let S⊆NS⊆NS \subseteq \mathbf{N}. Then SSS
is r.e. if and only if there is some Turing machine, MMM, such
that SSS is the image of the function computed by MMM, in
symbols,
S={M(n)∣n∈N;M(n)≠↗}.S={M(n)∣n∈N;M(n)≠↗}.
S = \{M(n) \mid n \in \mathbf{N}; M(n) \ne\nearrow\}.



Thus, SSS is r.e. just if it can be listed out by some Turing
machine. Suppose that SSS is r.e. and its elements are
enumerated by Turing machine MMM as above. We can then describe
another Turing machine, PPP, which, on input nnn, runs
MMM in a round-robin fashion on all its possible inputs until
eventually MMM outputs nnn. If this happens then
PPP halts and outputs “1”, i.e., P(n)=1P(n)=1P(n)=1. If
n∉Sn∉Sn \not\in S, then
MMM will never output nnn, so P(n)P(n)P(n) will
never halt, i.e., P(n)=↗P(n)=↗P(n)=\nearrow.



Let the notation P(n)↓P(n)↓P(n)\downarrow mean that Turing
machine PPP on input nnn eventually halts. For a Turing
machine, PPP, define L(P)L(P)L(P), the set
accepted by PPP, to be those numbers nnn such that
PPP on input nnn eventually halts,
L(P)={n∣P(n)↓}.L(P)={n∣P(n)↓}.
L(P) = \{n \mid P(n)\downarrow \}.



The above argument shows that if a set SSS is r.e. then it is
accepted by some Turing machine, PPP, i.e., S=L(P)S=L(P)S = L(P). The
converse of this statement holds as well.  That is, SSS is r.e. if
and only if it is accepted by some Turing machine, PPP.


We say that a set, SSS, is decidable if and only if there
is a total Turing machine, MMM, that decides for all n∈Nn∈Nn \in
\mathbf{N} whether or not n∈Sn∈Sn \in S. Think of “1” as
“yes” and “0” as “no”. For all
n∈Nn∈Nn\in \mathbf{N}, if n∈Sn∈Sn \in S, then M(n)=1M(n)=1M(n) = 1, i.e., MMM on
input nnn eventually halts and outputs “yes”, whereas if
n∉Sn∉Sn \not\in S, then M(n)=0M(n)=0M(n) = 0, i.e., MMM on input nnn
eventually halts and outputs “no”. Synonyms for decidable
are:
computable, solvable, and recursive.


For S⊆NS⊆NS \subseteq \mathbf{N}, the complement of
SSS is N−SN−S\mathbf{N} - S, i.e., the set of all
natural numbers not in SSS. We say that the set SSS is
co-r.e. if and only if its complement is r.e. If a set,
SSS, is r.e. and co-r.e. then we can list out all of its
elements in one column and we can list out all of its non-elements in a
second column. In this way we can decide whether or not a given
element, nnn, is in SSS: just scan the two columns and
wait for nnn to show up. If it shows up in the first column then
n∈Sn∈Sn \in S. Otherwise it will show up in the second
column and n∉Sn∉Sn \not\in S.
In fact, a set is recursive iff it is r.e. and co-r.e.
2.4 The Unsolvability of the Halting Problem


Turing asked whether every set of natural numbers is decidable. It
is easy to see that the answer is, “no”, by the following counting
argument. There are uncountably many subsets of NN\mathbf{N}, but
since there are only countably many Turing machines, there can be only
countably many decidable sets. Thus almost all sets are
undecidable.


Turing actually constructed a non-decidable set. As we will see, he
did this using a diagonal argument. The diagonal argument goes back to
Georg Cantor who used it to show that the real numbers are uncountable.
Gödel used a similar diagonal argument in his proof of the
Incompleteness Theorem in which he constructed a sentence, JJJ,
in number theory whose meaning could be understood to be, “JJJ
is not a theorem.”


Turing constructed a diagonal halting set, KKK, as
follows:
K={n∣Mn(n)↓}.K={n∣Mn(n)↓}.
K = \{n \mid M_{n}(n)\downarrow \}.



That is, KKK consists of those Turing machines that
eventually halt when input their own programs.


It is not hard to see that KKK is r.e. Suppose for the sake
of a contradiction that KKK is also co-r.e., and let ddd
be the number of a Turing machine that accepts the complement of
KKK. That is, for any nnn,
n∉K⇔Md(n)↓n∉K⇔Md(n)↓
n \not\in K \Leftrightarrow M_{d}(n)\downarrow



But consider what happens when we substitute ddd for
nnn in the above equation:
d∉K⇔Md(d)↓.d∉K⇔Md(d)↓.
d \not\in K \Leftrightarrow M_{d}(d)\downarrow.



However, the definition of KKK tells us that:
d∈K⇔Md(d)↓.d∈K⇔Md(d)↓.
d \in K \Leftrightarrow M_{d}(d)\downarrow .



Thus we have that
d∈K⇔d∉K,d∈K⇔d∉K,
d \in K \Leftrightarrow d \not\in K,



which is a contradiction. Thus our assumption that KKK is
co-r.e. is false. Thus KKK is not recursive. It follows that it
is not a computable problem to be given a Turing machine and its input
and to decide whether or not the Turing machine will eventually halt on
that input, i.e., the halting problem is unsolvable.
3. Primitive Recursive Functions


We next define the class of Primitive Recursive Functions.
This is a very interesting class of functions 
described in paper by Skolem (1923) and used by
Gödel in
his proof of the Incompleteness Theorem. We are interested in
functions fff from NrNr\mathbf{N}^{r} to
NN\mathbf{N}, for r=0,1,2,…r=0,1,2,…r = 0, 1, 2,\ldots . Here
rrr is called the arity of the function fff, i.e., the
number of arguments that it takes. Gödel started with three very
simple functions, the initial functions, and two natural closure
operations, composition and primitive recursion, each of which take
some already defined functions and use them to define a new one. We
next explain his definitions in detail. This section is technical and
can be safely skipped. The important idea is that the primitive
recursive functions comprise a very large and powerful class of
computable functions, all generated in an extremely simple way.


We begin with the three initial primitive recursive
functions:

ζζ\zeta, the zero function of arity 0,ζ0,ζ0, \zeta( ) =0=0= 0;
ηη\eta, the identity function of arity 1,η(n)=n1,η(n)=n1, \eta(n) = n; and,
σσ\sigma, the successor function of arity 1,σ(n)=n+11,σ(n)=n+11, \sigma(n) = n +1.



Now consider the following two operations:

Composition: if fff is a primitive recursive
function of arity aaa, and g1,…,gag1,…,gag_{1}, \ldots ,g_{a} are primitive
recursive functions of arities r1,…,rar1,…,rar_{1}, \ldots ,r_{a}, and k∈Nk∈Nk\in
\mathbf{N}, then the following is a primitive recursive function of
arity kkk:

h(x1,…,xk)=f(g1(w1),…,ga(wa)),h(x1,…,xk)=f(g1(w1),…,ga(wa)),
h(x_{1}, \ldots ,x_{k}) = f(g_{1}(w_{1}), \ldots ,g_{a}(w_{a})),



where each wiwiw_{i} is a list of ririr_{i} arguments, perhaps with
repetition, from x1,…,xkx1,…,xkx_{1}, \ldots ,x_{k}; and,
Primitive recursion: if fff and ggg
are primitive recursive functions of arity kkk and k+2k+2k+2,
respectively, then there is a primitive recursive function, hhh,
of arity k+1k+1k+1 satisfying the following conditions: 

h(0,x1,…,xk)h(n+1,x1,…,xk)=f(x1,…,xk); and,=g(h(n,x1,…,xk),n,x1,…,xk).h(0,x1,…,xk)=f(x1,…,xk); and,h(n+1,x1,…,xk)=g(h(n,x1,…,xk),n,x1,…,xk).\begin{align}

 h(0,x_{1},\ldots ,x_{k}) &amp;= f(x_{1},\ldots ,x_{k}); \text{ and,} \\

 h(n+1,x_{1},\ldots ,x_{k}) &amp;= g(h(n,x_{1},\ldots ,x_{k}), n,x_{1},\ldots ,x_{k}).

\end{align}



Here composition is the natural way to combine functions, and
primitive recursion is a restricted kind of recursion in which
hhh with first argument n+1n+1n+1 is defined in terms of
hhh with first argument nnn, and all the other arguments
unchanged. 


Define the primitive recursive functions to be the smallest
class of functions that contains the Initial functions and is closed
under Composition and Primitive Recursion. The set of primitive
recursive functions is equal to the set of functions computed using
bounded iteration (Meyer & Ritchie 1967), i.e. the set of
functions definable in the language Bloop from (Hofstadter 1979).


The primitive recursive functions have a very simple definition and
yet they are extremely powerful. Gödel proved inductively that
every primitive recursive function can be simply represented in
first-order number theory. He then used the primitive recursive
functions to encode formulas and even sequences of formulas by numbers.
He finally used the primitive recursive functions to compute properties
of the represented formulas including that a formula was well formed, a
sequence of formulas was a proof, and that a formula was a theorem.


It takes a long series of lemmas to show how powerful the primitive
recursive functions are. The following are a few examples showing that
addition, multiplication, and exponentiation are primitive
recursive.


Define the addition function, P(x,y)P(x,y)P(x,y), as
follows:
P(0,y)P(n+1,y)=η(y)=σ(P(n,y))P(0,y)=η(y)P(n+1,y)=σ(P(n,y))\begin{align}
   P(0,y)  &amp;= \eta(y) \\
  P(n+1,y) &amp;= \sigma(P(n,y))
\end{align}


(Note that this fits into the definition of primitive recursion
because the function g(x1,x2,x3)=η(σ(x1))g(x1,x2,x3)=η(σ(x1))g(x_{1},x_{2},x_{3}) = \eta(\sigma(x_{1})) is
definable from the initial functions ηη\eta and σσ\sigma by
composition.)


Next, define the multiplication function,
T(x,y)T(x,y)T(x,y), as follows:
T(0,y)T(n+1,y)=ζ( )=P(T(n,y),y).T(0,y)=ζ( )T(n+1,y)=P(T(n,y),y).\begin{align}
   T(0,y) &amp;= \zeta(\ ) \\
 T(n+1,y) &amp;= P(T(n,y),y).
\end{align}


Next, we define the exponential function,
E(x,y)E(x,y)E(x,y). (Usually 00000^{0} is
considered undefined, but since primitive recursive functions must be
total, we define EEE(0,0) to be 1.) Since
primitive recursion only allows us to recurse on the first argument,
we use two steps to define the exponential function:
R(0,y)R(n+1,y)=σ(ζ( ))=T(R(n,y),y).R(0,y)=σ(ζ( ))R(n+1,y)=T(R(n,y),y).\begin{align}
  R(0,y) &amp;= \sigma(\zeta(\ )) \\
R(n+1,y) &amp;= T(R(n,y),y).
\end{align}


Finally we can define E(x,y)=R(η(y),η(x))E(x,y)=R(η(y),η(x))E(x,y) = R(\eta(y),\eta(x)) by
composition. (Recall that ηη\eta is the identity function so this
could be more simply written as E(x,y)=R(y,x)E(x,y)=R(y,x)E(x,y) = R(y,x).)


The exponential function, EEE, grows very rapidly, for
example, EEE(10,10) is ten billion, and EEE(50,50) is
over 1084108410^{84} (and thus significantly more than the estimated
number of atoms in the universe). However, there are much faster
growing primitive recursive functions. As we saw, EEE was
defined from the slowly growing function, σσ\sigma, using three
applications of primitive recursion: one for addition, one for
multiplication, and then one more for exponentiation. We can continue
to apply primitive recursion to build a series of unimaginably fast
growing functions. Let’s do just one more step in the series defining
the hyper-exponential function, H(n,m)H(n,m)H(n,m) as 2
to the 2 to the 2 to the … to the mmm, with a tower of
nnn 2s. HHH is primitive recursive because it can be
defined from EEE using one more application of primitive
recursion:
H(0,y)H(n+1,y)=y=E(2,H(n,y))H(0,y)=yH(n+1,y)=E(2,H(n,y))\begin{align}
  H(0,y) &amp;= y \\
H(n+1,y) &amp;= E(2,H(n,y))
\end{align}


Thus H(2,2)=24=16,H(3,3)=2256H(2,2)=24=16,H(3,3)=2256H(2,2) = 2^{4} = 16, H(3,3) = 2^{256} is more than
1077107710^{77} and comparable to the number of atoms in the universe. If
that’s not big enough for you then consider H(4,4)H(4,4)H(4,4). To write
this number in decimal notation we would need a one, followed by more
zero’s than the number of particles in the universe.
3.1 Recursive Functions


The set of primitive recursive functions is a huge class of
computable functions. In fact, they can be characterized as the set of
functions computable in time that is some primitive recursive function
of nnn, where nnn is the length of the input. For
example, since H(n,n)H(n,n)H(n,n) is a primitive
recursive function, the primitive recursive functions include all of
TIME[H(n,n)H(n,n)H(n,n)]. (See the next section for a
discussion of computational complexity, including TIME.) Thus, the
primitive recursive functions include all functions that are feasibly
computable by any conceivable measure of feasible, and much beyond
that.


However, the primitive recursive functions do not include all
functions computable in principle. To see this, we can again use
diagonalization. We can systematically encode all definitions of
primitive recursive functions of arity 1, calling them
p1,p2,p3p1,p2,p3p_{1}, p_{2}, p_{3},
and so on.


We can then build a Turing machine to compute the value of the
following diagonal function, D(n)=pn(n)+1D(n)=pn(n)+1D(n) = p_{n}(n) + 1.


Notice that DDD is a total, computable function from
NN\mathbf{N} to NN\mathbf{N}, but it is not primitive
recursive. Why? Suppose for the sake of a contradiction that DDD
were primitive recursive. Then DDD would be equal to
pdpdp_{d} for some d∈Nd∈Nd\in \mathbf{N}. But it would then follow that
pd(d)=pd(d)+1,pd(d)=pd(d)+1,
p_{d}(d) = p_{d}(d) +1,



which is a contradiction. Therefore, DDD is not primitive
recursive.


Alas, the above diagonal argument works on any class of total
functions that could be considered a candidate for the class of all
computable functions. The only way around this, if we want all
functions computable in principle, not just in practice, is to add some
kind of unbounded search operation. This is what Gödel did to
extend the primitive recursive functions to the recursive
functions.


Define the unbounded minimization operator, μμ\mu, as follows. Let
fff be a perhaps partial function of arity k+1k+1k+1. Then
μ[fμ[f\mu[f] is defined as the following function of arity k. On
input x1,…,xkx1,…,xkx_{1}, \ldots ,x_{k} do the
following:

For i=0i=0i = 0 to ∞∞\infty do {{\{
if f(i,x1,…,xk)=1f(i,x1,…,xk)=1f(i,x_{1},\ldots ,x_{k}) = 1, then output iii

}}\}



Thus if f(i,x1,…,xk)=1f(i,x1,…,xk)=1f(i,x_{1},\ldots ,x_{k}) = 1, and for all j<i,f(j,x1,…,xk)j<i,f(j,x1,…,xk)j \lt i,
f(j,x_{1},\ldots ,x_{k}) is defined, but not equal to 1, then μ[f](x1,…,xk)=iμ[f](x1,…,xk)=i\mu
[f](x_{1}, \ldots ,x_{k}) = i. Otherwise μ[f](x1,…,xk)μ[f](x1,…,xk)\mu[f](x_{1}, \ldots
,x_{k}) is undefined.


Gödel defined the set of Recursive functions to be the
closure of the initial primitive recursive functions under composition,
primitive recursion, and μμ\mu . With this definition, the Recursive
functions are exactly the same as the set of partial functions
computable by the Lambda calculus, by Kleene Formal systems, by Markov
algorithms, by Post machines, and by Turing machines.
4. Computational Complexity: Functions Computable in Practice


During World War II, Turing helped design and build a specialized
computing device called the Bombe at Bletchley Park. He used the Bombe
to crack the German “Enigma” code, greatly aiding the
Allied cause [Hodges, 1992]. By the 1960’s computers were widely
available in industry and at universities. As algorithms were
developed to solve myriad problems, some mathematicians and scientists
began to classify algorithms according to their efficiency and to
search for best algorithms for certain problems. This was the
beginning of the modern theory of computation.


In this section we are dealing with complexity instead of
computability, and all the Turing machines that we consider will halt
on all their inputs. Rather than accepting by halting, we will assume
that a Turing machine accepts by outputting “1” and rejects by
outputting “0”, thus we redefine the set accepted by a total machine,
MMM,
L(M)={n∣M(n)=1}.L(M)={n∣M(n)=1}.
L(M) = \{n \mid M(n) = 1\}.



The time that an algorithm takes depends on the input and the
machine on which it is run. The first important insight in complexity
theory is that a good measure of the complexity of an algorithm is its
asymptotic worst-case complexity as a function of the size of the
input, nnn.


For an input, www, let n=|w|n=|w|n = \lvert w\rvert be the length of
www. Following [Hartmanis, 1989] we say that a Turing machine
MMM runs in time T(n)T(n)T(n) if for all www of length n,M(w)n,M(w)n,
M(w) takes at most T(n)T(n)T(n) steps and then halts. This is called
worst-case complexity because T(n)T(n)T(n) must be as large as the time
taken by any input of length nnn.


For any function T:N→NT:N→NT : \mathbf{N} \rightarrow \mathbf{N}, let
TIME[T(n)]={A∣A=L(M) for some M that runs in time T(n)}.TIME[T(n)]={A∣A=L(M) for some M that runs in time T(n)}.
\TIME[T(n)] = \{ A \mid A = L(M) \text{ for some }
  M \text{ that runs in time } T(n)\}.



Alan Cobham and Jack Edmonds identified the complexity class, PP\P, of
problems recognizable in some polynomial amount of time, as being an
excellent mathematical wrapper of the class of feasible 
problems—those problems all of whose moderately-sized instances can be
feasibly recognized,
P=⋃i=1,2,…TIME[ni]P=⋃i=1,2,…TIME[ni]
 \P = \bigcup_{i=1,2,\ldots} \TIME[n^{i}]



Any problem not in PP\P is certainly not feasible. On the other hand,
natural problems that have algorithms in PP\P, tend to eventually have
algorithms discovered for them that are actually feasible.


Many important complexity classes besides P have been defined and
studied; a few of these are NPNP\NP, PSPACEPSPACE\PSPACE, and EXPTIMEEXPTIME\EXPTIME.
PSPACEPSPACE\PSPACE consists of those problems solvable using some polynomial
amount of memory space. EXPTIMEEXPTIME\EXPTIME is the set of problems solvable in
time 2p(n)2p(n)2^{p(n)} for some polynomial, ppp.


Perhaps the most interesting of the above classes is NP:
nondeterministic polynomial time. The definition comes not from a real
machine, but rather a mathematical abstraction. A nondeterministic
Turing machine, NNN, makes a choice (guess) of one of two
possible actions at each step. If, on input www, some sequence
of these choices leads to acceptance, then we say that
NN\mathbf{N} accepts www, and we say the
nondeterministic time taken by NN\mathbf{N} on input
www, is just the number of steps taken in the sequence that
leads to acceptance. A nondeterministic machine is not charged for all
the other possible choices it might have made, just the single sequence
of correct choices.


NP is sometimes described as the set of problems, SSS, that
have short proofs of membership. For example, suppose we are given a
list of mmm large natural numbers: a1,…,ama1,…,ama_{1},
\ldots ,a_{m}, and a target number,
ttt. This is an instance of the Subset Sum problem: is there a
subset of the mmm numbers whose sum is exactly ttt? This
problem is easy to solve in nondeterministic linear time: for each
iii, we guess whether or not to take aiaia_{i}.
Next we add up all the numbers we decided to take and if the sum is
equal to ttt then accept. Thus the nondeterministic time is
linear, i.e., some constant times the length of the input, nnn.
However there is no known (deterministic) way to solve this problem in
time less than exponential in nnn.


There has been a large study of algorithms and the complexity of
many important problems is well understood. In particular reductions
between problems have been defined and used to compare the relative
difficulty of two problems. Intuitively, we say that AAA is
reducible to BBB (A≤B)(A≤B)(A \le B) if there
is a simple transformation, ττ\tau, that maps instances of AAA to
instances of BBB in a way that preserves membership, i.e.,
ττ\tau(w) ∈B⇔∈B⇔\in B \Leftrightarrow w ∈A∈A\in A.


Remarkably, a high percentage of naturally occurring computational
problems turn out to be complete for one of the above classes. (A
problem, AAA, is complete for a complexity class CCC if
AAA is a member of CCC and all other problems BBB
in CCC are no harder than AAA, i.e., B≤AB≤AB \le A. Two complete problems for the same class have equivalent
complexity.)


The reason for this completeness phenomenon has not been adequately
explained. One plausible explanation is that natural computational
problems tend to be universal in the sense of Turing’s universal
machine. A universal problem in a certain complexity class can simulate
any other problem in that class. The reason that the class NP is so
well studied is that a large number of important practical problems are
NP complete, including Subset Sum. None of these problems is known to
have an algorithm that is faster than exponential time, although some
NP-complete problems admit feasible approximations to their
solutions.


A great deal remains open about computational complexity. We know that
strictly more of a particular computational resource lets us solve
strictly harder problems, e.g. TIME[n]TIME[n]\TIME[n] is strictly contained in
TIME[n1.01]TIME[n1.01]\TIME[n^{1.01}] and similarly for SPACESPACE\SPACE and other
measures. However, the trade-offs between different computational
resources is still quite poorly understood. It is obvious that PP\P
is contained in NPNP\NP. Furthermore, NPNP\NP is contained in
PSPACEPSPACE\PSPACE because in PSPACEPSPACE\PSPACE we can systematically try every
single branch of an NPNP\NP computation, reusing space for the
successive branches, and accepting if any of these branches lead to
acceptance. PSPACEPSPACE\PSPACE is contained in EXPTIMEEXPTIME\EXPTIME because if a
PSPACEPSPACE\PSPACE machine takes more than exponential time, then it has
exactly repeated some configuration so it must be in an infinite
loop. The following are the known relationships between the above
classes:
P⊆NP⊆PSPACE⊆EXPTIMEP⊆NP⊆PSPACE⊆EXPTIME
\P \subseteq \NP \subseteq \PSPACE \subseteq \EXPTIME



However, while it seems clear that PP\P is strictly contained in
NPNP\NP, that NPNP\NP is strictly contained in PSPACEPSPACE\PSPACE, and that
PSPACEPSPACE\PSPACE is strictly contained in EXPTIMEEXPTIME\EXPTIME, none of these
inequalities has been proved. In fact, it is not even known that
PP\P is different from PSPACEPSPACE\PSPACE, nor that NPNP\NP is different
from EXPTIMEEXPTIME\EXPTIME. The only known proper inclusion from the above is
that PP\P is strictly contained in EXPTIMEEXPTIME\EXPTIME. The remaining
questions concerning the relative power of different computational
resources are fundamental unsolved problems in the theory of
computation.


There is an extensive theory of computational complexity.  This entry
briefly describes the area, putting it into the context of the
question of what is computable in principle versus in practice.  For
readers interested in learning more about complexity, there are
excellent books, for example, [Papadimitriou, 1994] and [Arora and
Barak, 2009]. There is also the entry on
 Computational Complexity Theory.

4.1 Significance of Complexity


The following diagram  maps out all the
complexity classes we have discussed and a few more as well.  The diagram comes from work in
Descriptive Complexity [Immerman, 1999] which 
shows that all important complexity 
classes have descriptive characterizations.  Fagin began this field by proving that NP = 
SO∃∃\exists, i.e., a property is in NP iff it is expressible in second-order existential logic
[Fagin, 1974].


Vardi and the author of this entry later independently proved that P =
FO(LFP): a property is in P iff it is expressible in first-order logic
plus a least fixed-point operator (LFP) which formalizes the power to
define new relations by induction.  A captivating corollary of this is
that P = NP iff SO = FO(LFP).  That is, P is equal to NP iff every
property expressible in second order logic is already expressible in
first-order logic plus inductive definitions. (The languages in
question are over finite ordered input structures.  See [Immerman,
1999] for details.)


The World of Computability and Complexity



The top right of the diagram shows the recursively enumerable (r.e.)
problems; this includes r.e.-complete problems such as the halting
problem (Halt). On the left is the set of co-r.e. problems including
the co-r.e.-complete problem Halt¯¯¯¯¯¯¯¯¯¯Halt¯\overline{{\rm Halt}} -- the set of
Turing Machines that never halt on a given input.  We mentioned at the
end of Section 2.3 that the intersection of the set of r.e problems
and the set of co-r.e problems is equal to the set of Recursive
problems.  The set of Primitive Recursive problems is a strict subset
of the Recursive problems.


Moving toward the bottom of the diagram, there is a region marked with
a green dotted line labelled “truly feasible”.  Note that
this is not a mathematically defined class, but rather an intuitive
notion of those problems that can be solved exactly, for all the
instances of reasonable size, within a reasonable amount of time,
using a computer that we can afford. (Interestingly, as the speed of
computers has dramatically increased over the years, our expectation
of how large an instance we should be able to handle has increased
accordingly.  Thus, the boundary of what is “truly
feasible” changes more slowly than the increase of computer
speed might suggest.)


As mentioned before, P is a good mathematical wrapper for the set of
feasible problems.  There are problems in P requiring n1,000n1,000n^{1,000}
time for problems of size nnn and thus not feasible.  Nature appears
to be our friend here, which is to say naturally occurring problems in
P favor relatively simple algorithms, and “natural”
problems tend to be feasible. The number of steps required for
problems of size nnn tends to be less than cnkcnkc n^k with small
multiplicative constants ccc, and very small exponents, kkk, i.e.,
k≤2k≤2k\leq 2.


In practice the asympototic complexity of naturally occurring problems
tends to be the key issue determining whether or not they are
feasible.  A problem with complexity 17n17n17n can be handled in under a
minute on modern computers, for every instance of size a
billion.  On the other hand, a problem with worst-case complexity
2n2n2^n cannot be handled in our lifetimes for some instance of
size a hundred.


Remarkably, natural problems tend to be complete for important
complexity classes, namely the ones in the diagram and only a very few
others.  This fascinating phenomenon means that algorithms and
complexity are more than abstract concepts; they are important at a
practical level.  We have had remarkable success in proving that our
problem of interest is complete for a well-known complexity class.  If
the class is contained in P, then we can usually just look up a known
efficient algorithm.  Otherwise, we must look at simplifications or
approximations of our problem which may be feasible.


There is a rich theory of the approximability of NP optimization
problems (See [Arora & Barak, 2009]).  For example, the Subset Sum
problem mentioned above is an NP-complete problem.  Most likely it
requires exponential time to tell whether a given Subset Sum problem
has an exact solution.  However, if we only want to see if we can
reach the target up to a fixed number of digits of accuracy, then the
problem is quite easy, i.e., Subset Sum is hard, but very easy to
approximate.


Even the r.e.-complete Halting problem has many important feasible
subproblems.  Given a program, it is in general not possible to figure
out what it does and whether or not it eventually halts.  However,
most programs written by programmers or students can be automatically
analyzed, optimized and even corrected by modern compilers and model
checkers.


The class NP is very important practically and philosophically.  It is
the class of problems, SSS, such that any input www is in SSS
iff there is a proof, p(w)p(w)p(w), that w∈Sw∈Sw\in S and p(w)p(w)p(w) is not
much larger than www.  Thus, very informally, we can think of NP has
the set of intellectual endeavors that may be in reach: if we find the
answer to whether w∈Sw∈Sw \in S, we can convince others that we have
done so.


The boolean satisfiability problem, SAT, was the first problem proved
NP complete [Cook, 1971], i.e., it is a hardest NP problem.  The fact
that SAT is NP complete means that all problems in NP are reducible to
SAT.  Over the years, researchers have built very efficient SAT
solvers which can quickly solve many SAT instances – i.e., find a
satisfying assignment or prove that there is none
-- even for instances with millions of variables.  Thus, SAT solvers are being used as general purpose
problem solvers.  On the other hand, there are known classes of small instances for which current
SAT solvers fail.  Thus part of the P versus NP question concerns the practical and theoretical
complexity of SAT [Nordström, 2015].
Computational complexity theory is a subfield of theoretical computer science one of whose primary goals is to classify and compare the practical difficulty of solving problems about finite combinatorial objects – e.g. given two natural numbers nnn and mmm, are they relatively prime? Given a propositional formula ϕϕ\phi, does it have a satisfying assignment? If we were to play chess on a board of size n×nn×nn \times n, does white have a winning strategy from a given initial position? These problems are equally difficult from the standpoint of classical  computability theory in the sense that they are all effectively decidable. Yet they still appear to differ significantly in practical difficulty. For having been supplied with a pair of numbers m>n>0m>n>0m \gt n \gt 0, it is possible to determine their relative primality by a method (Euclid’s algorithm) which requires a number of steps proportional to log(n)log(n)\log(n). On the other hand, all known methods for solving the latter two problems require a ‘brute force’ search through a large class of cases which increase at least exponentially in the size of the problem instance.
Complexity theory attempts to make such distinctions precise by proposing a formal criterion for what it means for a mathematical problem to be feasibly decidable – i.e. that it can be solved by a conventional Turing machine in a number of steps which is proportional to a polynomial function of the size of its input. The class of problems with this property is known as PP\textbf{P} – or polynomial time – and includes the first of the three problems described above. P\textbf{P} can be formally shown to be distinct from certain other classes such as EXP\textbf{EXP} – or exponential time – which includes the third problem from above. The second problem from above belongs to a complexity class known as NP\textbf{NP} – or non-deterministic polynomial time – consisting of those problems which can be correctly decided by some computation of a non-deterministic Turing machine in a number of steps which is a polynomial function of the size of its input. A famous conjecture – often regarded as the most fundamental in all of theoretical computer science – states that P\textbf{P} is also properly contained in NP\textbf{NP} – i.e. P⊊NP\textbf{P} \subsetneq \textbf{NP}.
Demonstrating the non-coincidence of these and other complexity classes remain important open problems in complexity theory. But even in its present state of development, this subject connects many topics in logic, mathematics, and surrounding fields in a manner which bears on the nature and scope of our knowledge of these subjects. Reflection on the foundations of complexity theory is thus of potential significance not only to the philosophy of computer science, but also to philosophy of mathematics and epistemology as well.
 
1. On computational complexity
Central to the development of computational complexity theory is the notion of a decision problem. Such a problem corresponds to a set XX in which we wish to decide membership. For instance the problem PRIMES\sc{PRIMES} corresponds to the subset of the natural numbers which are prime – i.e. {n∈N∣n is prime}\{n \in \mathbb{N} \mid n \text{ is prime}\}. Decision problems are typically specified in the form of questions about a class of mathematical objects whose positive instances determine the set in question – e.g.
SAT \sc{SAT}\  Given a formula ϕ\phi of propositional logic, does there exist a satisfying assignment for ϕ\phi?
TRAVELING SALESMAN (TSP) \sc{TRAVELING}\ \sc{SALESMAN}\ (\sc{TSP}) \  Given a list of cities VV, the integer distance d(u,v)d(u,v) between each pair of cities u,v∈Vu,v \in V, and a budget  b∈Nb \in \mathbb{N}, is there a tour visiting each city exactly once and returning to the starting city of total distance ≤b\leq b?
INTEGER PROGRAMMING \sc{INTEGER}\ \sc{PROGRAMMING}\  Given an n×mn \times m integer matrix AA and an nn-dimensional vector of integers →b\vec{b}, does there exist an mm-dimensional vector →x\vec{x} of integers such that A→x=bA \vec{x} = b?
PERFECT MATCHING \sc{PERFECT} \ \sc{MATCHING}\  Given a finite bipartite graph GG , does there exist a perfect matching in GG ?  (GG is bipartite just in case its vertices can be partitioned into two disjoints sets UU and VV such that all of its edges EE  connect a vertex in UU to one in VV.   A matching is a subset of edges  M⊆EM \subseteq E no two members of which share a common vertex.  MM is perfect if it matches all vertices.)
These problems are typical of those studied in complexity theory in two fundamental respects.  First, they are all effectively decidable.  This is to say that they may all be decided in the ‘in principle’ sense studied in computability theory – i.e. by an effective procedure which halts in finitely many steps for all inputs.  Second, they arise in contexts in which we are interested in solving not only isolated instances of the problem in question, but rather in developing methods which allow it to be efficiently solved on a mass scale – i.e. for all instances in which we might be practically concerned.  Such interest often arises in virtue of the relationship of computational problems to practical tasks which we seek to analyze using the methods of discrete mathematics.  For example, instances of SAT\sc{SAT} arise when we wish to check the consistency of a set of specifications (e.g. those which might arise in scheduling the sessions of a conference or  designing a circuit board), instances of TSP\sc{TSP} and INTEGER PROGRAMMING\sc{INTEGER}\ \sc{PROGRAMMING} arise in many logistical and planning applications, instances of PERFECT MATCHING\sc{PERFECT} \ \sc{MATCHING} arise when we wish to find an optimal means of pairing candidates with jobs, 
 etc.[1] 
 The resources involved in carrying out an algorithm to decide an instance of a problems can typically be measured in terms of the number of processor cycles (i.e. elementary computational steps) and the amount of memory space (i.e. storage for auxiliary calculations) which are required to return a solution.   The methods of complexity theory can be useful not only in deciding how we can most efficiently expend such resources, but also in helping us to distinguish which effectively decidable problems possess efficient decision methods in the first place.  In this regard, it is traditional to distinguish pre-theoretically between the class of feasibly decidable problems  – i.e. those which can be solved in practice by an efficient algorithm – and the class of intractable problems – i.e. those which lack such algorithms and may thus be regarded as intrinsically difficult to decide (despite possibly being decidable in principle). The significance of this distinction is most readily appreciated by considering some additional examples.
1.1 A preliminary example
 A familiar example of a computational problem is that of primality testing – i.e. that of deciding n∈PRIMESn \in \sc{PRIMES} ?  This problem was intensely studied in mathematics long before the development of digital computers.  (See, e.g., (Williams 1998) for a history of primality testing and  (Crandall and Pomerance 2005) for a recent survey of the state of the art.)   After a number of preliminary results in the 19th and 20th centuries, the problem PRIMES\sc{PRIMES} was shown in 2004 to possess a so-called polynomial time decision algorithm – i.e. the so-called AKS primality test (Agrawal, Kayal, and Saxena 2004). This qualifies PRIMES\sc{PRIMES} as feasibly decidable relative to the standards which are now widely accepted in complexity theory and algorithmic analysis (see 
 Section 2.2).
Two related problems can be used to illustrate the sort of contrasts in difficulty which complexity theorists seek to analyze:
RELATIVE PRIMALITY \sc{RELATIVE}\ \sc{PRIMALITY}\  Given natural
numbers xx and yy, do  xx and yy possess greatest common divisor 11?  (I.e. are  xx and yy  relatively prime?)

FACTORIZATION \sc{FACTORIZATION}\  Given natural numbers xx
and yy, does there exist 1<d≤y1 \lt d \leq y such that d∣xd \mid
x?
RELATIVE PRIMALITY\sc{RELATIVE}\ \sc{PRIMALITY} can be solved by applying Euclid’s greatest common divisor algorithm – i.e. on input y≤xy \leq x, repeatedly compute the remainders r0=rem(x,y)r_0 = \text{rem}(x,y), r1=rem(y,r0)r_1 = \text{rem}(y, r_0), r2=rem(r0,r1)r_2 = \text{rem}(r_0, r_1) …, until ri=0r_i = 0 and then return ‘yes’ if ri−1=1r_{i-1} = 1 and ‘no’ otherwise. It may be shown that the number of steps in this sequence is always less than or equal to 
 5⋅log10(x)5 \cdot \log_{10}(x).[2] 
 This means that in order to determine if xx and yy are relatively prime, it suffices to calculate a number of remainders which is proportional to the number of digits in the decimal representation of the smaller of the two numbers. As this may also be accomplished by an efficient algorithm (e.g. long division), it may plausibly be maintained that if we are capable of inscribing a pair of numbers x,yx,y – e.g. by writing their numerical representations in binary or decimal notation on a blackboard or by storing such numerals in the memory of a digital computer of current design – then either we or such a computer will also be able to carry out these algorithms in order to decide whether xx and yy are relatively prime. This is the hallmark of a feasibly decidable problem – i.e. one which can be decided in the ‘in practice’ sense of everyday concretely embodied computation.
FACTORIZATION\sc{FACTORIZATION} is a decision variant of the familiar problem of finding the prime factorization of a given number xx – i.e. the unique sequence of primes pip_i and exponents aia_i such that x=pa11⋅…⋅pakkx = p_1^{a_1} \cdot \ldots \cdot p_k^{a_k}. It is not difficult to see that if there existed an efficient algorithm for deciding FACTORIZATION\sc{FACTORIZATION}, then there would also exist an efficient algorithm for determining prime 
 factorizations.[3] 
 It is also easy to see that the function taking xx to its prime factorization is effectively computable in the traditional sense of computability theory. For instance, it can be computed by the trial division algorithm.
In its simplest form, trial division operates by successively testing xx for divisibility by each integer smaller than xx and keeping track of the divisors which have been found thus far. As the number of divisions required by this procedure is proportional to xx itself, it might at first seem that it is not a particularly onerous task to employ this method to factor numbers of moderate size using paper and pencil calculation – say x<100000x \lt 100000. Note, however, that we conventionally denote natural numbers using positional notations such as binary or decimal numerals. A consequence of this is that the length of the expression which is typically supplied as an input to a numerical algorithm to represent an input x∈Nx \in \mathbb{N} is proportional not to xx itself, but rather to logb(x)\log_b(x) where b≥2b \geq 2 is the base of the notation system in 
 question.[4] 
 As a consequence it is possible to concretely inscribe positional numerals of moderate length which denote astronomically large numbers. For instance a binary numeral of 60 digits denotes a number which is larger than the estimated age of the universe in seconds and a binary numeral of 250 digits denotes a number which is larger than the estimated age of the universe in Planck 
 times.[5]
There are thus natural numbers whose binary representations we can easily inscribe, but for which no human mathematician or foreseeable computing device can carry out the trial division algorithm. This again might not seem particularly troubling as this algorithm is indeed ‘naive’ in the sense that it admits to several obvious improvements – e.g. we need only test xx for divisibility by the numbers 2,…,√x2, \ldots, \sqrt{x} to find an initial factor, and of these we need only test those which are themselves prime (finitely many of which can be stored in a lookup table). Nonetheless, mathematicians have been attempting to find more efficient methods of factorization for several hundred years. The most efficient factorization algorithm yet developed is similar to the trial division algorithm in that it requires a number of primitive steps which grows roughly in proportion to xx (i.e. the size of its input, as opposed to the length of its binary 
 representation).[6]   
 A consequence of these observations is that there exist concretely inscribable numbers – say on the order of 400 decimal digits – with the following properties: (i) we are currently unaware of their factorizations; and (ii) it is highly unlikely we could currently find them even if we had access to whatever combination of currently available computing equipment and algorithms we wish.
 Like the problems introduced above, FACTORIZATION\sc{FACTORIZATION} is of considerable practical importance, perhaps most famously because the security of well known cryptographic protocols assume that it is intractable in the general case (see, e.g., Cormen, Leiserson, and Rivest 2005).  But the foregoing observations still do not entail any fundamental limitation on our ability to know a number’s prime factorization. For it might still be hoped that further research will yield a more efficient algorithm which will allow us to determine the prime factorization of every number xx in which we might take a practical interest. A comparison of Euclid’s algorithm and trial division again provides a useful context for describing the properties which we might expect such an algorithm to possess. For note that the prior observations suggest that we ought to measure the size of the input  x∈Nx \in \mathbb{N} to a numerical algorithm not by  xx itself, but rather in terms of the length of xx’s binary representation. If we let |x|=dflog2(x)\lvert x\rvert =_{df} \log_2(x) denote this quantity, then it is easy to see that the efficiency of Euclid’s algorithm is given by a function which grows proportionally to |x|c1\lvert x\rvert^{c_1} for fixed c1c_1 (in fact, c1=1c_1 = 1), whereas the efficiency of trial division is given by a function proportional c|x|2c_2^{\lvert x\rvert} for fixed c2c_2 (in fact, c2=2c_2 = 2).
The difference in the growth rate of these functions illustrates the contrast between polynomial time complexity – which is currently taken by complexity theorists as the touchstone of feasibility – and exponential time complexity – which has traditionally been taken as the touchstone of intractability. For instance, if it could be shown that no polynomial time factorization algorithm exists, it might then seem reasonable to conclude that FACTORIZATION\sc{FACTORIZATION} is a genuinely intractable problem.
Although it is currently unknown whether this is the case, contemporary results provide circumstantial evidence that FACTORIZATION\sc{FACTORIZATION} is indeed intractable (see 
 Section 3.4.1).  Stronger evidence can be adduced for the intractability of conjecture  SAT\sc{SAT}, TSP\sc{TSP}, and INTEGER PROGRAMMING\sc{INTEGER}\ \sc{PROGRAMMING} (and similarly for a great many other problems of practical interest in subjects like logic, graph theory, linear algebra, formal language theory, game theory, and combinatorics). The technical development of complexity theory aims to make such comparisons of  computational difficulty precise and to show that the classification of certain problems as intractable admits to rigorous mathematical analysis.
1.2 Basic conventions
As we have just seen, in computational complexity theory a problem XX is considered to be complex in proportion to the difficulty of carrying out the most efficient algorithm by which it may be decided. Similarly, one problem XX is understood to be more complex (or harder) than another problem YY just in case YY possesses a more efficient decision algorithm than the most efficient algorithm for deciding XX. In order to make these definitions precise, a number of technical conventions are employed, many of which are borrowed from the adjacent fields of computability theory (e.g. Rogers 1987) and algorithmic analysis (e.g. Cormen, Leiserson, and Rivest 2005). It will be useful to summarize these before proceeding further.


A reference model of computation M\mathfrak{M} is chosen to represent algorithms. M\mathfrak{M} is assumed to be a reasonable model in the sense that it accurately reflects the computational costs of carrying out the sorts of informally specified algorithms which are encountered in mathematical practice. The deterministic Turing machine model T\mathfrak{T} is traditionally selected for this purpose. (See 
 Section 2.2 
 for further discussion of reasonable models and the justification of this choice.)

Decision problems are represented as sets consisting of objects which can serve as the inputs for a machine M∈MM \in \mathfrak{M}. For instance, if T\mathfrak{T} is used as the reference model then it is assumed that all problems XX are represented as sets of finite binary strings – i.e. X⊆{0,1}∗X \subseteq \{0,1\}^*. This is accomplished by defining a mapping ⌜⋅⌝:X→{0,1}∗\ulcorner \cdot \urcorner:X \rightarrow \{0,1\}^* whose definition will depend on the type of objects which comprise XX. For instance, if X⊆NX \subseteq \mathbb{N}, then ⌜n⌝\ulcorner n \urcorner will typically be the binary numeral representing nn. And if XX is a subset of FormL\text{Form}_{\mathcal{L}} – i.e. the set of formulas over a formal language L\mathcal{L} such as that of propositional logic – then ⌜ϕ⌝\ulcorner \phi \urcorner will typically be a (binary) Gödel numeral for ϕ\phi. Based on these conventions, problems XX will henceforth be identified with sets of strings {⌜x⌝:x∈X}⊆{0,1}∗\{\ulcorner x \urcorner : x \in X\} \subseteq \{0,1\}^* (which are often referred to as languages) corresponding to their images under such an encoding.

A machine MM is said to decide a language XX just in case MM computes the characteristic function of XX relative to the standard input-output conventions for the model M\mathfrak{M}. For instance, a Turing machine TT  decides XX just in case for all x∈{0,1}∗x \in \{0,1\}^*, the result of applying TT to xx yields a halting computation ending in a designated accept state if x∈Xx \in X and a designated reject state if x∉Xx \not\in X. A function problem is that of computing the values of a given function f:A→Bf:A \rightarrow B. MM is said to solve a function problem f:A→Bf:A \rightarrow B just in case the mapping induced by its operation coincides with f(x)f(x) – i.e. if M(x)=f(x)M(x) = f(x) for all x∈Ax \in A where M(x)M(x) denotes the result of applying machine MM to input xx, again relative to the input-output conventions for the model M\mathfrak{M}.

For each problem XX, it is also assumed that an appropriate notion of problem size is defined for its instances. Formally, this is a function |⋅|:X→N\lvert\cdot \rvert: X \rightarrow \mathbb{N} chosen so that the efficiency of a decision algorithm for XX will varying uniformly in |x|\lvert x\rvert. As we have seen, if X⊆NX \subseteq \mathbb{N}, it is standard to take |n|=log2(n)\lvert n\rvert = \log_2(n) – i.e. the number of digits (or length) of the binary numeral ⌜n⌝\ulcorner n \urcorner representing nn. Similarly if XX is a class of logical formulas over a language L\mathcal{L} (e.g. that or propositional or first-order logic), then |ϕ|\lvert\phi\rvert will typically be a measure of ϕ\phi’s syntactic complexity (e.g. the number of propositional variables or clauses it contains). If XX is a graph theoretic problem its instances will consist of the encodings of finite graphs of the form G=⟨V,E⟩G = \langle V,E \rangle where VV is a set of vertices and E⊆V×VE \subseteq V \times V is a set of edges. In this case |G|\lvert G\rvert will typically be a function of the cardinalities of the sets VV and EE.

The efficiency of a machine MM is measured in terms of its time complexity – i.e. the number of basic steps timeM(x)time_M(x) required for MM to halt and return an output for the input xx (where the precise notion of ‘basic step’ will vary with the model M\mathfrak{M}). This measure may be converted into a function of type N→N\mathbb{N} \rightarrow \mathbb{N} by considering tM(n)=max{timeM(x):|x|=n}t_{M}(n) = \max \{time_M(x) : \lvert x\rvert = n \} – i.e. the worst case time complexity of MM defined as the maximum number of basic steps required for MM to halt and return an output for all inputs xx of size nn. The worst case space complexity of MM – denoted sM(n)s_{M}(n) – is defined similarly – i.e. the maximum number of tape cells (or other form of memory locations) visited or written to in the course of MM’s computation for all inputs of size nn.

The efficiency of two machines is compared according to the order of growth of their time and space complexities. In particular, given a function f:N→Nf:\mathbb{N} \rightarrow \mathbb{N} we define its order of growth to be O(f(n))={g(n):∃c∃n0∀n≥n0(g(n)<c⋅f(n))}O(f(n)) = \{g(n) : \exists c \exists n_0 \forall n \geq n_0(g(n) \lt c \cdot f(n)) \} – i.e. the set of all functions which are asymptotically bounded  by f(n)f(n), ignoring scalar factors. For instance, for all fixed k,c1,c2 ∈Nk, c_1, c_2\ \in \mathbb{N} the following functions are all in O(n2)O(n^2): the constant kk-function, log(n),n,c1⋅n2+c2\log(n), n, c_1 \cdot n^2 + c_2.  However 11000n3∉O(n2)\frac{1}{1000}n^3 \not\in O(n^2). A machine M1M_1 is said to have lower time complexity (or to run faster than) another machine M2M_2 if tM1(n)∈O(tM2(n))t_{M_1}(n) \in O(t_{M_2}(n)), but not conversely. Space complexity comparisons between machines are performed similarly.

The time and space complexity of a problem XX are measured in terms of the worst case time and space complexity of the asymptotically most efficient algorithm for deciding XX. In particular, we say that XX has time complexity O(t(n))O(t(n)) if the worst case time complexity of the most time efficient machine MM deciding XX is in O(t(n))O(t(n)). Similarly, YY is said to be harder to decide (or more complex) than XX if the time complexity of XX is asymptotically bounded by the time complexity of YY. The space complexity of a problem is defined similarly.

A complexity class can now be defined to be the set of problems for which there exists a decision procedure with a given running time or running space complexity. For instance, the class TIME(f(n))\textbf{TIME}(f(n)) denotes the class of problems with time complexity f(n)f(n). P\textbf{P} – or polynomial time – is used to denote the union of the classes TIME(nk)\textbf{TIME}(n^k) for k∈Nk \in \mathbb{N} with respect to the reference model T\mathfrak{T}. P\textbf{P} hence subsumes all problems for which there exists a decision algorithm which can be implemented by a Turing machine whose time complexity is of polynomial order of growth. SPACE(f(n))\textbf{SPACE}(f(n)) and PSPACE\textbf{PSPACE} – or polynomial space – is defined similarly. Several other complexity classes we will consider below (e.g. NP\textbf{NP}, BPP\textbf{BPP}, BQP\textbf{BQP}) are defined by changing the reference model of computation, the definition of what it means for a machine to accept or reject an input, or both.
1.3 Distinguishing notions of complexity
With these conventions in place, we can now record several respects in which the meaning assigned to the word ‘complexity’ in computational complexity theory differs from that which is assigned to this term in several other fields.  In computational complexity theory, it is problems – i.e. infinite sets of finite combinatorial objects like natural numbers, formulas, graphs  – which are assigned ‘complexities’.  As we have just seen, such assignments are based on  the time or space complexity of the most efficient algorithms by which membership in a problem can be decided. A distinct notion of complexity is studied in Kolmogorov complexity theory (e.g., Li and Vitányi 1997). Rather than studying the complexity of sets of mathematical objects, this subject attempts to develop a notion of complexity which is applicable to individual combinatorial objects – e.g. specific natural numbers, formulas, graphs, etc. For instance, the Kolmogorov complexity of a finite string x∈{0,1}∗x \in \{0,1\}^* is defined to be the size of the smallest program for a fixed universal Turing machine which outputs xx given the empty string as input. In this setting, the ‘complexity’ of an object is thus viewed as a measure of the extent to which its description can be compressed algorithmically.
Another notion of complexity is studied in descriptive complexity theory (e.g., Immerman 1999). Like computational complexity theory, descriptive complexity theory also seeks to classify the complexity of infinite sets of combinatorial objects. However, the ‘complexity’ of a problem is now measured in terms of the logical resources which are required to define its instances relative to the class of all finite structures for an appropriate signature. As we will see in 
 Section 4.4 this approach often yields alternative characterizations of the same classes studied in computational complexity theory. 

Yet another subject related to computational complexity theory is algorithmic analysis (e.g. Knuth (1973), Cormen, Leiserson, and Rivest 2005). Like computational complexity theory, algorithmic analysis studies the complexity of problems and also uses the time and space measures tM(n)t_M(n) and sM(x)s_M(x) defined above. The methodology of algorithmic analysis is different from that of computational complexity theory in that it places primary emphasis on gauging the efficiency of specific algorithms for solving a given problem. On the other hand, in seeking to classify problems according to their degree of intrinsic difficulty, complexity theory must consider the efficiency of all algorithms for solving a problem. Complexity theorists thus make greater use of complexity classes such as P,NP\textbf{P},\textbf{NP}, and PSPACE\textbf{PSPACE} whose definitions are robust across different choices of reference model.  In algorithmic analysis, on the other hand, algorithms are often characterized relative to the finer-grained hierarchy of running times log2(n),n,n⋅log2(n),n2,n3,…\log_2(n), n, n \cdot \log_2(n), n^2, n^3, \ldots within 
 P\textbf{P}.[7]

2. The origins of complexity theory
2.1 Church’s Thesis and effective computability
The origins of computational complexity theory lie in computability theory and early developments in algorithmic analysis. The former subject began with the work of Gödel, Church, Turing, Kleene, and Post originally undertaken during the 1930s in attempt to answer Hilbert’s Entscheidungsproblem – i.e. is the problem FO-VALID\sc{FO}\text{-}\sc{VALID} of determining whether a given formula of first-order logic is valid decidable? At this time, the concept of decidability at issue was that of effective decidability in principle – i.e. decidability by a rule governed method (or effective procedure) each of whose basic steps can be individually carried out by a finitary mathematical agent but whose execution may require an unbounded number of steps or quantity of memory space.
We now understand the Entscheidungsproblem to have been answered in the negative by Church (1936a) and Turing (1937). The solution they provided can be reconstructed as follows: 1) a mathematical definition of a model of computation M\mathfrak{M} was presented; 2) an informal argument was given to show that M\mathfrak{M} contains representatives of all effective procedures; 3) a formal argument was then given to show that no machine M∈MM \in \mathfrak{M} decides FO-VALID\sc{FO}\text{-}\sc{VALID}. Church (1936b) took M\mathfrak{M} to be the class of terms Λ\Lambda in the untyped lambda calculus, while Turing took M\mathfrak{M} to correspond the class of T\mathfrak{T} of Turing machines. Church also showed the class FΛ\mathcal{F}_{\Lambda} of lambda-definable functions is extensionally coincident with the class FR\mathcal{F}_{\mathfrak{R}} of general recursive functions (as defined by Gödel 1986b and Kleene 1936). Turing then showed that the class FT\mathcal{F}_{\mathfrak{T}} of functions computable by a Turing machine was extensionally coincident with FΛ\mathcal{F}_{\Lambda}.
The extensional coincidence of the classes FΛ,FR\mathcal{F}_{\Lambda}, \mathcal{F}_{\mathfrak{R}}, and FT\mathcal{F}_{\mathfrak{T}} provided the first  evidence for what Kleene (1952) would later dub Church’s Thesis – i.e.

(CT)

  A function f:Nk→Nf:\mathbb{N}^k \rightarrow \mathbb{N} is effectively computable if and only if f(x1,…,xk)f(x_1, \ldots, x_k) is recursive.


CT can be understood to assign a precise epistemological significance to Church and Turing’s negative answer to the Entscheidungsproblem. For if it is acknowledged that FR\mathcal{F}_{\mathfrak{R}} (and hence also FΛ\mathcal{F}_{\Lambda} and FT\mathcal{F}_{\mathfrak{T}}) contain all effectively computable functions, it then follows that a problem XX can be shown to be effectively undecidable – i.e. undecidable by any algorithm whatsoever, regardless of its efficiency – by showing that the characteristic function cX(x)c_X(x) of XX is not recursive. CT thus allows us to infer from the fact that problems XX for which cX(x)c_X(x) can be proven to be non-recursive – e.g. the Halting Problem (Turing 1937) or the word problem for semi-groups (Post 1947) – are not effectively decidable.
It is evident, however, that our justification for such classifications can be no stronger than the stock we place in CT itself. One form of evidence often cited in favor of the thesis is that the coincidence of the class of functions computed by the members of Λ,R\Lambda, \mathfrak{R} and T\mathfrak{T} points to the mathematical robustness of the class of recursive functions. Two related forms of inductive evidence are as follows: (i) many other independently motivated models of computation have subsequently been defined which describe the same class of functions; (ii) the thesis is generally thought to yield a classification of functions which has thus far coincided with our ability to compute them in the relevant ‘in principle’ sense.
But even if the correctness of CT is granted, it is also important to keep in mind that the concept of computability which it seeks to analyze is an idealized one which is divorced in certain respects from our everyday computational practices. For note that CT will classify f(x)f(x) as effectively computable even if it is only computable by a Turing machine TT with time and space complexity functions tT(n)t_T(n) and sT(n)s_T(n) whose values may be astronomically large even for small inputs.[8]
Examples of this sort notwithstanding, it is often claimed that Turing’s original characterization of effective computability provides a template for a more general analysis of what it could mean for a function to be computable by a mechanical device. For instance,  Gandy (1980) and Sieg (2009) argue that the process by which Turing originally arrived at the definition of a Turing machine can be generalized to yield an abstract characterization of a mechanical computing device.   Such characterizations may in turn be understood as describing the properties which a physical system would have to obey in order for it to be concretely implementable.  For instance the requirement that a Turing machine may only access or modify the tape cell which is currently being scanned by its read-write head may be generalized to allow modification to a machine’s state at a bounded distance from one or more computational loci.  Such a requirement can in turn be understood as reflecting the fact that classical physics does not allow for the possibility of action at a distance.
On this basis CT is also sometimes understood as making a prediction about which functions are physically computable – i.e. are such that their values can be determined by measuring the states of physical systems which we might hope to use as practical computing devices.  We might thus hope that further refinements of the Gandy-Sieg conditions (potentially along the lines of the proposals of Leivant (1994) or Bellantoni and Cook (1992) discussed in  Section 4.5)  will eventually provide insight as to why some mathematical models of computation appear to yield a more accurate account than others of the exigencies of concretely embodied computation which complexity theory seeks to analyze.
2.2 The Cobham-Edmond’s Thesis and feasible computability
Church’s Thesis is often cited as a paradigm example of a case in which mathematical methods have been successfully employed to provide a precise analysis of an informal concept – i.e. that of effective computability. It is also  natural to ask whether the concept of feasible computability described in Section 1 itself admits a mathematical analysis similar to Church’s Thesis.  

We saw above that FACTORIZATION\sc{FACTORIZATION} is an example of a problem of antecedent mathematical and practical interest for which more efficient algorithms have historically been sought.   The task of efficiently solving combinatorial problems of the sort exemplified by TSP\sc{TSP}, INTEGER PROGRAMMING\sc{INTEGER}\ \sc{PROGRAMMING} and PERFECT MATCHING\sc{PERFECT} \ \sc{MATCHING} grew in importance during the 1950s and 1960s due to their role in scientific, industrial, and clerical applications. At the same time, the availability of digital computers began to make many such problems mechanically solvable on a mass scale for the first time.
This era also saw several theoretical steps which heralded the attempt to develop a general theory of feasible computability. The basic definitions of time and space complexity for the Turing machine model were first systematically formulated by Hartmanis and Stearns (1965) in a paper called “On the Computational Complexity of Algorithms”. This paper is also the origin of the so-called Hierarchy Theorems (see Section 3.2) 
which demonstrate that a sufficiently large increase in the time or space bound for a Turing machine computation allows more problems to be decided.
A systematic exploration of the relationships between different models of computation was also undertaken during this period. This included variants of the traditional Turing machine model with additional heads, tapes, and auxiliary storage devices such as stacks. Another important model introduced at about this time was the random access (or RAM) machine A\mathfrak{A} (see, e.g, Cook and Reckhow 1973). This model provides a simplified representation of the so-called von Neumann architecture on which contemporary digital computers are based. In particular, a RAM machine AA consists of a finite sequence of instructions (or program) ⟨π1,…,πn⟩\langle \pi_1,\ldots,\pi_n \rangle expressing how numerical operations (typically addition and subtraction) are to be applied to a sequence of registers r1,r2,…r_1,r_2, \dots in which values may be stored and retrieved directly by their index.
Showing that one of these models M1\mathfrak{M}_1 determines the same class of functions as some reference model M2\mathfrak{M}_2 (such as T\mathfrak{T}) requires showing that for all M1∈M1M_1 \in \mathfrak{M}_1, there exists a machine M2∈M2M_2 \in \mathfrak{M}_2 which computes the same function as M1M_1 (and conversely). This is typically accomplished by constructing M2M_2 such that each of the basic steps of M1M_1 is simulated by one or more basic steps of M2M_2. Demonstrating the coincidence of the classes of functions computed by the models M1\mathfrak{M}_1 and M2\mathfrak{M}_2 thus often yields additional information about their relative efficiencies. For instance, it is generally possible to extract from the definition of a simulation between M1\mathfrak{M}_1 and M2\mathfrak{M}_2 time and space overhead functions ot(x)o_t(x) and os(x)o_s(x) such that if the value of a function f(x)f(x) can be computed in time t(n)t(n) and space s(n)s(n) by a machine M1∈M1M_1 \in \mathfrak{M}_1, then it can also be computed in time ot(t(n))o_t(t(n)) and space os(s(n))o_s(s(n)) by some machine M2∈M2M_2 \in \mathfrak{M}_2.
For a wide class of  models, a significant discovery was that efficient simulations can be found. For instance, it might at first appear that the model A\mathfrak{A} allows for considerably more efficient implementations of familiar algorithms than does the model T\mathfrak{T} in virtue of the fact that a RAM machine can access any of its registers in a single step whereas a Turing machine may move its head only a single cell at a time. Nonetheless it can be shown that there exists a simulation of the RAM model by the Turing machine model with cubic time overhead and constant space overhead – i.e. ot(t(n))∈O(t(n)3)o_t(t(n)) \in O(t(n)^3) and os(s(n))∈O(s(n))o_s(s(n)) \in O(s(n)) (Slot and Emde Boas 1988). On the basis of this and related results, Emde Boas (1990) formulated the following proposal to characterize the relationship between reference models which might be used for defining time and space complexity:
Invariance Thesis ‘Reasonable’ models of computation can simulate each other within a polynomially bounded overhead in time and a constant-factor overhead in space.
The 1960s also saw a number of advances in algorithmic methods applicable to problems in fields like graph theory and linear algebra. One example was a technique known as dynamic programming. This method can sometimes be used to find efficient solutions to  optimization problems which ask us to find an object which minimizes or maximizes a certain quantity from a range of possible solutions. An algorithm based on dynamic programming solves an instance of such a problem by recursively breaking it into subproblems, whose optimal values are then computed and stored in a manner which can then be efficiently reassembled to achieve an optimal overall solution. 

 Bellman (1962) showed that the naive time complexity of O(n!)O(n!) for TSP\sc{TSP} could be improved to O(2nn2)O(2^n n^2) via the use of dynamic programming.  The question thus arose whether it was possible to improve upon such algorithms further, not only for TSP\sc{TSP}, but also for other problem such as SAT\sc{SAT} for which efficient algorithms had been sought but were not known to exist.  In order to appreciate what is at stake with this question, observe that the naive algorithm for TSP\sc{TSP} works as follows: 1) enumerate the set SGS_G of all possible tours in GG and compute their weights; 2) check if the cost of any of these tours is ≤b\leq b. Note, however, that if GG has nn nodes, then SGS_G may contain as many as n!n! tours.
This is an example of a so-called brute force algorithm -- i.e. one which solves a problem by exhaustively enumerating all possible solutions and then successively testing whether any of them are correct.  Somewhat more precisely, a problem XX is said to admit a brute force solution if there exists a feasibly decidable relation RXR_X and a family of uniformly defined finite sets SxS_x such that x∈Xx \in X if and only if there exists a feasibly sized witness y∈Sxy \in S_x such that RX(x,y)R_X(x,y). Such a yy is often called a certificate for xx’s membership in XX. The procedure of deciding x∈Xx \in X by exhaustively searching through all of the certificates y0,y1,…∈Sxy_0, y_1, \ldots \in S_x and checking if RX(x,y)R_X(x,y) holds at each step is known as a brute force search. For instance, the membership of a propositional formula ϕ\phi with atomic letters among P0,…,Pn−1P_0,\ldots, P_{n-1} in the problem SAT\sc{SAT} can be established by searching through the set SϕS_{\phi} of possible valuation functions of type v:{0,…,n−1}→{0,1}v:\{0,\ldots,n-1\} \rightarrow \{0,1\}, to determine if there exists v∈Sϕv \in S_{\phi} such that [[ϕ]]v=1\llbracket \phi \rrbracket_v = 1. Note, however, that since there are 2n2^n functions in SϕS_{\phi}, this yields only an exponential time decision algorithm.
Many other problems came to light in the 1960s and 1970s which, like SAT\sc{SAT} and  TSP\sc{TSP}, can easily be seen to possess exponential time brute force algorithms but for which no polynomial time algorithm could be found. On this basis, it gradually came to be accepted that a sufficient condition for a decidable problem to be intractable is that the most efficient algorithm by which it can be solved has at best exponential time complexity. The corresponding positive hypothesis that possession of a polynomial time decision algorithm should be regarded as sufficient grounds for regarding a problem as feasibly decidable was first put forth by Cobham (1965) and Edmonds (1965a).
Cobham began by citing the evidence motivating the Invariance Thesis as suggesting that the question of whether a problem admits a polynomial time algorithm is independent of which model of computation is used to measure time complexity across a broad class of alternatives. He additionally presented a machine-independent characterization of the class FP\textbf{FP} – i.e. functions f:Nk→Nf:\mathbb{N}^k \rightarrow \mathbb{N} which are computable in polynomial time – in terms of a restricted form of primitive recursive definition known as bounded recursion on notation (see 
 Section 4.5).
 Edmonds (1965a) first proposed that polynomial time complexity could be used as a positive criterion of feasibility – or, as he put it, possessing a “good algorithm” – in a paper in which he showed that a problem which might a priori be thought to be solvable only by brute force search (a generalization of PERFECT MATCHING\sc{PERFECT}\ \sc{MATCHING} from above) was decidable by a polynomial time algorithm.  Paralleling a similar study of brute force search in the Soviet Union, in a subsequent paper Edmonds (1965b) also provided an informal description of the complexity class NP\textbf{NP}.  In particular, he characterized this class as containing those problems XX for which there exists a “good characterization” – i.e. XX is such that the membership of an instance xx may be verified by using brute force search to find a certificate yy of feasible size which certifies xx’s membership in 
 XX.[9]
These observations provided the groundwork for has come to be known as the Cobham-Edmonds Thesis (see, e.g., Brookshear et al. 2006; Goldreich 2010; Homer and Selman 2011):

(CET)

A function f:Nk→Nf:\mathbb{N}^k \rightarrow \mathbb{N} is feasibly computable if and only if f(→x)f(\vec{x}) is computed by some machine MM such that tM(n)∈O(nk)t_M(n) \in O(n^k) where kk is fixed and MM is drawn from a reasonable model of computation M\mathfrak{M}.


CET provides a characterization of the notion of a feasibly computable function discussed in  Section 1 which is similar in form to Church’s Thesis. The corresponding thesis for decision problems holds that a problem is feasibly decidable just in case it is in the class P\textbf{P}. As just formulated, however, CET relies on the informal notion of a reasonable model of computation. A more precise formulation can be given by replacing this notion with a specific model such as T\mathfrak{T} or A\mathfrak{A} from the first machine class as discussed in 
 Section 3.1 below.
CET is now widely accepted within theoretical computer science for reasons which broadly parallel those which are traditionally given in favor of Church’s Thesis.  For instance, not only is the definition of the class FP\textbf{FP} stable across different machine-based models of computation in the manner highlighted by the Invariance Thesis, but there also exist several machine independent characterizations of this class which we will consider in Section 4.  Such results testify to the robustness of the definition of polynomial time computability.
 It is also possible to make a case for CET which parallels the quasi-inductive argument for CT. For in cases where we can compute the values of a function (or decide a problem) uniformly for the class of instances we are concerned with in practice, this is typically so precisely because we have discovered a polynomial time algorithm which can be implemented on current computing hardware (and hence also as a Turing machine).  And in instances where we are currently unable to uniformly compute the values of a function (or decide a problem) for all arguments in which we take interest, it is typically the case that we have not discovered a polynomial time algorithm (and in many cases may also possess circumstantial evidence that such an algorithm cannot exist).
Nonetheless, there are several features of CET which  suggest it should be regarded as less well established than CT.  Paramount amongst these is that we do not as yet know whether P\textbf{P} is properly contained in complexity classes such as NP\textbf{NP} which appear to contain highly intractable problems.  The following additional caveats are also often issued with respect to the claim that the class of computational problems we can decide in practice neatly aligns with those decidable in polynomial time using a conventional deterministic Turing 
 machine.[10]


CET classifies as feasible those functions whose most efficient algorithms have time complexity c⋅nkc \cdot n^k for arbitrarily large scalar factors cc and exponents kk. This means that if a function is only computable by an algorithm with time complexity 21000⋅n2^{1000} \cdot n or n1000n^{1000}, it would still be classified as feasible. This is so despite the fact that we would be unable to compute its values in practice for most or all inputs.

CET classifies as infeasible functions whose most efficient algorithms have time complexity which is of super-polynomial order of growth inclusive of, e.g., 2.000001n2^{.000001n} or nlog(log(log(n)))n^{\log(\log(\log(n)))}. However, such algorithms would run more efficiently when applied to the sorts of inputs we are likely to be concerned with than an algorithm with time complexity of (say) O(n2)O(n^2).

There exist problems for which the most efficient known decision algorithm has exponential time complexity in the worst case (and in fact are known to be NP\textbf{NP}-hard in the general case – see Section 3.2)  but which operate in polynomial time either in the average case or for a large subclass of problem instances of practical interest. Commonly cited examples include SAT\sc{SAT} (e.g. Cook and Mitchell 1997), as well as some problems from graph theory (e.g. Scott and Sorkin 2006), and computational algebra (e.g. Kapovich et al. 2003).

Many of the problems studied in complexity theory are decision variants of optimization problems.  In cases where these problems are  NP\textbf{NP}-complete, it is a consequence of CET (together with P≠NP\textbf{P} \neq \textbf{NP} – see Section 4.1) that they do not admit feasible exact algorithms – i.e. algorithms which are guaranteed to always find a maximal or minimal solution.  Nonetheless, it is known that a significant subclass of NP\textbf{NP}-complete problems possess polynomial time approximation algorithms -- i.e. algorithms which are guaranteed to find a solution which is within a certain constant factor of optimality.   For instance the optimization version of the problem VERTEX COVER\sc{VERTEX}\ \sc{COVER} defined below possesses a simple polynomial time approximation algorithm which allows us to find a solution (i.e. a set of vertices including at least one from each edge of the input graph) which is no larger than twice the size of an optimal solution.

There are non-classical models of computation which are hypothesized to yield a different classification of problems with respect to the appropriate definition of ‘polynomial time computability’. A notable example is the existence of a procedure known as Shor’s algorithm which solves the problem FACTORIZATION\sc{FACTORIZATION} in polynomial time relative the a model of computation known as the Quantum Turing Machine (see 
 Section 3.4.3).

3. Technical development
3.1 Deterministic and non-deterministic models of computation
According to the Cobham-Edmonds Thesis the complexity class P\textbf{P} describes the class of feasibily decidable problems. As we have just seen, this class is defined in terms of the reference model T\mathfrak{T} in virtue of the assumption that it is a ‘reasonable’ model of computation. Several other models of computation are also studied in complexity theory not because they are presumed to be accurate representations of the costs of concretely embodied computation, but rather because they help us better understand the limits of feasible computability. The most important of these is the non-deterministic Turing machine model N\mathfrak{N}.
Recall that a deterministic Turing machine T∈TT \in \mathfrak{T} can be represented as a tuple ⟨Q,Σ,δ,s⟩\langle Q,\Sigma,\delta,s \rangle where QQ is a finite set of internal states, Σ\Sigma is a finite tape alphabet, s∈Qs \in Q is TT’s start state, and δ\delta is a transition function mapping state-symbol pairs ⟨q,σ⟩\langle q,\sigma \rangle into state-action pairs ⟨q,a⟩\langle q,a \rangle. Here aa is chosen from the set of actions {σ,⇐,⇒}\{\sigma,\Leftarrow,\Rightarrow \} – i.e. write the symbol σ∈Σ\sigma \in \Sigma on the current square, move the head left, or move the head right. Such a function is hence of type δ:Q×Σ→Q×α\delta: Q \times \Sigma \rightarrow Q \times \alpha. On the other hand, a non-deterministic Turing machine N∈NN \in \mathfrak{N} is of the form ⟨Q,Σ,Δ,s⟩\langle Q,\Sigma,\Delta,s \rangle where Q,ΣQ,\Sigma, and ss are as before but Δ\Delta is now only required to be a relation – i.e. Δ⊆(Q×Σ)×(Q×α)\Delta \subseteq (Q \times \Sigma) \times (Q \times \alpha). As a consequence, a machine configuration in which NN is in state qq and reading symbol σ\sigma can lead to finitely many distinct successor configurations – e.g. it is possible that Δ\Delta relates ⟨q,σ⟩\langle q,\sigma \rangle to both ⟨q′,a′⟩\langle q',a' \rangle and ⟨q″,a″⟩\langle q'',a'' \rangle for distinct states q′q' and q″q'' and actions a′a' and 
 a″a''.[11]
This difference in the definition of deterministic and non-deterministic machines also necessitates a change in the definition of what it means for a machine to decide a language XX. Recall that for a deterministic machine TT, a computation sequence starting from an initial configuration C0C_0 is a finite or infinite sequence of machine configurations C0,C1,C2,…C_0,C_1,C_2, \ldots Such a configuration consists of a specification of the contents of TT’s tape, internal state, and head position. Ci+1C_{i+1} is the unique configuration determined by applying the transition function δ\delta to the active state-symbol pair encoded by CiC_i and is undefined if δ\delta is itself undefined on this pair (in which case the computation sequence is finite, corresponding to a halting computation). If NN is a non-deterministic machine, however, there may be more than one configuration which is related to the current configuration by Δ\Delta at the current head position. In this case, a finite or infinite sequence C0,C1,C2,…C_0,C_1,C_2, \ldots is said to be a computation sequence for NN from the initial configuration C0C_0 just in case for all i≥0i \geq 0, Ci+1C_{i+1} is among the configurations which are related by Δ\Delta to CiC_i (and is similarly undefined if no such configuration exists).
We now also redefine what is required for the machine NN to decide a language XX:


NN always halts – i.e. for all initial configurations C0C_0, the computation sequence C0,C1,C2,…C_0,C_1,C_2, \ldots of NN is of finite length;

if x∈Xx \in X and C0(x)C_0(x) is the configuration of NN encoding xx as input, then there exists a computation sequence C0(x),C1(x),…,Cn(x)C_0(x),C_1(x), \ldots, C_n(x) of NN such that Cn(x)C_n(x) is an accepting state;

if x∉Xx \not\in X, then all computation sequences C0(x),C1(x),…,Cn(x)C_0(x),C_1(x), \ldots, C_n(x) of NN are such that Cn(x)C_{n}(x) is a rejecting state.

Note that this definition treats accepting and rejecting computations asymmetrically. For if x∈Xx \in X, some of NN’s computation sequences starting from C0(x)C_0(x) may still lead to rejecting states as long as it least one leads to an accepting state. On the other hand, if x∉Xx \not\in X, then all of NN’s computations from C0(x)C_0(x) are required to lead to rejecting states.
Non-deterministic machines are sometimes described as making undetermined ‘choices’ among different possible successor configurations at various points during their computation. But what the foregoing definitions actually describe is a tree TNC0\mathcal{T}^N_{C_0} of all possible computation sequences starting from a given configuration C0C_0 for a deterministic machine NN (an example is depicted in Figure 1). Given the asymmetry just noted, it will generally be the case that all of the branches in TNC0(x)\mathcal{T}^N_{C_0(x)} must be surveyed in order to determine NN’s decision about the input xx.


Figure 1. A potential computation tree TNC0(x)\mathcal{T}^N_{C_0(x)} for a non-deterministic Turing machine NN starting from the initial configuration C0(x)C_0(x). Accepting states are labeled ⊙\odot and rejecting states ∘\circ. Note that although this tree contains an accepting computation sequence of length 4, its maximum depth of 5 still counts towards the determination of timeN(n)=max{depth(TNC0(x)) : |x|=ntime_N(n) = \max \{\text{depth}(\mathcal{T}^N_{C_0(x)})\ :\ \lvert x\rvert = n}.

The time complexity tN(n)t_N(n) of a non-deterministic machine NN is the maximum of the depths of the computation trees TNC0(x)\mathcal{T}^N_{C_0(x)} for all inputs xx such that |x|=n\lvert x\rvert = n. Relative to this definition, non-deterministic machines can be used to implement many brute force algorithms in time polynomial in nn. For instance the SAT\sc{SAT} problem can be solved by a non-deterministic machine which on input ϕ\phi uses part of its tape to non-deterministically construct (or ‘guess’) a string representing a valuation vv assigning a truth value to each of ϕ\phi’s nn propositional variables and then computes [[ϕ]]v\llbracket \phi \rrbracket_v using the method of truth tables (which is polynomial in nn). As ϕ∈SAT\phi \in \sc{SAT} just in case a satisfying valuation exists, this is a correct method for deciding SAT\sc{SAT} relative to conventions (i)–(iii) from above. This means that SAT\sc{SAT} can be solved in polynomial time relative to N\mathfrak{N}.
This example also illustrates why adding non-determinism to the original deterministic model T\mathfrak{T} does not enlarge the class of decidable problems. For if N∈NN \in \mathfrak{N} decides XX, then it is possible to construct a machine TN∈TT_N \in \mathfrak{T} which also decides XX, by successively simulating each of the finitely many possible sequences of non-deterministic choices NN might have made in the course of its 
 computation.[12] 
 It is evident that if NN has time complexity f(n)f(n), then TNT_N must generally check O(kf(n))O(k^{f(n)}) sequences of choices (for fixed kk) in order to determine the output of NN for an input of length nn. While the availability of such simulations shows that the class of languages decided by N\mathfrak{N} is the same as that decided by T\mathfrak{T} – i.e. exactly the recursive ones – it also illustrates why the polynomial time decidability of a language by a non-deterministic Turing machine only guarantees that the language is decidable in exponential time by a deterministic Turing machine.
In order to account for this observation, Emde Boas (1990) introduced a distinction between two classes of models of computation which he labels the first and  second machine classes. The first machine class contains the basic Turing machine model T\mathfrak{T} as well as other models which satisfy the Invariance Thesis with respect to this model. As we have seen, this includes the multi-tape and multi-head Turing machine models as well as the RAM model. On the other hand, the second machine class is defined to include those deterministic models whose members can be used to efficiently simulate non-deterministic computation. This can be shown to include a number of standard models of parallel computation such as the PRAM machine which will be discussed in 
 Section 3.4.3.  For such models the definitions of polynomial time, non-deterministic polynomial time, and polynomial space coincide.
Experience has borne out that members of the first machine class are the ones which we should consider reasonable models of computation in the course of formulating the Cobham-Edmonds Thesis. It is also widely believed that members of the second machine class do not provide realistic representations of the complexity costs involved in concretely embodied computation (Chazelle and Monier (1983), Schorr (1983), Vitányi (1986)). Demonstrating this formally would, however, require proving separation results for complexity classes which are currently unresolved. Thus while it is widely believed that the second machine class properly extends the first, this is currently an open problem.
3.2 Complexity classes and the hierarchy theorems
Recall that a complexity class is a set of languages all of which can be decided within a given time or space complexity bound t(n)t(n) or s(n)s(n) with respect to a fixed model of computation. To avoid pathologies which would arise were we to define complexity classes for ‘unnatural’ time or space bounds (e.g. non-recursive ones) it is standard to restrict attention to complexity classes defined when t(n)t(n) and s(n)s(n) are time or space constructible. t(n)t(n) is said to be time constructible just in case there exists a Turing machine which on input consisting of 1n1^n (i.e. a string of nn 1s) halts after exactly t(n)t(n) steps. Similarly, s(n)s(n) is said to be space constructible just in case there exists a Turing machine which on input 1n1^n halts after having visited exactly s(n)s(n) tape cells. It is easy to see that the time and space constructible functions include those which arise as the complexities of algorithms which are typically considered in practice – log(n),nk,2n,n!\log(n), n^k, 2^n, n!, etc.
When we are interested in deterministic computation, it is conventional to base the definitions of the classical complexity classes defined in this section on the model T\mathfrak{T}.  Supposing that t(n)t(n) and s(n)s(n) are respectively time and space constructible functions, the classes TIME(t(n))\textbf{TIME}(t(n)) and SPACE(s(n))\textbf{SPACE}(s(n)) are defined as follows:
TIME(t(n))={X⊆{0,1}∗ : ∃T∈T∀n(timeT(n)≤t(n))and T decides X}SPACE(s(n))={X⊆{0,1}∗ : ∃T∈T∀n(spaceT(n)≤s(n))and T decides X}
\begin{align*}
\textbf{TIME}(t(n)) = \{X \subseteq \{0,1\}^* \ : \ \exists &amp; T \in \mathfrak{T} \forall n(time_T(n) \leq t(n)) \\
             &amp;  \text{and } T \text{ decides } X \} \\

\textbf{SPACE}(s(n))= \{X \subseteq \{0,1\}^* \ : \ \exists &amp; T \in \mathfrak{T}\forall n(space_T(n) \leq s(n)) \\
             &amp; \text{and } T \text{ decides } X \}
\end{align*}
Since all polynomials in the single variable nn are of order O(nk)O(n^k) for some kk, the classes known as polynomial time and polynomial space are respectively defined as P=⋃k∈NTIME(nk)\textbf{P} = \bigcup_{k \in \mathbb{N}} \textbf{TIME}(n^k) and PSPACE=⋃k∈NSPACE(nk)\textbf{PSPACE} =  \bigcup_{k \in \mathbb{N}} \textbf{SPACE}(n^k). It is also standard to introduce the classes EXP=⋃k∈NTIME(2nk)\textbf{EXP} =  \bigcup_{k \in \mathbb{N}} \textbf{TIME}(2^{n^k}) (exponential time) and L=SPACE(log(n))\textbf{L} = \textbf{SPACE}(\log(n)) (logarithmic space).
In addition to classes based on the deterministic model T\mathfrak{T}, analogous non-deterministic complexity classes based on the model N\mathfrak{N} are also studied. In particular, the classes NTIME(t(n))\textbf{NTIME}(t(n)) and NSPACE(s(n))\textbf{NSPACE}(s(n)) are defined as follows:
NTIME(t(n))={X⊆{0,1}∗ : ∃N∈N∀n(timeN(n)≤t(n))and N decides X}NSPACE(s(n))={X⊆{0,1}∗ : ∃N∈N∀n(spaceN(n)≤s(n))and N decides X}
\begin{align*}
\textbf{NTIME}(t(n)) = \{X \subseteq \{0,1\}^* \ : \ \exists  &amp; N \in \mathfrak{N} \forall n(time_N(n) \leq t(n)) \\ 
   &amp; \text{and } N \text{ decides } X \} \\

\textbf{NSPACE}(s(n)) = \{X \subseteq \{0,1\}^* \ : \ \exists &amp; N \in \mathfrak{N} \forall n(space_N(n) \leq s(n)) \\ 
   &amp; \text{and } N \text{ decides } X \}
\end{align*}

The classes NP\textbf{NP} (non-deterministic polynomial time), NPSPACE\textbf{NPSPACE} (non-deterministic polynomial space), NEXP\textbf{NEXP} (non-deterministic exponential time), and NL\textbf{NL} (non-deterministic logarithmic space) are defined analogously to P\textbf{P}, NP\textbf{NP}, EXP\textbf{EXP} and L\textbf{L} – e.g. NP=⋃k∈NNTIME(nk)\textbf{NP} = \bigcup_{k \in \mathbb{N}} \textbf{NTIME}(n^k).
Many classical results and important open questions in complexity theory concern the inclusion relationships which hold among these classes. Central among these are the so-called Hierarchy Theorems which demonstrate that the classes TIME(t(n))\textbf{TIME}(t(n)) form a proper hierarchy in the sense that if t2(n)t_2(n) grows sufficiently faster than t1(n)t_1(n), then TIME(t2(n))\textbf{TIME}(t_2(n)) is a proper superset of TIME(t1(n))\textbf{TIME}(t_1(n)) (and similarly for NTIME(t(n))\textbf{NTIME}(t(n)) and SPACE(s(n))\textbf{SPACE}(s(n))).

Theorem 3.1  Suppose that t1(n)t_1(n) and t2(n)t_2(n) are time constructible, s1(n)s_1(n) and s2(n)s_2(n) are space constructible, t2(n)≥t1(n)≥nt_2(n) \geq t_1(n) \geq n, and s2(n)≥s1(n)≥ns_2(n) \geq s_1(n) \geq n. 


If 
limn→∞t1(n)log(t1(n))t2(n)=0,\lim_{n \rightarrow \infty} \frac{t_1(n) \log(t_1(n))}{t_2(n)} = 0, 

then TIME(t1(n))⊊TIME(t2(n))\textbf{TIME}(t_1(n)) \subsetneq \textbf{TIME}(t_2(n)). (Hartmanis and Stearns 1965)

 If
limn→∞t1(n+1)t2(n)=0,\lim_{n \rightarrow \infty} \frac{t_1(n+1)}{t_2(n)} = 0,

then NTIME(t1(n))⊊NTIME(t2(n))\textbf{NTIME}(t_1(n)) \subsetneq \textbf{NTIME}(t_2(n)). (Cook 1972)

 If
limn→∞s1(n)s2(n)=0,\lim_{n \rightarrow \infty} \frac{s_1(n)}{s_2(n)} = 0,

then SPACE(s1(n))⊊SPACE(s2(n))\textbf{SPACE}(s_1(n)) \subsetneq \textbf{SPACE}(s_2(n)). (Stearns, Hartmanis, and Lewis 1965)


These results may all be demonstrated by modifications of the diagonal argument by which Turing (1937) originally demonstrated the undecidability of the classical 
 Halting Problem.[13] 
 Nonetheless, Theorem 3.1 already has a number of interesting consequences about the relationships between the complexity classes introduced above. For instance, since the functions nkn^{k} and nk+1n^{k+1} satisfy the hypotheses of parts i), we can see that TIME(nk)\textbf{TIME}(n^k) is always a proper subset of TIME(nk+1)\textbf{TIME}(n^{k+1}) . It also follows from part i) that P⊊TIME(f(n))\textbf{P} \subsetneq \textbf{TIME}(f(n)) for any time bound f(n)f(n) which is of super-polynomial order of growth – e.g. 2n.00012^{n^{.0001}} or 2log(n)22^{\log(n)^2}. Similarly, parts i) and ii) respectively implies that P⊊EXP\textbf{P} \subsetneq \textbf{EXP} and NP⊊NEXP\textbf{NP} \subsetneq \textbf{NEXP}. And it similarly follows from part iii) that L⊊PSPACE\textbf{L} \subsetneq \textbf{PSPACE}.
Note that since every deterministic Turing machine is, by definition, a non-deterministic machine, we clearly have P⊆NP\textbf{P} \subseteq \textbf{NP} and PSPACE⊆NPSPACE\textbf{PSPACE} \subseteq \textbf{NPSPACE}. Some additional information about the relationship between time and space complexity is reported by the following classical results:

Theorem 3.2  Suppose that f(n)f(n) is both time and space constructible. Then

NTIME(f(n))⊆SPACE(f(n))\textbf{NTIME}(f(n)) \subseteq \textbf{SPACE}(f(n))
NSPACE(f(n))⊆TIME(2O(f(n)))\textbf{NSPACE}(f(n)) \subseteq \textbf{TIME}(2^{O(f(n))})


The first of these results recapitulates the prior observation that a non-deterministic machine NN with running time f(n)f(n) may be simulated by a deterministic machine TNT_N in time exponential in f(n)f(n). In order to obtain Theorem 3.2.i), note this process can be carried out in space bounded by f(n)f(n) provided that we make sure to erase the cells which have been used by a prior simulation before starting a new 
 one.[14]
Another classical result of Savitch (1970) relates SPACE(f(n))\textbf{SPACE}(f(n)) and NSPACE(f(n))\textbf{NSPACE}(f(n)):

Theorem 3.3
For any space constructible function s(n)s(n), NSPACE(s(n))⊆SPACE((s(n))2)\textbf{NSPACE}(s(n)) \subseteq \textbf{SPACE}((s(n))^2).
A corollary is that PSPACE=NPSPACE\textbf{PSPACE} = \textbf{NPSPACE}, suggesting that non-determinism is computationally less powerful with respect to space than it appears to be with respect to time.
The foregoing results establish the following basic relationships among complexity classes which are also depicted in Figure 2:
L⊆NL⊆P⊆NP⊆PSPACE⊆EXP⊆NEXP⊆EXPSPACE
\textbf{L} \subseteq \textbf{NL} \subseteq \textbf{P} \subseteq \textbf{NP} \subseteq \textbf{PSPACE} \subseteq \textbf{EXP} \subseteq \textbf{NEXP} \subseteq \textbf{EXPSPACE}

As we have seen, it is a consequence of Theorem 3.1 that L⊊PSPACE\textbf{L} \subsetneq \textbf{PSPACE} and P⊊EXP\textbf{P} \subsetneq \textbf{EXP}. It thus follows that at least one of the first four displayed inclusions must be proper and also at least one of the third, fourth, or fifth.


Figure 2. Inclusion relationships among major complexity classes. The only depicted inclusions which are currently known to be proper are L⊊PSPACE\textbf{L} \subsetneq \textbf{PSPACE} and P⊊EXP\textbf{P} \subsetneq \textbf{EXP}.

At the moment, however, this is all that is currently known – i.e. although various heuristic arguments can be cited in favor of the properness of the other displayed inclusions, none of them has been proven to hold. Providing unconditional proofs of these claims remains a major unfulfilled goal of complexity theory. For instance, the following is often described as the single most important open question in all of theoretical computer science:
Open Question 1 
Is P\textbf{P} properly contained in NP\textbf{NP}?
The significance of Open Question 1 as well as several additional unresolved questions about the inclusion relations among other complexity classes will be considered at greater length in 
 Section 4.1.
3.3 Reductions and NP\textbf{NP}-completeness
Having now introduced some of the major classes studied in complexity theory, we next turn to the question of their internal structure. This can be studied using the notions of the reducibility of one problem to another and of a problem being complete for a class. Informally speaking, a problem XX is said to be reducible to another problem YY just in case a method for solving YY would also yield a method for solving XX. The reducibility of XX to YY may thus be understood as showing that solving YY is at least as difficult as solving XX. A problem XX is said to be complete for a complexity class C\textbf{C} just in case XX is a member of C\textbf{C} and all problems in C\textbf{C} are reducible to XX. The completeness of XX for C\textbf{C} may thus be understood as demonstrating that XX is representative of the most difficult problems in C\textbf{C}.
The concepts of reduction and completeness were originally introduced in computability theory. Therein a number of different definitions of reduction are studied, of which many-one and Turing reducibility are the most familiar (see, e.g., Rogers 1987). Analogues of both of these have been studied in complexity theory under the names polynomial time many-one reducibility – also known as Karp reducibility (Karp 1972) – and polynomial time Turing reducibility – also known as Cook reducibility (Cook 1971). For simplicity we will consider only the former 
 here.[15]

Definition 3.1  
For languages X,Y⊆{0,1}∗X, Y \subseteq \{0,1\}^*, XX is said to be polynomial time many-one reducible to YY just in case there exists a polynomial time computable function f(x)f(x) such that
for all x∈{0,1}∗,x∈X if and only if f(x)∈Y
\text{for all } x \in \{0,1\}^*, x \in X \text{ if and only if } f(x) \in Y

In this case we write X≤PYX \leq_P Y and say that f(x)f(x) is a polynomial time reduction of XX to YY.

Note that if XX is polynomial time reducible to YY via f(x)f(x), then an efficient algorithm AA for deciding membership in YY would also yield an efficient algorithm for deciding membership in XX as follows: (i) on input xx, compute f(x)f(x); (ii) use AA to decide if f(x)∈Yf(x) \in Y, accepting if so, and rejecting if not.
It is easy to see that ≤P\leq_P is a reflexive relation. Since the composition of two polynomial time computable functions is also polynomial time computable, ≤P\leq_P is also transitive. We additionally say that a class C\textbf{C} is closed under ≤P\leq_P if Y∈CY \in \textbf{C} and X≤PYX \leq_P Y implies X∈CX \in \textbf{C}. It is also easy to see that the classes P,NP,PSPACE,EXP,NEXP\textbf{P}, \textbf{NP}, \textbf{PSPACE},\textbf{EXP}, \textbf{NEXP} and EXPSPACE\textbf{EXPSPACE} are closed under this relation.[16] A problem YY is said to be hard for a class C\textbf{C} if X≤PYX \leq_P Y for all X∈CX \in \textbf{C}. Finally YY is said to be complete for C\textbf{C} if it is both hard for C\textbf{C} and also a member of C\textbf{C}.
Since the mid-1970s a major focus of research in complexity theory has been the study of problems which are complete for the class NP\textbf{NP} – i.e. so-called NP-complete problems. A canonical example of such a problem is a time-bounded variant of the Halting Problem for N\mathfrak{N} (whose unbounded deterministic version is also the canonical Turing- and many-one complete problem in computability theory):
BHP \sc{BHP}\  Given the index of a
non-deterministic Turing machine N∈NN \in \mathfrak{N}, an input
xx, and a time bound tt represented as a string 1t1^t, does
NN accept xx in tt steps?
It is evident that BHP\sc{BHP} is in NP\textbf{NP} since on input ⟨⌜N⌝,x,1t⟩\langle \ulcorner N \urcorner, x,1^t \rangle an efficient universal non-deterministic machine can determine if NN accepts xx in time polynomial in |x|\lvert x\rvert and tt. To see that BHP\sc{BHP} is hard for NP\textbf{NP}, observe that if Y∈NPY \in \textbf{NP}, then YY corresponds to the set of strings accepted by some non-deterministic machine N∈NN \in \mathfrak{N} with polynomial running time p(n)p(n). If we now define f(x)=⟨⌜N⌝,x,1p(|x|)⟩f(x) = \langle \ulcorner N \urcorner,x,1^{p(\lvert x\rvert)} \rangle, then it is easy to see that f(x)f(x) is a polynomial time reduction of YY to BHP\sc{BHP}.
Since BHP\sc{BHP} is NP\textbf{NP}-complete, it follows from the closure of NP\textbf{NP} under ≤P\leq_P that this problem is in P\textbf{P} only if P=NP\textbf{P} = \textbf{NP}. Since this is widely thought not to be the case, this provides some evidence that BHP\sc{BHP} is an intrinsically difficult computational problem. But since BHP\sc{BHP} is closely related to the model of computation N\mathfrak{N} itself this may appear to be of little practical significance. It is thus of considerably more interest that a wide range of seemingly unrelated problems originating in many different areas of mathematics are also NP\textbf{NP}-complete.
One means of demonstrating that a given problem XX is NP\textbf{NP}-hard is to show that BHP\sc{BHP} may be reduced to it. But since most mathematically natural problems bear no relationship to Turing machines, it is by no means obvious that such reductions exist. This problem was circumvented at the beginning of the study of NP\textbf{NP}-completeness by Cook (1971) and Levin (1973) who independently demonstrated the following:[17]

Theorem 3.4  SAT\sc{SAT} is NP\textbf{NP}-complete.
We have already seen that SAT\sc{SAT} is in NP\textbf{NP}. In order to demonstrate Theorem 3.4 it thus suffices to show that all problems X∈NPX \in \textbf{NP} are polynomial time reducible to SAT\sc{SAT}. Supposing that X∈NPX \in \textbf{NP} there must again be a non-deterministic Turing machine N=⟨Q,Σ,Δ,s⟩N = \langle Q,\Sigma,\Delta,s \rangle accepting XX with polynomial time complexity p(n)p(n). The proof of Theorem 3.4 then proceeds by showing that for all inputs xx of length nn for NN, we can construct a propositional formula ϕN,x\phi_{N,x} which is satisfiable if and only if NN accepts xx within p(n)p(n) steps.[18]
Although SAT\sc{SAT} is still a problem about a particular system of logic, it is of a more combinatorial nature than BHP\sc{BHP}. In light of this, Theorem 3.4 opened the door to showing a great many other problems to be NP\textbf{NP}-complete by showing that SAT\sc{SAT} may be efficiently reduced to them. For instance, the problems TSP\sc{TSP}  and INTEGER PROGRAMMING \sc{INTEGER}\ \sc{PROGRAMMING}\  introduced above are both NP\textbf{NP}-complete.  Here are some other examples of NP\textbf{NP}-complete problems:
3-SAT 3\text{-}\sc{SAT}\  Given a propositional
formula ϕ\phi in 3-conjunctive normal form (3-CNF3\text{-}\sc{CNF}) –
i.e. ϕ\phi  is the conjunction of disjunctive clauses containing
exactly three negated or unnegated propositional variables – does
there exist a satisfying assignment for ϕ\phi?
HAMILTONIAN PATH \sc{HAMILTONIAN}\ \sc{PATH}\  Given a finite graph G=⟨V,E⟩G =
\langle V,E \rangle, does GG contain a Hamiltonian Path (i.e. a path which visits each vertex exactly once)?
INDEPENDENT SET \sc{INDEPENDENT}\ \sc{SET}\  Given a graph G=⟨V,E⟩G =
\langle V,E \rangle and a natural number k≤|V|k \leq \lvert V\rvert,
does there exist a set of vertices V′⊆VV' \subseteq V of cardinality
≥k\geq k such that no two vertices in V′V' are connected by an
edge?
VERTEX COVER \sc{VERTEX}\ \sc{COVER}\  Given a graph G=⟨V,E⟩G =
\langle V,E \rangle and a natural number k≤|V|k \leq \lvert V\rvert,
does there exist a set of vertices V′⊆VV' \subseteq V of cardinality ≤k\leq k such that for each edge ⟨u,v⟩∈E\langle u,v \rangle \in E, at least one of uu or
vv is a member of V′V'?
SET COVERING \sc{SET}\ \sc{COVERING}\  Given a finite set UU,
a finite family S\mathcal{S} of subsets of UU and a natural number
kk, does there exist a subfamily S′⊆S\mathcal{S}' \subseteq
\mathcal{S} of cardinality ≤k\leq k such that ⋃S′=U\bigcup
\mathcal{S}' = U?
The problem 3-SAT3\text{-}\sc{SAT} was shown to be NP\textbf{NP}-complete in Cook’s original paper (Cook 1971).[19] The other examples just cited are taken from a list of 21 problems (most of which had previously been identified in other contexts) which were shown by Karp (1972) to be NP\textbf{NP}-complete. The reductions required to show the completeness of these problems typically require the construction of what has come to be known as a gadget – i.e. a constituent of an instance of one problem which can be used to simulate a constituent of an instance of a different problem.  For instance, in order to see how 3-SAT3\text{-}\sc{SAT} may be reduced to INDEPENDENT SET\sc{INDEPENDENT}\ \sc{SET}, first observe that a 3-CNF3\text{-}\sc{CNF} formula has the form
(ℓ11 ∨ ℓ12 ∨ ℓ23) ∧ (ℓ21 ∨ ℓ22 ∨ ℓ23) ∧ … ∧ (ℓn1 ∨ ℓn2 ∨ ℓn3)
(\ell^1_1 \ \vee \ \ell^1_2 \ \vee \ \ell^2_3) \ \wedge \ (\ell^2_1 \ \vee \ \ell^2_2 \ \vee \ \ell^2_3) \ \wedge \ \ldots \  \wedge \ (\ell^n_1 \ \vee \ \ell^n_2 \ \vee \ \ell^n_3)

where each ℓij\ell^i_j is a literal – i.e. ℓij=pk\ell^i_j = p_k or ℓij=¬pk\ell^i_j = \neg p_k for some propositional variable pkp_k. A formula ϕ\phi of this form is satisfiable just in case there exists a valuation satisfying at least one of the literals ℓi1,ℓi2\ell^i_1, \ell^i_2 or ℓi3\ell^i_3 for all 1≤i≤n1 \leq i \leq n. On the other hand, suppose that we consider the family of graphs GG which can be partitioned into nn  disjoint triangles in the manner depicted in Figure 3. It is easy to see that any independent set of size nn in such a graph must  contain exactly one vertex from each triangle in GG. This in turn suggests the idea of using a graph of this form as a gadget for representing the clauses of a 3-CNF3\text{-}\sc{CNF} formula.


Figure 3. The graph GϕG_{\phi} for the formula (p1∨p2∨p3)∧(¬p1∨p2∨¬p3)∧(p1∨¬p2∨¬p3)(p_1 \vee p_2 \vee p_3) \wedge (\neg p_1 \vee p_2 \vee \neg p_3) \wedge (p_1 \vee \neg p_2 \vee \neg p_3).

A reduction of 3-SAT3\text{-}\sc{SAT} to INDEPENDENT SET\sc{INDEPENDENT}\ \sc{SET} can now be described as follows:


Let ϕ\phi be a 3-CNF3\text{-}\sc{CNF} formula consisting of nn clauses as depicted above.

We construct a graph Gϕ=⟨V,E⟩G_{\phi} = \langle V,E \rangle consisting of nn-triangles T1,…,TnT_1,\ldots,T_n such that the nodes of TiT_i are respectively labeled with the literals ℓi1,ℓi2,ℓi3\ell^i_1, \ell^i_2,\ell^i_3 comprising the iith clause of ϕ\phi. Additionally, GG contains an edge connecting nodes in each triangle corresponding to literals of opposite sign as depicted in Figure 3.[20]

Now define a mapping from instances of 3-SAT3\text{-}\sc{SAT} to instances of INDEPENDENT SET\sc{INDEPENDENT}\ \sc{SET} as f(ϕ)=⟨Gϕ,n⟩f(\phi) = \langle G_{\phi},n \rangle. As GϕG_\phi contains 3n3n vertices (and hence at most O(n2)O(n^2) edges), it is evident that f(x)f(x) can be computed in polynomial time. To see that f(ϕ)f(\phi) is a reduction of 3-SAT3\text{-}\sc{SAT} to INDEPENDENT SET\sc{INDEPENDENT}\ \sc{SET}, first suppose that vv is a valuation such that [[ϕ]]v=1\llbracket \phi \rrbracket_v = 1.  Then we must have [[ℓij]]v=1\llbracket \ell^i_j \rrbracket_v = 1 for at least one literal in every clause of ϕ\phi. Picking the nodes corresponding to this literal in each triangle TiT_i in ϕ\phi thus yields an independent set of size nn in GϕG_{\phi}. Conversely, suppose that V′⊆VV' \subseteq V is an independent set of size nn in GϕG_{\phi}. By construction, V′V' contains exactly one vertex in each of the TiT_i. And since there is an edge between each pair of nodes labeled with oppositely signed literals in different triangles in GϕG_{\phi}, V′V' cannot contain any contradictory literals. A satisfying valuation vv for ϕ\phi can be constructed by setting v(pi)=1v(p_i) = 1 if a node labeled with pip_i appears in V′V' and v(pi)=0v(p_i) = 0 otherwise.
Since ≤P\leq_P is transitive, composing polynomial time reductions together provides another means of showing that various problems are NP\textbf{NP}-complete. For instance, the completeness of TSP\sc{TSP} was originally demonstrated by Karp (1972) via the series of reductions
SAT≤P3-SAT≤PINDEPENDENT SET≤PVERTEX COVER≤PHAMILTONIAN PATH≤PTSP.
\sc{SAT} \leq_P 3\text{-}\sc{SAT} \leq_P \sc{INDEPENDENT}\ \sc{SET} \leq_P \sc{VERTEX}\ \sc{COVER} \leq_P \sc{HAMILTONIAN}\ \sc{PATH} \leq_P \sc{TSP}.


Thus although the problems listed above are seemingly unrelated in the sense that they concern different kinds of mathematical objects – e.g. logical formulas, graphs, systems of linear equations, etc. – the fact that they are NP\textbf{NP}-complete can be taken to demonstrate that they are all computationally universal for NP\textbf{NP} in the same manner as BHP\sc{BHP}.[21]
It also follows from the transitivity of ≤P\leq_P that the existence of a polynomial time algorithm for even one NP\textbf{NP}-complete problem would entail the existence of polynomial time algorithms for all problems in NP\textbf{NP}. The existence of such an algorithm would thus run strongly counter to expectation in virtue of the extensive effort which has been devoted to finding efficient solutions for particular NP\textbf{NP}-complete problems such as INTEGER PROGRAMMING\sc{INTEGER}\ \sc{PROGRAMMING} or TSP\sc{TSP}. Such problems are thus standardly regarded as constituting the most difficult problems in NP\textbf{NP}.
As long as Open Question 1 is answered in the positive – i.e. P⊊NP\textbf{P} \subsetneq \textbf{NP} – NP\textbf{NP}-complete problems thus fit the description of effectively decidable problems which are intrinsically difficult in the sense described in Section 1 .  As we are now about to see, however, they are by no means the hardest problems studied by complexity theorists. Nor is complexity theory incapable of making further distinctions about the difficulty of problems which lie inside P\textbf{P} or between P\textbf{P} and NP\textbf{NP} (presuming the latter class is non-empty).
3.4 Other complexity classes
3.4.1 NP\textbf{NP} and coNP\textbf{coNP}
In contrast to the non-deterministic Turing machine model N\mathfrak{N}, the acceptance and rejection conventions for deterministic models of computation such as T\mathfrak{T} are symmetric. In other words, for a deterministic machine TT to either accept or reject an input xx, it is both necessary and sufficient that there exist a single halting computation C0(x),…,Cn(x)C_0(x),\ldots,C_n(x). The output of the machine is then determined by whether Cn(x)C_n(x) is an accepting or rejecting configuration. A consequence of this is that complexity classes like P\textbf{P} which are defined in terms of this model are closed under complementation – i.e. for all languages X⊆{0,1}∗X \subseteq \{0,1\}^*, XX is in P\textbf{P} if and only if its complement ¯X=df{x∈{0,1}∗:x∉X}\overline{X} =_{df} \{x \in \{0,1\}^* : x \not\in X\} is also in P\textbf{P}.[22] If we define the class coC\textbf{coC} to consist of those problems whose complements are in the class C\textbf{C}, it thus follows that P=coP\textbf{P} = \textbf{coP}.
In contrast to this, there is no a priori guarantee that classes such as NP\textbf{NP} defined in terms of non-deterministic models are closed under complementation. For consider a problem such as SAT\sc{SAT} belonging to this class. ¯SAT\overline{\sc{SAT}} – i.e. the complement of SAT\sc{SAT}– consists of the set of formulas for which there does not exist a satisfying valuation – i.e. the contradictory formulas of propositional logic. From this it follows that ϕ∈¯SAT\phi \in \overline{\sc{SAT}} if and only if ¬ϕ∈VALID\neg \phi \in \sc{VALID} – i.e. the set of valid propositional formulas. As this observation provides an easy polynomial time reduction of ¯SAT\overline{\sc{SAT}} to VALID\sc{VALID}, the latter is thus not only a member of coNP\textbf{coNP}, but is also complete for this class.[23]
Note, however, that in order to show that ϕ∈VALID\phi \in \sc{VALID} requires that we show that ϕ\phi is true with respect to all valuations.  As there exist 2n2^n valuations for a formula containing nn  propositional variables, it is by no means evident that the membership of an arbitrary formula in VALID\sc{VALID} can be decided in polynomial time by an algorithm which can be implemented as a non-deterministic Turing machine relative to the acceptance and rejection conventions describe above. In fact, the following is another fundamental unresolved question in contemporary complexity theory:
Open Question 2 
Are the classes NP\textbf{NP} and coNP\textbf{coNP} distinct?
It is widely believed that like Open Question 1, Open Question 2 has an affirmative answer. But observe that if any NP\textbf{NP}-complete problem could be shown to be in coNP\textbf{coNP}, it would also follow that NP=coNP\textbf{NP}  = \textbf{coNP}.[24] This in turn suggests that problems which are known to be in the class NP∩coNP\textbf{NP} \cap \textbf{coNP} are unlikely to be NP\textbf{NP}-complete (and by symmetric considerations, also unlikely to be coNP\textbf{coNP}-complete). This class consists of those problems XX which possess polynomial sized certificates for demonstrating both membership and 
non-membership.

It is easy to see that NP∩coNP\textbf{NP} \cap \textbf{coNP} includes all problems in P\textbf{P}. But this class also contains some problems which are not currently known to be feasibly decidable. An important example is FACTORIZATION\sc{FACTORIZATION} (as defined in  Section 1.1).   For on the one hand, a number 1<d≤m1 \lt d \leq m which divides nn serves as a certificate for the membership of ⟨n,m⟩\langle n,m \rangle in FACTORIZATION\sc{FACTORIZATION}. And on the other hand, in order to demonstrate the membership of ⟨n,m⟩\langle n,m \rangle in ¯FACTORIZATION\overline{\sc{FACTORIZATION}}, it suffices to exhibit a prime factorization of nn in which no factor is less than mm. This follows because prime factorizations are unique (unlike, e.g., falsifying valuations in the case of VALIDITY\sc{VALIDITY}) and because the primality of the individual factors can be verified in polynomial time by the AKS algorithm. Hence FACTORIZATION\sc{FACTORIZATION} is in NP\textbf{NP} and coNP\textbf{coNP} simultaneously.
As with other computational problems of practical importance, considerable effort has been put into finding an efficient algorithm for FACTORIZATION\sc{FACTORIZATION}. If our inability to find an efficient factorization algorithm is indeed indicative that this problem is not in P\textbf{P}, then a positive answer to Open Question 2 would entail that there are natural mathematical problems which are not feasibly decidable but which are also not NP\textbf{NP}-complete. This in turn suggests that the degree structure of problems lying between P\textbf{P} and NP\textbf{NP} (presuming this class is non-empty) may provide a yet finer grained analysis of intrinsic computational difficulty than is provided by the Cobham-Edmonds Thesis alone.[25]
3.4.2 The Polynomial Hierarchy, polynomial space, and exponential time
If initial attempts to find an efficient algorithm for solving a problem XX which is known to be decidable are unsuccessful, a common strategy is to attempt to show that XX is NP\textbf{NP}-complete. For assuming P⊊NP\textbf{P} \subsetneq \textbf{NP}, it then follows from the Cobham-Edmonds thesis that no feasible algorithm for solving XX can exist.
Nonetheless, fields which make use of discrete mathematics often give rise to decidable problems which are thought to be considerably more difficult than NP\textbf{NP}-complete ones. For recall that it is a consequence of Theorem 3.1 that the classes EXP\textbf{EXP} and NEXP\textbf{NEXP} (i.e. exponential time and non-deterministic exponential time) both properly extend P\textbf{P}. Hence problems complete for these classes can currently be classified as infeasible regardless of how Open Question 1 is resolved. Such problems include several sub-cases of the decision problem for first-order logic (which will be considered in 
 Section 4.2) 
 as well as versions of some of the game-theoretic problems considered in this section.[26]
A complexity class which is likely to be properly contained in EXP\textbf{EXP} but which still contains many apparently infeasible problems which arise in computational practice is PSPACE\textbf{PSPACE}. A prototypical example of a problem in PSPACE\textbf{PSPACE} can be formulated using the notion of a quantified boolean formula [QBF] – i.e. a statement of the form Q1xi…QnxnψQ_1 x_i \ldots Q_n x_n\psi where Qi=∃Q_i = \exists or ∀\forall and ψ\psi is a formula of propositional logic containing the propositional variables x1,…,xnx_1,\ldots,x_n which are treated as bound by these quantifiers. A QBF-formula is said to be true if when QixiQ_i x_i is interpreted as an existential or universal quantifier over the truth value which is assigned to xix_i, ψ\psi is true with respect to all of the valuations determined by the relevant quantifier prefix – e.g. ∀x1∃x2(x1∨x2)\forall x_1 \exists x_2 (x_1 \vee x_2) is a true quantified boolean formula, while ∀x1∃x2∀x3((x1∨x2)∧x3)\forall x_1 \exists x_2 \forall x_3 ((x_1 \vee x_2) \wedge x_3) is not. We may now define the problem
TQBF \sc{TQBF}\ 
Given a quantified boolean formula ϕ\phi, is ϕ\phi true?
Stockmeyer and Meyer (1973) showed the following:

Theorem 3.5  TQBF\sc{TQBF} is PSPACE\textbf{PSPACE}-complete.[27]
This result suggests that there is a close connection between problems which can be decided using unbounded time but a feasible amount of memory space and those which could be solved if we were able to answer certain kinds of existential or universal queries in a single step using a bounded number of alternations between the two kinds of queries. This observation can be used to give an alternative characterization of several of the complexity classes we have considered. Recall, for instance, that NP\textbf{NP} can be informally characterized as the set of problems XX for which membership can be established by providing a certificate of the appropriate sort – e.g. ϕ∈SAT\phi \in \sc{SAT} can be established by exhibiting a satisfying valuation. Similarly, coNP\textbf{coNP} can be similarly characterized as the set of problems for which non-membership can be established by the existence of such a certificate – e.g. ϕ ∉VALID\phi\ \not\in \sc{VALID} can also be established by exhibiting a satisfying valuation.
On the basis of these observations, it is not difficult to prove the following:

Proposition 3.1   Call a relation R(x,y)R(x,y) polynomial decidable if {⟨x,y⟩∣R(x,y)}∈P\{\langle x,y \rangle \mid R(x,y) \} \in \textbf{P}.


A problem XX is in NP\textbf{NP} just in case there exists a polynomial decidable relation R(x,y)R(x,y) and a polynomial p(x)p(x) such that x∈Xx \in X if and only if ∃y≤p(|x|)R(x,y)\exists y \leq p(\lvert x\rvert) R(x,y).

A problem XX is in coNP\textbf{coNP} just in case there exists a polynomial decidable relation R(x,y)R(x,y) and a polynomial p(x)p(x) such that x∈Xx \in X if and only if ∀y≤p(|x|)R(x,y)\forall y \leq p(\lvert x\rvert) R(x,y).


Proposition 3.1 provides the basis for defining a hierarchy of complexity classes – i.e. the so-called Polynomial Hierarchy [PH\textbf{PH}] – based on the logical representation of computational problems.  We first define ΔP0,ΣP0\Delta^P_0, \Sigma^P_0 and ΠP0\Pi^P_0 to be alternative names for the class P\textbf{P} – i.e. ΔP0=ΣP0=ΠP0=P\Delta^P_0 = \Sigma^P_0 = \Pi^P_0 = \textbf{P}. We then define ΣPn+1\Sigma^P_{n+1} to be the class of problems of the form X={x∣∃y≤p(|x|)R(x,y)}X = \{x \mid \exists y \leq p(\lvert x\rvert) R(x,y)\} where R(x,y)∈ΠPnR(x,y) \in \Pi^P_{n} and ΠPn+1\Pi^P_{n+1} to be the class of problems of the form X={x∣∀y≤q(|x|)S(x,y)}X = \{x \mid \forall y \leq q(\lvert x\rvert) S(x,y)\} where S(x,y)∈ΣPnS(x,y) \in \Sigma^P_{n} and p(n)p(n) and q(n)q(n) are both polynomials. ΔPn\Delta^P_n is the set of problems which are in both ΣPn\Sigma^P_{n} and ΠPn\Pi^P_{n}. Finally, the class PH\textbf{PH} is then defined as ⋃k∈NΣPk\bigcup_{k \in \mathbb{N}} \Sigma^P_k.[28]
It follows immediately from Proposition 3.1 that ΣP1=NP\Sigma^P_1 = \textbf{NP} and ΠP1=coNP\Pi^P_1 = \textbf{coNP}. It is also evident from the foregoing definitions that we have the containment relations ΣPn⊆ΔPn+1⊆ΣPn+1\Sigma^P_n \subseteq \Delta^P_{n+1} \subseteq \Sigma^P_{n+1} and ΠPn⊆ΔPn+1⊆ΠPn+1\Pi^P_n \subseteq \Delta^P_{n+1} \subseteq \Pi^P_{n+1} depicted in Figure 4. It is also not difficult to show that PH⊆PSPACE\textbf{PH} \subseteq \textbf{PSPACE}. These relationships are similar to those which obtain between the analogously defined Σ0n\Sigma^0_n- and Π0n\Pi^0_n-sets in the Arithmetic Hierarchy studied in computability theory (see, e.g., Rogers 1987). But whereas it can be shown by a standard diagonal argument that the arithmetic hierarchy does not collapse, the following are also currently unresolved questions of fundamental importance:

Open Question 3


Is PH\textbf{PH} a proper hierarchy – i.e. is it the case that for all kk, ΣPk⊊ΣPk+1\Sigma^P_{k} \subsetneq \Sigma^P_{k+1} and ΣPk≠ΠPk\Sigma^P_{k} \neq \Pi^P_{k}?

Is PH\textbf{PH} properly contained in PSPACE\textbf{PSPACE}?





Figure 4. The Polynomial Hierarchy.

To better understand why Open Questions 3a,b) are also expected to be decided positively, it will be useful to further explore the relationship between PSPACE\textbf{PSPACE} and the characterization of PH\textbf{PH} in terms of quantifier alternations. To this end, we first show that the problem TQBF\sc{TQBF} may be redefined in terms of a game between a player called Verifier– who can be thought of as attempting to demonstrate that a QBF-formula ϕ\phi is true – and another player called Falsifier– who can be thought of as attempting to demonstrate that ϕ\phi is not true.[29] Supposing that ϕ\phi is of the form ∃x1∀x2…Qnψ\exists x_1 \forall x_2 \ldots Q_n \psi, a play of the verification game for ϕ\phi is defined as follows:


•
In the first round, Verifier moves by selecting a value for v(x1)v(x_1).


•
In the second round, Falsifier moves by selecting a value for v(x2)v(x_2).



⋮\quad\vdots


•
 In the nnth round,  either  Verifier or Falsifier selects a value of v(xn)v(x_n), depending on whether Qn=∃Q_n = \exists or ∀\forall.


•
If [[ϕ]]v=1\llbracket \phi \rrbracket_v = 1 then Verifier is the winner and if [[ϕ]]v=0\llbracket \phi \rrbracket_v = 0 then Falsifier is the winner.


A winning strategy for Verifier in the verification game for ϕ\phi is a sequence of moves and countermoves for all possible replies and counter-replies of Falsifier which is guaranteed to result in a win for Verifier. Let us call the problem of deciding whether Verifier has such a winning strategy for ϕ\phi TWO PLAYER SAT\sc{TWO}\ \sc{PLAYER}\ \sc{SAT}. As the moves of the two players mirror the interpretations of ∃\exists and ∀\forall for QBF formulas, it is not difficult to see that ϕ∈TWO PLAYER SAT\phi \in \sc{TWO}\ \sc{PLAYER}\ \sc{SAT} just in case ϕ∈TQBF\phi \in \sc{TQBF}. And for any QBF-formula ϕ\phi with nn initial quantifiers we may efficiently construct an equivalent formula with at most 2n2n quantifiers in the required alternating form by interspersing dummy quantifiers as needed. It thus follows that like TQBF\sc{TQBF}, TWO PLAYER SAT\sc{TWO}\ \sc{PLAYER}\ \sc{SAT} is also PSPACE\textbf{PSPACE}-complete.
Although TWO PLAYER SAT\sc{TWO}\ \sc{PLAYER}\ \sc{SAT} is defined in terms of a very simple game, similar results can be obtained for suitable variations of a variety of well familiar board games. Consider, for instance the following variation on the standard rules of Go: (i) the game is played on an n×nn \times n board; (ii) the winner of the game is the player with the most stones at the end of n2n^2 rounds.   Lichtenstein and Sipser (1980) demonstrated the PSPACE\textbf{PSPACE}-completeness of the following problem:
GO \sc{GO}\ 
Given a board position in an n×nn \times n game of Go, does there exist a winning strategy for black (i.e. the player who moves first)?
Subject to similar modification to the end game rules, analogous results have also been obtained for chess (Fraenkel and Lichtenstein 1981) and checkers (Robson 1984).[30]
What these games have in common is that the definition of a winning strategy for the player who moves first involves the alternation of existential and universal quantifiers in a manner which mimics the definition of the classes ΣPn\Sigma^P_n and ΠPn\Pi^P_n which comprise PH\textbf{PH}. Taking this into account, suppose we define TWO PLAYER SATn\sc{TWO}\ \sc{PLAYER}\ \sc{SAT}_n to be the variant of TWO PLAYER SAT\sc{TWO}\ \sc{PLAYER}\ \sc{SAT} wherein there are at most nn alternations of quantifiers in ϕ\phi (it thus follows that all of the games for formulas in this class will be of at most nn rounds). TWO PLAYER SATn\sc{TWO}\ \sc{PLAYER}\ \sc{SAT}_n may be shown to be complete for the class ΣPn\Sigma^P_n in the Polynomial Hierarchy. Note, however, as the value of nn increases, we expect that it should become more difficult to decide membership in TWO PLAYER SATn\sc{TWO}\ \sc{PLAYER}\ \sc{SAT}_n in much the same way that it appears to become more difficult to determine whether a given player has a winning strategy for increasingly long games of Go or chess.
This observation provides part of the reason why it is currently expected that the answer to Open Question 3a is positive. For as we can now see, the assertion PH=ΣPk\textbf{PH} = \Sigma^P_k for some kk is equivalent to the assertion that the problem of determining whether Verifier has a winning strategy for nn-round verification games is no harder than that of deciding this question for kk-round games for all n≥kn \geq k. A related observation bears on the status of Open Question 3b. For note that if PH=PSPACE\textbf{PH} = \textbf{PSPACE}, then TWO PLAYER SAT\sc{TWO}\ \sc{PLAYER}\ \sc{SAT} would be complete for PH\textbf{PH} (as it is for PSPACE\textbf{PSPACE}). Since PH\textbf{PH} is defined as ⋃k∈NΣPk\bigcup_{k \in \mathbb{N}} \Sigma^P_k, it would then follow that TWO PLAYER SAT∈ΣPk\sc{TWO}\ \sc{PLAYER}\ \sc{SAT} \in \Sigma^P_k for some kk. But in this case, we would again have that the problem of determining whether Verifier has a winning strategy for nn-round verification games would be no harder than that of deciding this question for kk-round games for all n≥kn \geq k. As this again runs contrary to expectation, it is also widely believed not only that PH⊊PSPACE\textbf{PH} \subsetneq \textbf{PSPACE} but also that the former class differs from the latter in failing to have complete problems.
3.4.3 Parallel, probabilistic, and quantum complexity
Even taking into account our current inability to resolve Open Questions 1–3, the hierarchy of complexity classes depicted in Figure 2 ranging from P\textbf{P} to EXP\textbf{EXP} represent the most robust benchmarks of computational difficulty now available. Beyond this hierarchy a wide array of additional classes are also studied which are believed to demarcate additional structure either inside P\textbf{P} or between P\textbf{P} and NP\textbf{NP}. Comprehensive consideration of the diversity of these classes is beyond the scope of the current survey. But to complement our understanding of classes defined relative to T\mathfrak{T} and N\mathfrak{N}, it will be useful to consider three additional classes – NC\textbf{NC}, BPP\textbf{BPP}, and BQP\textbf{BQP} – which are defined relative to other models of computation.
One way of defining NC\textbf{NC} is in terms of a model of computation P\mathfrak{P} known as the Parallel RAM (or PRAM) machine. This model is a variant of the conventional RAM model described in  Section 2.2 which allows for parallelism. A PRAM machine PP consists of a sequence of programs Π1,…,Πq(n)\Pi_1,\ldots,\Pi_{q(n)}, each comprised by a finite sequence of instructions for performing operations on registers as before. Each program also controls a separate processor with its own program counter and accumulator. The processors operate in parallel and can write to a common block of registers, adopting some policy for resolving with conflicts. Finally, the number of programs (and hence processors) comprising PP is not fixed, but can vary with the size of the input nn according to the function q(n)q(n). In this way, a PRAM machine can recruit more processors to operate on larger inputs, although the number of processors employed must be fixed uniformly in the size of the input.
Although the PRAM model is deterministic, it is easy to see that its members can be used to efficiently simulate the operation of a non-deterministic Turing machine NN by recruiting sufficiently many processors to carry out all paths in a given computation tree for NN in parallel. This model is hence a member of van Emde Boas’s (1990) second machine class and as such is not considered to be a reasonable model of computation. Nonetheless, P\mathfrak{P} is a useful theoretical model in that it provides a formal medium for implementing procedures which call for certain operations to be carried out simultaneously in parallel.
It has long been known that certain problems – e.g. matrix multiplication, graph reachability, and sorting – admit parallel algorithms which in some cases are faster than the most efficient known sequential ones (see, e.g., Papadimitriou 1994). But this observation would still be of little practical significance if the algorithms in question achieved such speed up only at the cost of having to employ exponentially many processors relative to the size of their inputs. For in this case it seems that we would have little hope of building a computing device on which they could be concretely implemented.
It is notable, however, that the parallel algorithms for the problems just mentioned only require a number of processors which is polynomial in size of their input nn and also only require time polylogarithmic in nn (i.e. O(logk(n))O(\log^k(n)) for fixed kk). This motivates the definition of NC\textbf{NC} as the class of problems which can be solved by a PRAM machine in time O(logc(n))O(\log^c(n)) using O(nk)O(n^k) processors (for fixed c,k∈Nc,k \in \mathbb{N}).[31] In particular, the amount of work (defined as the sum of the number of steps required by each processor) performed by a machine satisfying this definition will be polynomial in the size of its input.
We have seen that the Cobham-Edmond’s Thesis suggests that P\textbf{P} coincides with the class of problems which can be feasibly decided by algorithms implementable relative to a sequential model of computation such as T\mathfrak{T}. In much the same way, it is often suggested that NC\textbf{NC} coincides with the class of problems which can be feasibly decided by algorithms which can be implemented relative to a parallel model such as P\mathfrak{P} (see, e.g. Greenlaw, Hoover, and Ruzzo 1995). In particular, it can be shown that any PRAM machine which runs in time O(logc(n))O(\log^c(n)) using O(nk)O(n^k) processors can be simulated by a polynomial time sequential machine. From this, it follows that NC\textbf{NC} is a subset of P\textbf{P}.
NC\textbf{NC} is of interest in part because the opposite inclusion is not known to hold – i.e. the following question is also open:
Open Question 4  Is NC\textbf{NC} properly contained in P\textbf{P}?
The current consensus is that like Open Questions 1–3, Open Question 4 will also be answered in the positive. In this case, the heuristic argument derives from the observation that if NC=P\textbf{NC} = \textbf{P}, then it would be the case that every problem XX possessing a O(nj)O(n^j) sequential algorithm could be ‘sped up’ in the sense of admitting a parallel algorithm which requires only time O(logc(n))O(\log^c(n)) using O(nk)O(n^{k}) processors. Even allowing for the case where kk is substantially larger than jj, this is thought to be unlikely in virtue of the fact that certain problems in P\textbf{P} appear to be ‘inherently sequential’ in the sense of exhibiting structure which makes them resistant to parallelization. In particular, this is conjectured to be true of P\textbf{P}-complete problems such as determining whether a boolean circuit evaluates to true, or if a formula in the Horn fragment of propositional logic is satisfiable.[32]
Another active area of research in complexity theory concerns classes defined in terms of probabilistic models of computation. Instances of such models are assumed to have access to some genuine source of randomness – e.g. a fair coin or a quantum mechanical random number generator – which can be used to determine the course of their computation. This basic idea has given rise to a number of different paradigms for modeling probabilistic computation, of which the original non-deterministic Turing machine model N\mathfrak{N} is an early exemplar. As we have seen, however, non-deterministic Turing machines cannot be employed as a practical model of probabilistic computation. For in order to use NN to conclude that xx is not a member of a given language XX requires that we examine all of NN’s potential computations on this input (which is generally infeasible to do).
It is thus also reasonable to ask how we might modify the definition of N\mathfrak{N} so as to obtain a characterization of probabilistic algorithms which we might usefully employ. One answer is embodied in the definition of the class BPP\textbf{BPP}, or bounded-error probabilistic polynomial time.[33] This class can be most readily defined relative to a model of computation known as the probabilistic Turing machine C\mathfrak{C}. Such a device CC has access to a random number generator which produces a new bit at each step in its computation but is otherwise like a conventional Turing machine. The action taken by CC at the next step is then determined by the value of this bit, in addition to its internal state and the currently scanned symbol.
BPP\textbf{BPP} can now be defined to include the problems XX such that there exists a probabilistic Turing machine C∈CC \in \mathfrak{C} and a constant 12<p≤1\frac{1}{2} \lt p \leq 1 with the following properties:


CC runs in polynomial time for all inputs;

for all inputs x∈Xx \in X, at least fraction pp of the possible computations of CC on xx accept;

for all inputs x∉Xx \not\in X, at least fraction pp of the possible computations of CC on xx reject.

This definition seeks to formalize the intuitive characterization of a problems decidable by a probabilistic algorithm as one for which there exists a decision procedure which can make undetermined choices during its computation but still solves the problem correctly in a ‘clear majority’ of cases (i.e. with probability pp bounded away from 12\frac{1}{2}). It is not difficult to see that if we possessed an algorithm for deciding XX which is implementable by a machine CC satisfying (i)–(iii), then we could correctly decide whether x∈Xx \in X with arbitrarily high probability by applying the algorithm repeatedly and checking whether the majority of its computations accepted or rejected.[34]
As the standard Turing machine model T\mathfrak{T} corresponds to a special case of C\mathfrak{C}, it follows that P⊆BPP\textbf{P} \subseteq \textbf{BPP}. For a long time, it was also thought that PRIMES\sc{PRIMES} might be an example of a problem which was in BPP\textbf{BPP} but not P\textbf{P}.[35] However, we now know that this problem is in P\textbf{P} in virtue of the AKS primality algorithm. At present, not only are there few known candidates for separating these classes, but it is also not known if NP\textbf{NP} is contained in BPP\textbf{BPP} (although the latter has been shown to be included in ΣP2\Sigma^P_2  by Lautemann (1983)). Thus although it might at first seem plausible that the ability to make random choices during the course of a computation allows us to practically solve certain problems which resist efficient deterministic algorithms, it is again an open problem whether this is true.
A final complexity class which has attracted considerable attention in recent years is known as BQP\textbf{BQP} (or bounded error quantum polynomial time). BQP\textbf{BQP} is defined analogously to BPP\textbf{BPP} but using a model Q\mathfrak{Q} of quantum computation instead of the model C\mathfrak{C}.  Such a model can be roughly characterized as a device which makes use of quantum-mechanical phenomena  (such as entanglement or interference) to perform operations on data represented by sequences qubits – i.e. quantum superpositions of vectors of 0s and 1s. Based on early suggestions of Manin (1980) and Feynman (1982), a number of such models were proposed in the 1980s and 90s on which the precise definition of BQP\textbf{BQP} can be based – e.g. the Quantum Turing Machine of Deutsch (1985).
Several algorithms have been discovered which can be implemented on such devices  which run faster than the best known classical algorithms for the same problem. Among these are Grover’s algorithm (Grover 1996) for searching an unsorted database (which runs in time O(n1/2)O(n^{1/2}), whereas the best possible classical algorithm is O(n)O(n)) and Shor’s algorithm (Shor 1999) for integer factorization (which runs in O(log2(n)3)O(\log_2(n)^3), whereas the best known classical algorithm is O(2log2(log2(n))1/3)O(2^{\log_2(\log_2(n))^{1/3})}). Since it can be shown that quantum models can simulate models such as the classical Turing machine, BQP\textbf{BQP} contains P\textbf{P} and BPP\textbf{BPP}. And as we have just seen, BQP\textbf{BQP} contains at least one problem – i.e. FACTORIZATION\sc{FACTORIZATION} – which is not known to be contained in P\textbf{P}.
Although it can be shown that BQP⊆PSPACE\textbf{BQP} \subseteq \textbf{PSPACE}, the relationship between the former class and NP\textbf{NP} is currently not well understood. In particular, no polynomial time quantum algorithms have been discovered for solving NP\textbf{NP}-complete problems which are implementable on the more widely accepted models of quantum computation. There is also considerable controversy as to whether it will prove possible to construct physical realizations of such models which are sufficiently robust to reliably solve instances of problems which cannot currently be solved using classical hardware.   Even if a proof of P⊊BQP\textbf{P} \subsetneq \textbf{BQP} were found, further empirical investigation would thus be required to determine the bearing of quantum computation on either the limits of feasible computation or on the status of the Cobham-Edmonds thesis itself. Nonetheless, quantum computation is a very active area of research at present.[36]
4. Connections to logic and philosophy
There has to date been surprisingly little philosophical engagement with computational complexity theory.  Although several questions which have their origins in computability theory – e.g. the status of Church’s Thesis and the significance of effectively undecidable problems – have been widely discussed, the analogous questions about the Cobham-Edmonds Thesis and the significance of effectively decidable but feasibly undecidable problems have attracted little attention amongst philosophers of mathematics. And despite ongoing interest in logical knowledge and resource bounded reasoning, fields such as epistemology, decision theory, and social choice theory have only recently begun to make use of complexity-theoretic concepts and results.  This section will attempt to bridge some of these gaps by highlighting connections between computational complexity and traditional topics in logic and philosophy.
4.1 On the significance of P≠NP\textbf{P} \neq \textbf{NP}?
The appreciation of complexity theory outside of theoretical computer science is largely due to the notoriety of open questions such as 1–4. Of these, Open Question 1 – henceforth P≠NP?\textbf{P} \neq \textbf{NP}? – has attracted the greatest attention. Along with long standing unresolved questions from pure and applied mathematics such as the Riemann Hypothesis and the Hodge Conjecture, it is the subject of a major prize competition – i.e. the Millennium Problems (Cook 2006). It is also frequently the focus of survey articles and popular expositions – e.g. (Sipser 1992), (Fortnow 2009), and (Fortnow 2013).
There are indeed several reasons to suspect that the resolution of P≠NP?\textbf{P} \neq \textbf{NP}? will prove to have far reaching practical and theoretical consequences outside of computer science. Perhaps the most significant of these revolves around the possibility that despite the various heuristic arguments which can currently be offered in favor of the hypothesis that P≠NP\textbf{P} \neq \textbf{NP}, there remains the possibility that P=NP\textbf{P} = \textbf{NP} is true after all. Recall that P\textbf{P} can be characterized as the class of problems membership in which can be decided efficiently, whereas NP\textbf{NP} can be characterized as the class of problems for which membership can be verified efficiently once an appropriate certificate is provided. If it turned out that P=NP\textbf{P} = \textbf{NP}, then the difficulty of these two tasks would coincide (up to a polynomial factor) for all problems in NP\textbf{NP}. Some consequences of this would be that the task of finding a satisfying valuation vv for a propositional formula ϕ\phi is no harder than constructing its truth table, the task of factoring a natural number would be no more difficult than verifying that a given factorization is correct, etc.
Our intuitions strongly reflect the fact that the former problems in such pairs seem more difficult than the latter. But beyond collapsing distinctions currently appear almost self-evident, the coincidence of P\textbf{P} and NP\textbf{NP} would also have a more radical effect on the situation which we currently face in mathematics. For suppose that T\mathsf{T} is a recursively axiomatic theory which is sufficiently strong to formalize our current mathematical theorizing – e.g. Zermelo Fraenkel set theory with the Axiom of Choice [ZFC\mathsf{ZFC}], supplemented as needed with large cardinal hypotheses. Observe that regardless of the proof theoretic strength of T\mathsf{T}, the following question about derivability in this theory will still presumably be decidable in polynomial time:
PROOF CHECKINGT \sc{PROOF}\ \sc{CHECKING}_{\mathsf{T}}\  
Given a formula ϕ\phi in the language of T\mathsf{T} and a finite object D\mathcal{D} of the appropriate sort (e.g. a derivation sequence or tree), is D\mathcal{D} a well-formed proof of ϕ\phi from the axioms of T\mathsf{T}?[37]
For such standard definitions it will also be the case that for each nn, there will be of order O(2n)O(2^n) T\mathsf{T}-derivations of length n=|D|n = \lvert \mathcal{D}\rvert (say measured in the number of symbols in D\mathcal{D}). We can thus non-deterministically check if ϕ\phi is derivable by a proof of size ≤n\leq n by guessing a sequence of symbols length nn and then checking (in polynomial time) whether it is a well-formed proof of ϕ\phi. It thus follows that the following problem is in NP\textbf{NP}:
n-PROVABILITYT n\text{-}\sc{PROVABILITY}_{\mathsf{T}}\ 
Given a formula ϕ\phi in the language of T\mathsf{T} and a natural number nn, does there exist a valid T\mathsf{T}-derivation D\mathcal{D} of length ≤n\leq n such that D\mathcal{D} is a valid proof of ϕ\phi?
We are, of course, generally interested in the question of whether ϕ\phi is provable in T\mathsf{T} without a restriction on the length of its derivation – e.g. in the case where ϕ\phi expresses the Riemann Hypothesis and T\mathsf{T} is a theory like ZFC\textsf{ZFC}. But in a letter to von Neumann, Gödel (1956) observed that if we were able to efficiently decide n-PROVABILITYTn\text{-}\sc{PROVABILITY}_{\mathsf{T}}, then this would already have enormous significance for mathematical practice. For note that it seems plausible to assume that no human mathematician will ever be able to comprehend a proof containing 100 million symbols (≈25000\approx 25000 pages). If we were able to efficiently check if ϕ∈n-PROVABILITYT\phi \in n\text{-}\sc{PROVABILITY}_{\mathsf{T}} (say for n=108n = 10^8) and obtained a negative answer, Gödel concludes that “there would be no reason to think further about [ϕ\phi]” (1956, p. 612). For in this case a demonstration that ϕ∉n-PROVABILITYT\phi \not\in n\text{-}\sc{PROVABILITY}_{\mathsf{T}} (for a sufficiently large nn and a sufficiently powerful T\mathsf{T}) would be sufficient to show that we have no hope of ever comprehending a proof of ϕ\phi even if one were to exist.
But now note that since n-PROVABILITYT∈NPn\text{-}\sc{PROVABILITY}_{\mathsf{T}} \in \textbf{NP}, if it so happened that P=NP\textbf{P} = \textbf{NP} then the task of determining whether a mathematical formula is derivable in our preferred mathematical theory by a proof of feasible length would be checkable by an efficient algorithm. Gödel suggests that this would have the following consequence:

[D]espite the unsolvability of the Entscheidungsproblem the mental effort of the mathematician in the case of yes-or-no questions could be completely replaced by machines.[38] (Gödel 1956, 612)

Building on this observation, several more recent commentators (e.g. Impagliazzo 1995, Arora and Barak 2009, and Fortnow 2013) have also underscored the significance of Open Question 1 by suggesting that P=NP\textbf{P} = \textbf{NP} would entail the dispensability of creativity not just in mathematics, but also for a number of other tasks – e.g. theory construction or music composition – which are traditionally thought to involve elements of non-algorithmic insight.
But although the coincidence of P\textbf{P} and NP\textbf{NP} would have intriguing consequences, it also seems likely that the discovery of a proof validating the consensus view that P≠NP\textbf{P} \neq \textbf{NP} would be regarded as foundationally significant. As we have seen, the sort of evidence most often cited in favor of the proper inclusion of P\textbf{P} in NP\textbf{NP} is the failure of protracted attempts to find polynomial time algorithms for problems in NP\textbf{NP} in which we have a strong practical interest either in deciding in practice or in proving to be intractable.[39] A proof that P≠NP\textbf{P} \neq \textbf{NP} would thus not only have the effect of validating such inductive evidence, but it would provide additional evidence that the Cobham-Edmonds Thesis provides a correct analysis of the pre-theoretical notion of feasibility. In particular, it would allow us to unconditionally assert that NP\textbf{NP}-hard problems are intractable in the general case.
Recall, however, that such inductive considerations form only part of the overall evidence which can be cited in favor of P≠NP\textbf{P} \neq \textbf{NP}. In particular, various heuristic considerations also point to the non-coincidence of the classes NP\textbf{NP} and coNP\textbf{coNP} and of PH\textbf{PH} and PSPACE\textbf{PSPACE}, and hence to positive answers for Open Questions 2 and 3. But were it to be the case that P=NP\textbf{P} = \textbf{NP}, then these questions would be resolved negatively in a manner which runs strongly counter to our current expectations (see Sections 
 3.4.1 and 3.4.2).
Given the convergence of several forms of non-demonstrative evidence for the conjecture that P≠NP\textbf{P} \neq \textbf{NP}, it is also reasonable to ask why this statement has proven so difficult to resolve in practice. For note that although this statement originates in theoretical computer science, it may be easily formulated as statements about natural numbers. In particular, P≠NP\textbf{P} \neq \textbf{NP} is equivalent to the statement that for all indices ee and exponents kk, there exists a propositional formula ϕ\phi such that the deterministic Turing machine TeT_e does not correctly decide ϕ\phi’s membership in SAT\sc{SAT} in |ϕ|k\lvert \phi\rvert^k steps. Using familiar techniques from the arithmetization of syntax, it is not difficult to see that this statement can be formalized in the language of first-order arithmetic as a Π02\Pi^0_2-statement – i.e. a statement Θ\Theta of the form ∀x∃yψ(x,y)\forall x \exists y \psi(x,y) where ψ(x,y)\psi(x,y) contains only bounded numerical quantifiers.
Based on its logical form alone, we cannot currently exclude the possibility that Θ\Theta is independent not just of first-order Peano arithmetic [PA\mathsf{PA}], but of even stronger axiom systems such as ZFC\textsf{ZFC}. But although early results suggested P≠NP\textbf{P} \neq \textbf{NP} may be independent of some very weak axiomatic theories (e.g. DeMillo and Lipton 1980), it is now believed that this statement is unlikely to be independent of stronger theories like PA\mathsf{PA} which approximate the mathematical axioms we employ in practice. In particular, Ben-David and Halevi (1992) showed that if it were possible to use current proof theoretic techniques to show the independence of Θ\Theta from PA\textsf{PA}, then this demonstration itself would also show that NP\textbf{NP} is very close to being in P\textbf{P} in a sense which can be made precise using techniques from circuit complexity.[40] As this is thought to be implausible, Aaronson (2003) suggests that we currently possess no reason to suspect that P≠NP\textbf{P} \neq \textbf{NP} is more likely to be independent of PA\textsf{PA} (let alone ZFC\mathsf{ZFC}) than other currently open number theoretic statements.
But at the same time, a consensus has also developed that it is unlikely that we will be able to settle the status of P≠NP?\textbf{P} \neq \textbf{NP}? on the basis of currently known methods of proof. A familiar example is the method of diagonalization, as employed in the proof of the undecidability of the classical Halting Problem (from which it follows that the recursive languages are properly included in the recursively enumerable ones). We have seen that a variant of this method can be used to show P⊊EXP\textbf{P} \subsetneq \textbf{EXP} in the proof of Theorem 3.1. But diagonalization proofs yielding such separations typically relativize to oracle-based computation in the following sense: if such a proof yields C1≠C2\textbf{C}_1 \neq \textbf{C}_2 for complexity classes C1\textbf{C}_1 and C2\textbf{C}_2, then a suitable modification will also typically yield CA1≠CA2\textbf{C}^A_1 \neq \textbf{C}^A_2 for all oracles A⊆{0,1}∗A \subseteq \{0,1\}^*.[41]  Baker, Gill, and Solovay (1975) famously established the existence of oracles AA and BB such that PA=NPA\textbf{P}^A = \textbf{NP}^A and PB≠NPB\textbf{P}^B \neq \textbf{NP}^B. As we would expect a proof of P≠NP\textbf{P} \neq \textbf{NP} based on diagonalization to relativize to both AA and BB, this suggests that it is not possible to use this method to separate P\textbf{P} and NP\textbf{NP}.
Despite the negative character of this and other results which are often taken to bear on the status of P≠NP\textbf{P} \neq \textbf{NP}?, resolving this and the other open questions remains an important topic of research in theoretical computer science. Several programs for separating complexity classes have recently been explored using techniques from circuit complexity (e.g. Razborov and Rudich 1994), proof theory (e.g. Buss 1999), and algebraic geometry (e.g. Mulmuley and Sohoni 2001). However, the current consensus (e.g. Fortnow 2009) is that these approaches are still in need of substantial refinement or that genuinely new methods will be required in order to yield the desired separations.
It thus seems reasonable to summarize the current status of the P≠NP\textbf{P} \neq \textbf{NP}? problem as follows: (i) P≠NP\textbf{P} \neq \textbf{NP} is widely believed to be true on the basis of convergent inductive and heuristic evidence; (ii) we currently have no reason to suspect that this statement is formally independent of the mathematical theories which we accept in practice; but (iii) a proof P≠NP\textbf{P} \neq \textbf{NP} is still considered to be beyond the reach of current techniques.
4.2 Satisfiability, validity, and model checking
We have seen that logic provides many examples of problems which are studied in complexity theory. Of these, the most often considered are satisfiability, validity, and model checking. In order to formulate these problems uniformly, it is convenient to take a logic L\mathcal{L} to be a triple ⟨FormL,A,⊨L⟩\langle \text{Form}_{\mathcal{L}}, \mathfrak{A}, \models_{\mathcal{L}} \rangle  consisting of a set of formulas FormL\text{Form}_{\mathcal{L}}, a class of structures A\mathfrak{A} in which the members of FormL\text{Form}_{\mathcal{L}} are interpreted, and a satisfaction relation ⊨L\models_{\mathcal{L}} which holds between structures and formulas. We may assume that FormL\text{Form}_{\mathcal{L}} is specified in the manner typified by propositional logic – i.e. as a set of syntactic primitives and inductive formulation rules. On the other hand, the definitions of both A\mathfrak{A} and ⊨L\models_{\mathcal{L}} will vary according to the sort of logic in question – e.g. in the case of classical propositional logic, A\mathfrak{A} will be the class of atomic valuations v:N→{0,1}v:\mathbb{N} \rightarrow \{0,1\} and v⊨Lϕv \models_{\mathcal{L}} \phi holds just in case ϕ\phi is true with respect to vv (as defined by the usual truth table rules), and in the case of modal logic A\mathfrak{A} will typically be a class of Kripke models satisfying a given frame condition and ⊨L\models_{\mathcal{L}} is the standard forcing relation A,w⊩ϕ\mathcal{A},w \Vdash \phi which holds between a model, a world, and a formula.
Given a logic L\mathcal{L} specified in this manner, we may now formulate the following problems:
SATISFIABILITYL \sc{SATISFIABILITY}_{\mathcal{L}}\ 
Given a formula ϕ∈FormL\phi \in \text{Form}_{\mathcal{L}}, does there exist a structure A∈A\mathcal{A} \in \mathfrak{A} such that A⊨Lϕ\mathcal{A} \models_{\mathcal{L}} \phi?
VALIDITYL \sc{VALIDITY}_{\mathcal{L}}\ 
Given a formula ϕ∈FormL\phi \in \text{Form}_{\mathcal{L}}, is it the case that for all A∈A\mathcal{A} \in \mathfrak{A}, A⊨Lϕ\mathcal{A} \models_{\mathcal{L}} \phi?
MODEL CHECKINGL \sc{MODEL}\ \sc{CHECKING}_{\mathcal{L}}\ 
Given a formula ϕ∈FormL\phi \in \text{Form}_{\mathcal{L}} and a structure A∈A\mathcal{A} \in \mathfrak{A}, is it the case that A⊨Lϕ\mathcal{A} \models_{\mathcal{L}} \phi?
To specify these problems formally, a suitable encoding of the members of FormL\text{Form}_{\mathcal{L}} and A\mathfrak{A} as finite binary strings must be provided. Although this is typically unproblematic in the case of FormL\text{Form}_{\mathcal{L}}, A\mathfrak{A} may sometimes include infinite structures. In this case, the model checking problem is only considered for finite structures (which we must also assume are encoded as finite strings – e.g. in the manner described by Marx 2007). And in cases where a logic is standardly characterized proof theoretically rather than semantically – i.e. as the set of formulas derivable from some set of axioms of ΓL\Gamma_{\mathcal{L}} rather than the class of formulas true in all structures – the validity problem is understood to coincide with the problem of deciding whether ϕ\phi is derivable from ΓL\Gamma_{\mathcal{L}}. In such cases, the satisfiability and model checking problems are generally not considered.
The problems SATISFIABILITYL\sc{SATISFIABILITY}_{\mathcal{L}}, VALIDITYL\sc{VALIDITY}_{\mathcal{L}}, and MODEL CHECKINGL\sc{MODEL}\ \sc{CHECKING}_{\mathcal{L}} have been studied for many of the logics employed in philosophy, computer science, and artificial intelligence. A number of results are summarized in Table 1, focusing on systems surveyed in other articles in this encyclopedia. The reader is referred to these and the other references provided for the definition of the fragment or system in question as well as the relevant definitions of A\mathfrak{A}, ⊨L\models_{\mathcal{L}}, or ⊢ΓL\vdash_{\Gamma_{\mathcal{L}}}.[42]



Classical Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING
full propositional
NP\textbf{NP}-complete
coNP\textbf{coNP}-complete
∈P\in \textbf{P}
   (Cook 1971; Buss 1987) 
Horn clauses
P-complete
P-complete
∈P\in \textbf{P}
   (Papadimitriou 1994) 
full first-order
coRE\textbf{coRE}-complete
RE\textbf{RE}-complete
PSPACE\textbf{PSPACE}-complete
   (Church 1936a; Vardi 1982) 
monadic first-order
NEXP\textbf{NEXP}-complete
coNEXP\textbf{coNEXP}-complete

   (Löwenheim 1967; Lewis 1980) 
∃∗∀∗\exists^* \forall^*
NEXP\textbf{NEXP}-complete
coNEXP\textbf{coNEXP}-complete

   (Bernays and Schöfinkel 1928; Lewis 1980) 
∃∗∀∃∗\exists^* \forall \exists^*
EXP\textbf{EXP}-complete
EXP\textbf{EXP}-complete

   (Ackermann 1928; Lewis 1980) 
∃∗∀∀∃∗\exists^* \forall \forall \exists^*
NEXP\textbf{NEXP}-complete
coNEXP\textbf{coNEXP}-complete

   (Gödel 1932; Lewis 1980) 






Intuitionistic Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING
full propositional
PSPACE\textbf{PSPACE}-complete
PSPACE\textbf{PSPACE}-complete
P\textbf{P}-complete
   (Statman 1979; Mundhenk and Weiß 2010) 

full first-order
coRE\textbf{coRE}-complete
RE\textbf{RE}-complete

   (Church 1936a; Gödel 1986a) 




Modal Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

K,T,S4\textsf{K},\textsf{T},\textsf{S4}
PSPACE\textbf{PSPACE}-complete
PSPACE\textbf{PSPACE}-complete
P\textbf{P}-complete
   (Ladner 1977; Fischer and Ladner 1979; Mundhenk and Weiß 2010) 

S5\textsf{S5}
NP\textbf{NP}-complete
coNP\textbf{coNP}-complete
∈P\in \textbf{P}
   (Ladner 1977; Mundhenk and Weiß 2010) 



Epistemic Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

S5n\mathsf{S5}_n, n≥2n \geq 2
PSPACE\textbf{PSPACE}-complete
PSPACE\textbf{PSPACE}-complete

   (Halpern and Moses 1992; Fagin et al. 1995) 

KCn,TCn\mathsf{K}^{\mathsf{C}}_n, \mathsf{T}^{\mathsf{C}}_n, n≥2n \geq 2  
EXP\textbf{EXP}-complete
EXP\textbf{EXP}-complete

   (Halpern and Moses 1992; Fagin et al. 1995) 




Provability Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

GL\mathsf{GL}
PSPACE\textbf{PSPACE}-complete
PSPACE\textbf{PSPACE}-complete

   (Chagrov 1985) 



Justification Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

LP\textsf{LP}
ΣP2\Sigma^P_2-complete
ΠP2\Pi^P_2-complete

   (Kuznets 2000; Milnikel 2007) 



Dynamic Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

PDL\mathsf{PDL}
EXP\textbf{EXP}-complete
NEXP\textbf{NEXP}-complete
P\textbf{P}-complete
   (Fischer and Ladner 1979; Kozen and Parikh 1981; Lange 2006)



Temporal Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

PLTL\textsf{PLTL}
PSPACE\textbf{PSPACE}-complete
PSPACE\textbf{PSPACE}-complete
PSPACE\textbf{PSPACE}-complete
   (Sistla and Clarke 1985) 

CTL\textsf{CTL}
EXP\textbf{EXP}-complete
EXP\textbf{EXP}-complete
∈P\in \textbf{P}
   (Emerson and Jutla 1988; Vardi and Stockmeyer 1985; Wolper 1986) 

CTL∗\textsf{CTL}^*
2-EXP\textbf{2-EXP}-complete
2-EXP\textbf{2-EXP}-complete
PSPACE\textbf{PSPACE}-complete
   (Emerson and Jutla 1988; Vardi and Stockmeyer 1985; Clarke et al. 1986)



Relevance Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

T,E,R\textsf{T}, \textsf{E}, \textsf{R}

RE\textbf{RE}-complete

   (Urquhart 1984) 



Linear Logic
SATISFIABILITY
VALIDITY
MODEL CHECKING

MLL\textsf{MLL}

NP\textbf{NP}-complete

   (Kanovich 1994) 

MALL\textsf{MALL}

PSPACE\textbf{PSPACE}-complete

   (Lincoln et al. 1992) 

LL\textsf{LL}

RE\textbf{RE}-complete

   (Lincoln 1995) 

Table 1. The complexity of the satisfiability, validity, and model checking problems for some common logics.

4.3 Proof complexity
We have seen that the satisfiability and validity problems for propositional logic are respectively complete for NP\textbf{NP} and coNP\textbf{coNP}. Although these problems are characterized in terms of the semantics for propositional logic, certain questions about its proof theory may also be addressed using techniques from complexity theory. By a proof system P\mathcal{P} for propositional logic we understand a definition of the symbol ⊢P\vdash_{\mathcal{P}} which characterizes what it means for a formula ϕ\phi to be derivable from a given set of axioms and rules. We write ⊢Pϕ\vdash_{\mathcal{P}} \phi just in case there is a derivation D\mathcal{D} (e.g. a finite tree or sequence of formulas) with ϕ\phi as its conclusion in the system P\mathcal{P}. Examples of such formalisms include standard definitions of Hilbert systems P1\mathcal{P}_1,[43] natural deduction systems P2\mathcal{P}_2, and sequent systems P3\mathcal{P}_3 for propositional logic (see, e.g., Troelstra and Schwichtenberg 2000; Negri and Von Plato 2001). Each of these systems may be shown to be sound and complete for propositional validity – i.e. for all propositional formulas ϕ\phi, ϕ∈VALID\phi \in \sc{VALID} if and only if ⊢Piϕ\vdash_{\mathcal{P}_i} \phi for i∈{1,2,3}i \in \{1,2,3\}.
In the context of complexity theory, it is convenient to reformulate the definition of a proof system as a mapping P:{0,1}∗→VALID\mathcal{P}: \{0,1\}^* \rightarrow  \sc{VALID} whose domain consist of all binary string and whose range is the class of all valid formulas. Recall, for instance, that a Hilbert derivation is a finite sequences of formulas ψ1,…,ψn\psi_1,\ldots,\psi_n each of whose members is either a logical axiom or follows from earlier members by modus ponens.  A finite sequence of formulas  can be encoded as a binary string so that it is may be recognized in polynomial time if  x∈{0,1}∗x \in \{0,1\}^* is the code of a well-formed proof.  P \mathcal{P} can now be defined so if x x encodes such a derivation, then P(x)=ψn \mathcal{P}(x) = \psi_n  (i.e. the conclusion of the derivation) and P(x) \mathcal{P}(x) is some fixed tautology otherwise.

Such a system is said to be polynomially bounded if there exists a polynomial p(n)p(n) such that for all ϕ∈VALID\phi \in \sc{VALID}, there is a derivation D\mathcal{D} such that P(⌜D⌝)=ϕ\mathcal{P}(\ulcorner \mathcal{D} \urcorner) = \phi and |⌜D⌝|≤p(|ϕ|)\lvert \ulcorner \mathcal{D}\urcorner \rvert \leq p(\lvert \phi \rvert) – i.e. just in case all tautologies of size nn possess P\mathcal{P}-proofs of size at most p(n)p(n).
The basic observation about such systems is as follows:

Theorem 4.1 (Cook and Reckhow 1979) There exist a polynomially bounded proof system if and only if NP=coNP\textbf{NP} = \textbf{coNP}.[44]
Since it is strongly suspected that NP≠coNP\textbf{NP} \neq \textbf{coNP} (see 
 Section 3.4.1), the current consensus is that polynomial proof systems do not exist. At present, however, the failure of polynomial boundedness has not been proven for most familiar proof systems, inclusive of P1\mathcal{P}_1, P2\mathcal{P}_2, and P3\mathcal{P}_3.
A natural question to ask about a proof system P\mathcal{P} is thus whether it is possible to identify classes of tautologies HH which are ‘hard’ in the sense that any P\mathcal{P}-proof demonstrating that ϕ∈H\phi \in H is valid must be infeasibly long relative to the size of ϕ\phi. A positive answer was obtained by Haken (1985) for the system known as resolution on which many automated theorem-provers are based. Haken’s proof made use of Cook and Reckhow’s (1979) observation that we may formulate the Pigeon Hole Principle (PHP) – i.e. the statement that any assignment of n+1n+1 pigeons to nn holes must assign two pigeons to some hole – in propositional logic by using the atomic letter PijP_{ij} to express that pigeon ii gets placed in hole jj. The formula
PHPn=⋀0≤i≤n ⋁0≤j<nPij→⋁0≤i<m≤n⋁0≤j<n(Pij∧Pmj)
\text{PHP}_n = \bigwedge_{0 \leq i \leq n} \ \bigvee_{0 \leq j \lt n} P_{ij} \rightarrow \bigvee_{0 \leq i \lt m \leq n} \bigvee_{0 \leq j \lt n} (P_{ij} \wedge P_{mj})

which formalizes the nn-pigeon version of PHP is thus a tautology for each nn. PHPn\text{PHP}_n is hence provable in any complete proof system for propositional logic.
Haken showed that any resolution proof of PHPn\text{PHP}_n must have size at least exponential in nn. From this it follows that resolution is not polynomially bounded. However, it was later shown by Buss (1987) that the system P1\mathcal{P}_1 (and hence also systems like P2\mathcal{P}_2, P3\mathcal{P}_3 which can be shown to efficiently simulate P1\mathcal{P}_1) do admit proofs of PHPn\text{PHP}_n which are of size polynomial in nn.  One subsequent direction of research in proof complexity has been to identify additional proof systems for which PHP or related combinatorial principles are also hard.  See, e.g., Buss (2012), Segerlind (2007).
4.4 Descriptive complexity
Another connection between logic and computational complexity is provided by the subject known as descriptive complexity theory. As we have seen, a problem XX is taken to be ‘complex’ in the sense of computational complexity theory in proportion to how difficult it is to decide algorithmically. On the other hand, descriptive complexity takes a problem to be ‘complex’ in proportion to the logical resources which are required to describe its instances. In other words, the descriptive complexity of XX is measured according to the sort of formulas which is needed to define its instances relative to an appropriate background class of finitary structures.
Descriptive complexity begins with the observation that since computational problems are comprised by finite combinatorial objects (e.g. strings, graphs, formulas, etc.), it is possible to describe their instances as finite structures in the conventional sense of first-order model theory. In particular, given a problem XX, we associate with each of its instances x∈Xx \in X a finite structure Ax\mathcal{A}_x over a relational signature τ\tau whose non-logical vocabulary will depend on the type of objects which comprise XX.[45]
Given such a signature τ\tau, we define Mod(τ)\text{Mod}(\tau) to be the class of all τ\tau-structures with finite domain. In the context of descriptive complexity theory, a logic L\mathcal{L} is taken to be an extension of the language of first-order logic with one or more classes of additional expressions such as higher-order quantifiers or fixed-point operators. These are treated semantically as logical symbols. If L\mathcal{L} is such a logic, and τ\tau a signature, we write SentL(τ)\text{Sent}_{\mathcal{L}(\tau)} to denote the class of well-formed sentences constructed using the logical symbols from L\mathcal{L} and the relational symbols from τ\tau. A sentence ϕ∈SentL(τ)\phi \in \text{Sent}_{\mathcal{L}(\tau)} is said to define a problem XX just in case XX is coextensive with the set of τ\tau-structures satisfying ϕ\phi – i.e.
{Ax∣x∈X}={A∈Mod(τ)∣A⊨ϕ}
\{\mathcal{A}_x \mid x \in X \} = \{\mathcal{A} \in \text{Mod}(\tau) \mid \mathcal{A} \models \phi \}

If C\textbf{C} is a complexity class, then a logic L\mathcal{L} is said to capture C\textbf{C} just in case for every problem X∈CX \in \textbf{C}, there is a signature τ\tau and a formula ϕ∈FormL(τ)\phi \in \text{Form}_{\mathcal{L}(\tau)} which defines XX.
Descriptive characterizations have been obtained for many of the major complexity classes considered in Section 3, several of which are summarized in Table 2. The first such characterization was established with respect to second-order existential logic (SO∃\mathsf{SO}\exists). The language of this system includes formulas of the form ∃Z1…∃Znϕ(→x,Z1,…,Zn)\exists Z_1 \ldots \exists Z_n \phi(\vec{x},Z_1,\ldots,Z_n) where the variables ZiZ_i are second-order and ϕ\phi itself contains no other second-order quantifiers. Fagin (1974) established the following:
Theorem 4.2 
NP\textbf{NP} is captured by the logic SO∃\mathsf{SO}\exists.
This result provided one of the first machine independent characterizations of an important complexity class – i.e. one whose formulation does not make reference to a specific model of computation such as T\mathfrak{T} or A\mathfrak{A}. The availability of such characterizations is often taken to provide additional evidence for the mathematical robustness of classes like NP\textbf{NP}.
Theorem 4.2 generalizes to provide a characterization of the classes which comprise the Polynomial Hierarchy. For instance, the logics Σ1i\Sigma^1_i and Π1i\Pi^1_i uniformly capture the complexity classes ΣPi\Sigma^P_i and ΠPi\Pi^P_i (where SO∃=Σ11\mathsf{SO}\exists = \Sigma^1_1 ). Moreover SO\mathsf{SO} (i.e. full second-order logic) captures PH\textbf{PH} itself. On the other hand, it can also be shown that first-order logic (FO\mathsf{FO}) captures only a very weak complexity class known as AC0\textbf{AC}^0 consisting of languages decidable by polynomial size circuits of constant depth.
In order to characterize additional classes, other means of extending the expressive capacity of first-order logic must be considered such as adding least fixed point or transitive closure operators. Consider, for instance, a formula ψ(R,→x)\psi(R,\vec{x}) in which the nn-ary relation RR only appears positively (i.e. in the scope of an even number of negations) and →x\vec{x} is of length mm. If AA is the domain of a structure A\mathcal{A}, then such a formula will induce a monotone mapping of type Φψ(R,→x)\Phi_{\psi(R,\vec{x})} from the power set of AnA^n to the power set of AmA^m defined by Φψ(R,→x)(B)={→a∈Am∣ARB⊨ψ(R,→a)}\Phi_{\psi(R,\vec{x})}(B) = \{\vec{a} \in A^m \mid \mathcal{A}^R_B \models \psi(R,\vec{a}) \} where ARB\mathcal{A}^R_B denotes the model in which the symbol RR is interpreted as B⊆AnB \subseteq A^n and is just like A\mathcal{A} otherwise. In such a case it follows that there will exist a least fixed point for the mapping Φψ(R,→x)\Phi_{\psi(R,\vec{x})} – i.e. a set F⊆AnF \subseteq A^n such that Φψ(R,→x)(F)=F\Phi_{\psi(R,\vec{x})}(F) = F and which is contained in all other sets with this property (see, e.g., Moschovakis 1974). Let us denote this set by FixA(ψ(R,→x))\text{Fix}^{\mathcal{A}}(\psi(R,\vec{x})).[46]
The logic FO(LFP)\textsf{FO}(\texttt{LFP}) can now be defined as the extension of first-order logic with the new relation symbols LFPψ(R,→x)\texttt{LFP}_{\psi({R,\vec{x}})} for each formula ψ(R,→x)\psi(R,\vec{x}) in which the relation variable appears only positively and with new atomic formulas of the form LFPψ(R,→x)(→t)\texttt{LFP}_{\psi({R,\vec{x}})}(\vec{t}). Such formulas are interpreted as follows: A⊨LFPψ(R,→x)(→t)\mathcal{A} \models \texttt{LFP}_{\psi({R,\vec{x}})}(\vec{t}) if and only if →tA∈FixA(ψ(R,→x))\vec{t}^{\mathcal{A}} \in \text{Fix}^{\mathcal{A}}(\psi(R,\vec{x})). The logic FO(TC)\textsf{FO}(\texttt{TC})is similarly defined by adding a transitive closure operator TCψ(→x)(→x)\texttt{TC}_{\psi(\vec{x})}(\vec{x}) which holds of a term →t\vec{t} just in case →tA\vec{t}^{\mathcal{A}} is in the transitive closure of the relation denoted by ψ(→x)\psi(\vec{x}) in A\mathcal{A}.  The logic SO(LFP)\textsf{SO}(\texttt{LFP}) and SO(TC)\textsf{SO}(\texttt{TC}) are defined analogously by adding these operators to SO\textsf{SO} and allowing them to apply to formulas containing second-order variables.
We may now state another major result of descriptive complexity theory:
Theorem 4.3 (Immerman 1982; Vardi 1982) P\textbf{P} is captured by FO(LFP)\textsf{FO}(\texttt{LFP}) relative to ordered models (i.e. models A\mathcal{A} for structures interpreting ≤\leq as a linear order on AA).
Immerman (1999, p. 61) describes Theorem 4.3 as “increas[ing] our intuition that polynomial time is a class whose fundamental nature goes beyond the machine models with which it is usually defined”.  Taken in conjunction with Theorem 4.2 it also provides a logical reformulation of the P≠NP?\textbf{P} \neq \textbf{NP}? problem itself – i.e. P≠NP\textbf{P} \neq \textbf{NP} if and only if there exists a class of ordered structures definable in existential second-order logic which is not definable by a formula of FO(LFP)\textsf{FO}(\texttt{LFP}).
 On the other hand, the restriction to ordered structures in the formulation of Theorem 4.3 is known to be essential in the sense that there are simply describable languages in P\textbf{P} – e.g.
PARITY={w∈{0,1}∗:w contains an odd number of 1s}
 \sc{PARITY} = \{w \in \{0,1\}^* : w \text{ contains an odd number of 1s}\}


– which cannot be define over FO(LFP)\textsf{FO}(\texttt{LFP}) without using ≤\leq. More generally, the question of there exists a logic which captures  P \textbf{P} over unordered structures is currently one of the major open questions in descriptive complexity.  See, e.g., (Ebbinghaus and Flum 1999) and (Chen and Flum 2010).



Complexity class
Logic
Reference


AC0\textbf{AC}^0
FO\mathsf{FO}
(Immerman 1999)


NL\textbf{NL}
FO(TC)\textsf{FO}(\texttt{TC})
(Immerman 1987)


P\textbf{P}
FO(LFP)\textsf{FO}(\texttt{LFP})
(Immerman 1982), (Vardi 1982)


NP\textbf{NP}
SO∃\textsf{SO}\exists
(Fagin 1974)


ΣPi\Sigma^P_i
SOΣ1i\textsf{SO}\Sigma^1_i
(Stockmeyer 1977)


ΠPi\Pi^P_i
SOΠ1i\textsf{SO}\Pi^1_i
(Stockmeyer 1977)


PH\textbf{PH}
SO\textsf{SO}
(Stockmeyer 1977)


PSPACE\textbf{PSPACE}
SO(TC)\textsf{SO}(\texttt{TC})
(Immerman 1987)


EXP\textbf{EXP}
SO(LFP)\textsf{SO}(\texttt{LFP})
(Immerman 1999)


Table 2. Descriptive characterization of complexity classes.

4.5 Bounded arithmetic
Another connection between logic and computational complexity is provided by first-order arithmetical theories which are similar in form to familiar systems such as Primitive Recursive Arithmetic and Peano arithmetic. Connections between formal arithmetic and computability theory have been known since the 1940s – e.g. the sets of natural numbers definable by Δ01\Delta^0_1-formulas and Σ01\Sigma^0_1-formulas in the standard model of arithmetic respectively correspond to the recursive and recursively enumerable sets, the provably total functions of IΣ01\textrm{I}\Sigma^0_1 correspond to the primitive recursive functions while those of PA\mathsf{PA} correspond to the ϵ0\epsilon_0-recursive functions (cf., e.g.,   Schwichtenberg 2011). From the 1970s onwards, a number of similar results have been obtained which link the levels of the Polynomial Hierarchy to a class of first-order theories collectively known as bounded arithmetics.
In the course of studying the relationship between arithmetic and complexity theory, it is often useful to consider functions in addition to sets. For instance we may consider the class FP=df◻P1\textbf{FP} =_{\text{df}}\Box^P_1 of functions computable by a deterministic Turing machine in polynomial time. Similarly, we define ◻Pn+1\Box^P_{n+1} to be the class of functions computable by a deterministic Turing machine in polynomial time with the use of an oracles for a set in level ΣPn\Sigma^P_n of PH\textbf{PH}. As in the case of PH\textbf{PH}, it is not known whether the hierarchy ◻P1⊆◻P2⊆…\Box^P_1 \subseteq \Box^P_2 \subseteq \ldots collapses.
A first link between formal arithmetic and complexity was provided by Cobham’s (1965) original characterization of FP\textbf{FP} in terms of a functional algebra similar to that by which the primitive recursive functions are defined. The class in question is generated by the following class of basis functions F0\mathcal{F}_0:
z(x)=0,s0(x)=2⋅x,s1(x)=2x+1,πin(x1,…,xn)=xi,x#y=2|x|⋅|y|
z(x) = 0, s_0(x) = 2 \cdot x, s_1(x) = 2 x + 1, \pi^i_n(x_1, \dots ,x_n) = x_i, x \# y = 2^{\lvert x\rvert \cdot \lvert y\rvert}

We also define the following variant of primitive recursion:

Definition 4.1  The function f(→x,y)f(\vec{x},y) is said to be defined from g(→x),h0(→x,y,z),h1(→x,y,z)g(\vec{x}), h_0(\vec{x},y,z), h_1(\vec{x},y,z) and k(→x,y)k(\vec{x},y) by limited recursion on notation just in case
f(→x,0)=g(→x)f(→x,s0(y))=h0(→x,y,f(→x,y))f(→x,s1(y))=h1(→x,y,f(→x,y))
\begin{aligned}
 f(\vec{x},0) &amp;= g(\vec{x})\\
 f(\vec{x},s_0(y)) &amp;= h_0(\vec{x},y,f(\vec{x},y)) \\
 f(\vec{x},s_1(y)) &amp;= h_1(\vec{x},y,f(\vec{x},y))
\end{aligned}

and f(→x,y)≤k(→x,y)f(\vec{x},y) \leq k(\vec{x},y) for all →x,y\vec{x},y.

We define the class F\mathcal{F} of functions definable by limited recursion on notation to be the least class containing F0\mathcal{F}_0 and closed under composition and the foregoing scheme. A slight variant of Cobham’s original result may now be stated as follows:
Theorem 4.4  (Cobham 1965; Rose 1984) f(→x)∈FPf(\vec{x}) \in \textbf{FP} if and only if f(→x)∈Ff(\vec{x}) \in \mathcal{F}.
Like Theorems 4.2 and 4.3, Theorem 4.4 is significant because it provides another machine-independent characterization of an important complexity class. Recall, however, that Cobham was working at a time when the mathematical status of the notion of feasibility was still under debate. So it is also reasonable to ask if the definition of F\mathcal{F} can  be understood as providing an independently motivated analysis of feasible computability akin to the analyses which Church and Turing are often said to have provided for effective computability.
Relative to the standards discussed in  Section 1.1 
 it seems reasonable to maintain  that the basis functions F0\mathcal{F}_0 are feasibly computable and also that this property is preserved under composition. Now suppose that f(x,y)f(x,y) is defined by limited recursion on notation from g(x),h0(x,y,z),h1(x,y,z)g(x), h_0(x,y,z), h_1(x,y,z) and k(x,y)k(x,y). In this case, f(x,y)=hi(x,⌈y2⌉,f(x,⌈y2⌉))f(x,y) = h_i(x,\lceil \frac{y}{2} \rceil,f(x,\lceil \frac{y}{2} \rceil)) depending on whether y>0y \gt 0 is even (i=0i = 0) or odd (i=1i = 1). From this it follows that the depth of the recursion required to compute the value of f(x,y)f(x,y) will be proportional to log2(y)\log_2(y) – i.e. proportional to the length of yy’s binary representation, as opposed to yy itself as when f(x,y+1)f(x,y+1) is defined as h(x,y,f(x,y))h(x,y,f(x,y)) by ordinary primitive recursion. This suggests that feasibility is also preserved when a function is defined by limited recursion on notation.
It is harder to motivate the inclusion of the function x#yx \# y in F0\mathcal{F}_0 and of the condition f(x,y)≤k(x,y)f(x,y) \leq k(x,y) in Definition 4.1 on pre-theoretical grounds alone. For these features of the definition of F\mathcal{F} can be see to have precisely the effect of placing a polynomial bound on the auxiliary functions which can be computed during the sort of length-bounded recursion just described. On the other hand, such indirect reference to polynomial rates of growth is avoided in similar functional characterizations of FP\textbf{FP} due to Leivant (1994) (using a form of positive second-order definability over strings) and Bellantoni and Cook (1992) (using a structural modification of the traditional primitive recursion scheme).
Direct reference to polynomial rates of growth is also avoided in the formulation of the first-order arithmetical theory now known as IΔ0\text{I}\Delta_0 (which was originally introduced by Parikh (1971) under the name PB\textsf{PB}). IΔ0\text{I}\Delta_0 is formulated over the traditional language of first-order arithmetic – i.e. La={0,s,+,x,<}\mathcal{L}_a = \{0,s,+,x,\lt\} – and consists of the axioms of Robinson’s Q\mathsf{Q} together with the restriction of the induction scheme of PA\mathsf{PA} arithmetic to Δ0\Delta_0-formulas – i.e. those containing only bounded quantifiers of the form ∀x≤t\forall x \leq t or ∃x≤t\exists x \leq t where tt is a term not containing xx.
Prior to this Bennett (1962) had shown that there exists a Δ0\Delta_0-formula ε(x,y,z)\varepsilon(x,y,z) which defines the graph of the exponentiation function relative to the standard model of arithmetic. IΔ0\text{I}\Delta_0 proves that ε(x,y,z)\varepsilon(x,y,z) satisfies the standard defining equations for exponentiation. But it may also be shown that this theory does not prove the totality of exponentiation under this or any other definition in the following sense:
Theorem 4.5  (Parikh 1971) 
Suppose that IΔ0⊢∀→x∃yϕ(→x,y)\text{I}\Delta_0 \vdash \forall \vec{x}  \exists y \phi(\vec{x},y) where ϕ(→x,y)\phi(\vec{x},y) is itself Σ1\Sigma_1 . Then for some La\mathcal{L}_a-term t(→x)t(\vec{x}), IΔ0⊢∀→x∃y≤t(→x)ϕ(→x,y)\text{I}\Delta_0 \vdash \forall \vec{x} \exists y \leq t(\vec{x}) \phi(\vec{x},y).
Recall that a function f(→x)f(\vec{x}) is provably total in a theory T\mathsf{T} just in case there is a Σ1\Sigma_1-formula ϕf(→x,y)\phi_f(\vec{x},y) defining the graph of f(→x)f(\vec{x}) in the language of T\mathsf{T} such that T⊢∀→x∃!yϕf(→x,y)\mathsf{T} \vdash \forall \vec{x} \exists ! y \phi_f(\vec{x},y). Since terms in the language of arithmetic are polynomials with positive coefficients, it follows from Theorem 4.3 that all of the provably total functions of IΔ0\text{I}\Delta_0 are of polynomial rate of growth. In particular, exponentiation is not provably total in this theory.  For this reason Parikh referred to IΔ0\text{I}\Delta_0 as an “anthropomorphic system”.[47]
 In order to obtain arithmetical theories which describe P\textbf{P} and the higher levels of PH\textbf{PH} precisely, two approaches have been pursued, respectively due to Buss (1986) and to Zambella (1996). Buss presented a sequence of first-order theories
S12⊆T12⊆S22⊆…Si2⊆Ti2⊆Si+12⊆…
\mathsf{S}^1_2 \subseteq \mathsf{T}^1_2 \subseteq \mathsf{S}^2_2 \subseteq \ldots \mathsf{S}^i_2 \subseteq \mathsf{T}^i_2 \subseteq \mathsf{S}^{i+1}_2 \subseteq \ldots

whose provably total functions correspond to the levels of the hierarchy ◻Pi\Box^P_i. These theories are stated over the language Lba={0,s,+,x,<,|⋅|,⌊x2⌋,#}\mathcal{L}^b_a = \{0,s,+,x,\lt,\lvert \cdot\rvert,\lfloor \frac{x}{2} \rfloor, \#\}, where the intended interpretation of |x|\lvert x\rvert is ⌈log2(x+1)⌉\lceil \log_2(x+1) \rceil, and of x#yx\#y is 2|x|⋅|y|2^{\lvert x\rvert \cdot \lvert y\rvert} as above. Whereas a traditional bounded quantifier is of the form ∀x<t\forall x \lt t or ∃x<t\exists x \lt t, a so-called sharply bounded quantifier is of the form ∀x<|t|\forall x \lt \lvert t\rvert or ∃x<|t|\exists x \lt \lvert t\rvert (for tt an Lba\mathcal{L}^b_a-term not involving xx). The syntactic classes Σbi\Sigma^b_i and Πbi\Pi^b_i are defined in a manner reminiscent to how the classes Σ0i\Sigma^0_i and Π0i\Pi^0_i are defined in the traditional arithmetical hierarchy – i.e. by counting alternations of bounded quantifiers, ignoring sharply bounded ones.
The theories Si2\mathsf{S}^i_2 and Ti2\mathsf{T}^i_2 both extend a base theory known as BASIC\textsf{BASIC}. BASIC\textsf{BASIC} contains 32 axioms, which include those of Q\mathsf{Q}, as well as others which axiomatize the intended interpretations of |…|\lvert \dots\rvert, ⌊x2⌋\lfloor \frac{x}{2} \rfloor and #\# (see, e.g., Hájek and Pudlák 1998, 321–22). We also define the following induction schemas:
Σbi\Sigma^b_i-IND  \text{IND}\ \  ϕ(0)∧∀x(ϕ(x)→ϕ(x+1))→∀xϕ(x)\phi(0) \wedge \forall x(\phi(x) \rightarrow \phi(x+1)) \rightarrow \forall x \phi(x)
for all ϕ(x)∈Σbi\phi(x) \in \Sigma^b_i.
Σbi\Sigma^b_i-PIND   \text{PIND}\ \ \  ϕ(0)∧∀x(ϕ(⌊x2⌋)→ϕ(x))→∀xϕ(x)\phi(0) \wedge \forall x (\phi(\lfloor \frac{x}{2} \rfloor) \rightarrow \phi(x)) \rightarrow \forall x \phi(x) 
for all ϕ(x)∈Σbi\phi(x) \in \Sigma^b_i.
Finally, we define the theories Si2=BASIC+Σbi\mathsf{S}^i_2 = \mathsf{BASIC} + \Sigma^b_i-PIND\text{PIND} and Ti2=BASIC+Σbi\mathsf{T}^i_2 = \mathsf{BASIC} + \Sigma^b_i-IND\text{IND} and also S2=⋃i∈NSi2\mathsf{S}_2 = \bigcup_{i \in \mathbb{N}} \mathsf{S}^i_2 and T2=⋃i∈NTi2\mathsf{T}_2 = \bigcup_{i \in \mathbb{N}} \mathsf{T}^i_2.
Some basic results linking the language Lba\mathcal{L}^b_a and these theories to the Polynomial Hierarchy are as follows:
Theorem 4.6 (Kent and Hodgson 1982) A set XX belongs to ΣPi\Sigma^P_i if and only XX is definable by a Σbi\Sigma^b_i-formula ϕ(x)\phi(x) in the standard model – i.e. X={n∈N∣N⊨ϕ(¯n)}X = \{n \in \mathbb{N} \mid \mathcal{N} \models \phi(\overline{n}) \}.
Theorem 4.7 (Buss 1986) A function f(→x)f(\vec{x}) belongs to ◻Pi\Box^P_i if and only if f(→x)f(\vec{x}) is provably total in Si2\mathsf{S}^i_2 relative to a Σbi\Sigma^b_i-definition – i.e. if and only if there is a Σbi\Sigma^b_i-formula ϕf(→x,y)\phi_f(\vec{x},y) such that Si2⊢∀→x∃!yϕf(→x,y)\mathsf{S}^i_2 \vdash \forall \vec{x} \exists ! y \phi_f(\vec{x},y).
It follows from Theorem 4.7 that f(→x)f(\vec{x}) is computable in polynomial time just in case it is provably total in S12\mathsf{S}^1_2 relative to a Σb1\Sigma^b_1-definition. Buss (1986) also showed that Si2⊆Ti2⊆Si+12\mathsf{S}^i_2 \subseteq \mathsf{T}^i_2 \subseteq \mathsf{S}^{i+1}_2, that Si2\mathsf{S}^i_2 and Ti2\mathsf{T}^i_2 are finitely axiomatizable, and that S2=T2\mathsf{S}_2 = \mathsf{T}_2. It is not known if either of the hierarchies of theories displayed above collapses or whether either S2\mathsf{S}_2 or T2\mathsf{T}_2 is finitely axiomatizable. However, either the existence of an ii such that Ti2=Si+12\mathsf{T}^i_2 = \mathsf{S}^{i+1}_2 or the finite axiomatizability of S2\mathsf{S}_2 or T2\mathsf{T}_2 can be shown to entail the collapse of the Polynomial Hierarchy.
The second approach for relating formal arithmetic to PH\textbf{PH} employs a series of second-order theories V0⊆V1⊆…\mathsf{V}^0 \subseteq \mathsf{V}^1 \subseteq \ldots originally introduced by Zambella (1996) (see also Cook and Kolokolova (2001) and Cook and Nguyen 2010). These extend the first-order language La\mathcal{L}_a with symbols ∈\in and |⋅|\lvert \cdot\rvert as well as the quantifiers ∀X\forall X and ∃X\exists X intended to range over finite sets of natural numbers. The theory V1\mathsf{V}^1 extends Q\mathsf{Q} with second-order quantifier axioms, extensionality for sets, axioms formalizing that |X|\lvert X\rvert is intended to denote the largest element of XX, and a second-order comprehension schema for ΣB1\Sigma^B_1-formulas – i.e. formulas containing only bounded first-order quantifiers and set quantifiers of the form ∃X(|X|≤t)\exists  X(\lvert X\rvert \leq t) (where tt is a first-order term not containing XX). In parallel to Theorem 4.7, it can be shown that a function f(→x)f(\vec{x}) is in FP\textbf{FP} just in case it is definable by a ΣB1\Sigma^B_1-formula relative to which it is provably total in V1\mathsf{V}^1. Similar characterizations of the classes ◻Pi\Box^P_i may be obtained by considering the theories Vi\mathsf{V}^i (for i>1i &gt; 1) obtained by extending comprehension to wider classes of bounded second-order formulas.
4.6 Strict finitism
The most direct links between philosophy of mathematics and computational complexity theory have thus far arisen in the context of discussions of the view traditionally known as strict finitism. This view is most prominently associated with Yessenin-Volpin (1961; 1970), who is in turn best known for questioning whether expressions such as 101210^{12} or 2502^{50} denote natural numbers. Presuming that such expressions denote at all, Yessenin-Volpin contended that they cannot be taken to describe what he referred to as feasible numbers – i.e. numbers up to which we may count in practice. On this basis, he outlined a foundational program wherein feasibility is treated as a basic notion and traditional arguments in favor of the validity of mathematical induction and the uniqueness of the natural number series are called into question. Antecedents for such a view may be found in (e.g.) Bernays (1935), van Dantzig (1955), and Wang (1958).[48]
Recall that one tenet of the traditional form of finitism associated with the Hilbert Program is that the natural numbers should not be conceived as comprising a completed infinite totality. Strict finitists are sometimes described as going one step further than this and actively denying that there are infinitely many natural numbers. However, the following are more characteristic of claims which such theorists explicitly endorse:

(S1)

It is possible to identify a class of natural numbers corresponding to those which can be constructed in practice by counting or whose values are otherwise explicitly representable as numerals, either concretely or in intuition.
(S2)

We possess notations – e.g. decimal numerals such as 101010,67257729,101210^{10^{10}}, 67^{257^{729}}, 10^{12} – which do not denote numbers in this class.

In emphasizing the practical aspects of how we use numerals to represent natural numbers in the course of performing arithmetical computations, it is evident that the concerns which motivate strict finitism anticipate some of those which also inspired the development of complexity theory. Nonetheless, strict finitism has attracted few followers. To see why this is so, observe that (S1) makes clear that strict finitists propose to identify natural numbers with numerals such as the familiar sequence 0,0′,0″,…0,0',0'',\ldots of unary numerals.
If we regard such expressions as tokens rather than types, then it makes sense to consider the task of concretely ‘counting up to’ a number by 
constructing its unary representation in the sense described by Yessenin-Volpin.  For recall that it is a characteristic feature of numerals that they may be generated from certain initial symbols by the application of a finite set of syntactic formation rules – e.g. the unary numerals are generated by applying the formation rule σ↦σ′\sigma \mapsto \sigma' to the initial symbol 00. But once this is acknowledged, it also seems difficult to resist the following additional thesis:

(S3)

If a numeral σ\sigma can be feasibly constructed, then so can σ′\sigma' (i.e. the unary numeral denoting the successor of the number denoted by σ\sigma).[49]

In attempting to accommodate (S1)–(S3) simultaneously, we must confront a tension which has led many authors to follow Dummett (1975) in concluding that strict finitism lacks a coherent formulation. For suppose we let F(x)F(x) symbolize that xx denotes a feasible number in Yessenin-Volpin’s sense. Then it would seem to follow from (S1) that we ought to accept

F(0)F(0)

But it would also seem to follow from (S3) that we should also accept

∀x(F(x)→F(x+1))\forall x(F(x) \rightarrow F(x+1))

and from (S2) that we should accept

¬F(τ)\neg F(\tau)

where τ\tau is an expression denoting an ‘infeasible number’ such as  101210^{12}, 10101010^{10^{10}} or 6725772967^{257^{729}}.
Dummett observed that two different forms of the classical sorites paradox now intervene to show that (i)–(iii) are inconsistent. For on the one hand, by applying mathematical induction – i.e. the principle that for all definite properties P(x)P(x) of natural numbers, we may infer ∀xP(x)\forall x P(x) from P(0)P(0) and ∀x(P(x)→P(x+1))\forall x(P(x) \rightarrow P(x+1)) – to the predicate F(x)F(x), we may conclude that ∀xF(x)\forall x F(x) from (i) and (ii). But then F(τ)F(\tau) follows by universal instantiation, in contradiction to (iii). And on the other hand, even without appealing to induction, a contradiction can also be derived in the form of a conditional sorites argument by deriving F(τ)F(\tau) as the result of repeatedly applying modus ponens to the series F(0),F(0)→F(0′),F(0′)→F(0″),…F(0), F(0) \rightarrow F(0'), F(0') \rightarrow F(0''), \ldots obtained from (ii) by universal instantiation.
Although it would be uncharitable to think that Yessenin-Volpin was unaware of these observations, the first person to directly reply to the charge that (S1)-(S3) are inconsistent appears to have been Parikh (1971).[50] His response can be understood as consisting of two parts, both of which bear on the analysis of the informal notion of feasibility considered in Sections 
 1.1 and 
 2.2. 
 On the one hand, Parikh considers what he refers to as the almost consistent theory PAF\mathsf{PA}^F. This theory is formulated over the language La∪{F(x)}\mathcal{L}_a \cup \{F(x)\} supplemented with terms for all closed primitive recursive functions and contains the statement ¬F(τ)\neg F(\tau) where τ\tau is some fixed primitive recursive term intended to denote an ‘infeasible’ number such as 101210^{12}. Finally, PAF\mathsf{PA}^F contains the axioms of PA\mathsf{PA} with the induction schema restricted to formulas not containing F(x)F(x).
In light of the latter restriction, it is not possible to mimic the inductive form of the sorites argument in PAF\mathsf{PA}^F. But of course PAF\mathsf{PA}^F is still inconsistent in virtue of the conditional form of the argument. Nonetheless, Parikh showed that for appropriate choices of τ\tau, any proof of a contradiction in PAF\mathsf{PA}^F must itself be very long.  For instance, if we consider the super-exponential function 2⇑0=12 \Uparrow 0 = 1 and 2⇑(x+1)=22⇑x2 \Uparrow (x+1) = 2^{2 \Uparrow x} and let τ\tau be the primitive recursive term 2⇑2k2 \Uparrow 2^k, it is a consequence of Parikh’s result that any proof of a contradiction in PAF\mathsf{PA}^F must be on the order of 2k2^k steps long. For example if τ=2⇑21000\tau = 2 \Uparrow 2^{1000}, then such a proof must be at least 210002^{1000} lines 
 long.[51] 
This suggests the possibility of a two-part reply on behalf of the strict finitist to Dummett’s argument against strict 
 finitism:[52]


The inductive form of the sorites is ruled out in virtue of the fact that F(x)F(x) is not a definite (i.e. non-vague) property of natural numbers.

At least for certain choices of τ\tau, the conditional form is also not a threat because the only derivation of a contradiction from (i)–(iii) is too long to be carried out in practice.

The possibility of such a reply notwithstanding, it is also natural to ask at this point whether the notion of feasibility considered in complexity theory might also be vague in a manner which could render it susceptible to the sort of soritical argument envisioned by Dummett (1975). Note, however, that our discussion thus far suggests that the notion of feasibility in which strict finitists have been interested is a property of individual natural numbers. This is out of keeping with the way we have seen that this notion is treated in complexity theory. For instance, in order to employ the Cobham-Edmond’s Thesis to judge whether a problem XX is feasibly decidable, we consider the order of growth of the time complexity t(n)t(n) of the most efficient algorithm for deciding XX. From the perspective of complexity theory, feasibility is thus a notion which applies not to individual natural numbers nn, but either to time complexity functions of type N→N\mathbb{N} \rightarrow \mathbb{N} or their rates of growth.
Next observe that the following are consequences of the Cobham-Edmonds thesis:


O(1)O(1) (i.e. constant time) is a feasible rate of growth.

Since all polynomial orders of growth are feasible, it follows that if O(nk)O(n^k) is feasible, then so is O(nk+1)O(n^{k+1}).

Super-polynomial orders of growth such as O(2n)O(2^n) are not feasible.

On this basis one might indeed fear that the intuitions about feasibly computable functions codified by the Cobham-Edmonds thesis exhibit the same kind of instability as do intuitively acceptable principles about feasibly constructible numbers.
To see that this is not the case, however, observe that we compare the growth rates of f,g:N→Nf,g: \mathbb{N} \rightarrow \mathbb{N} not by the standard ordering <\lt on the natural numbers but rather the following ordering ≺\prec:
O(f(n))≺O(g(n)) if and only if there exists n0,c∈N such that for all n>n0,f(n)<c⋅g(n)
 O(f(n)) \prec O(g(n)) \text{ if and only if there exists } 
 n_0,c \in \mathbb{N} \text{ such that for all } 
 n \gt n_0, f(n) \lt c \cdot g(n)

It is a consequence of this definition that O(1)≺O(n)≺O(n2)≺O(n3)…O(1) \prec O(n) \prec O(n^2) \prec O(n^3) \ldots – i.e. the polynomial orders of growth do form an ω\omega-sequence with respect to ≺\prec. However, it also follows that O(nk)≺O(2n)O(n^k) \prec O(2^n) for all k∈Nk \in \mathbb{N} – i.e. O(2n)O(2^n) is a ‘point at infinity’ sitting above each polynomial order of growth with respect to this ordering (as are all other super-polynomial rates of growth).[53] And from this it follows that such orders cannot be reached from below by a sorties-like sequence of feasible orders of growth O(1)≺O(n)≺O(n2)≺O(n3)…O(1) \prec O(n) \prec O(n^2) \prec O(n^3) \ldots When analyzed according to the Cobham-Edmonds thesis, it hence appears that the ‘naive’ notion of feasible computability does not suffer from the sort of instability which Dummett takes to plague the notion of a feasibly constructible number.
These observations point to another theme within the writings of some strict finitists which suggests that they also anticipate the way in which predicates like ‘feasible’ and ‘infeasible’ are now employed in complexity theory. For instance, while van Dantzig (1955) suggests that the feasible numbers are closed under addition and multiplication, Yessenin-Volpin (1970) explicitly states that they should not be regarded as closed under exponentiation. The possibility that exponentiation should be understood to play a role in the formulation of strict finitism itself is also suggested by the fact that the particular examples of ‘infeasible numbers’ which have been put forth by Yessenin-Volpin and others have typically employed exponential or iterated exponential notations of the forms such as nn21n_1^{{n}_2} or nnn321n_1^{n_2^{n_3}}. This in turn suggests another reply to the sorties-based argument against strict finitism.
Recall that the theory IΔ0\text{I}\Delta_0 introduced in 
 Section 4.5 
 allows us to define the graph of the exponential function xyx^y via a Δ0\Delta_0-formula ε(x,y,z)\varepsilon(x,y,z) in the sense that IΔ0⊢∀xε(x,0,1)\text{I}\Delta_0 \vdash \forall x\varepsilon(x,0,1) and IΔ0⊢∀x∀y∀z(ε(x,y,z)→ε(x,y+1,z⋅x))\text{I}\Delta_0 \vdash \forall x \forall y \forall z(\varepsilon(x,y,z) \rightarrow \varepsilon(x,y+1,z \cdot x)). But as we have seen, IΔ0\text{I}\Delta_0 does not prove the totality of the exponential function nor (as can be also be shown) does Buss’s theory S12\mathsf{S}^1_2. From this it follows that the theory S12+∃y¬∃zε(2,y,z)\mathsf{S}^1_2 + \exists y \neg \exists z \varepsilon(2,y,z) is proof-theoretically consistent. By the completeness theorem for first-order logic, there thus exists a model M\mathcal{M} for the language of bounded of arithmetic such that M⊨S12+∃y¬∃zε(2,y,z)\mathcal{M} \models \mathsf{S}^1_2 + \exists y \neg \exists z \varepsilon(2,y,z). But although it follows from Theorem 4.5 that all the polynomial time computable functions are defined for all inputs in M\mathcal{M}, there also exists an object aa in the domain of M\mathcal{M} satisfying ¬∃zε(2,a,z)\neg \exists z \varepsilon(2,a,z). And from this it follows that the expression we would conventionally write as ‘2a2^a’ fails to denote a value in M\mathcal{M}.[54]
Of course any model M⊨S12+∃y¬∃zε(2,y,z)\mathcal{M} \models \mathsf{S}^1_2 + \exists y \neg \exists z \varepsilon(2,y,z) must be nonstandard. Thus not only must the domain of M\mathcal{M} be infinite, but also there will exist ‘natural numbers’ in the sense of M\mathcal{M} which will have infinitely many predecessors when viewed from the outside of M\mathcal{M}. One might thus at first think that the use of structures like M\mathcal{M} to explore the consequences of strict finitism would be antithetical to its proponents.
Note, however, that such theorists are typically careful to avoid explicitly asserting that there are only finitely many feasible numbers. Instead they give examples of expressions whose denotations (were they to exist) would be infeasibly large – e.g. those given in the formulation of (S2). Certainly the claim that there is a largest feasibly constructible number would invite the challenge that the strict finitist nominate such a number nn. And any such nomination would in turn invite the rejoinder that if nn is feasibly constructible, then n+1n+1 must be as well. But in the sort of model M\mathcal{M} under consideration the definite description ‘the largest xx such that 2x2^x exists’ will be non-denoting in virtue of the fact that the elements n∈Mn \in \mathcal{M} such that M⊨∃zε(2,n,z)\mathcal{M} \models \exists z \varepsilon(2,n,z) form a proper initial segment. The existence of such models might thus be taken to elucidate Yessenin-Volpin’s (1970) claim that there are distinct natural number sequences which are distinguished by the primitive recursive functions under which they are closed.[55]
4.7 Logical knowledge and the complexity of deductive reasoning
The most direct links between complexity theory and epistemology which have thus far been discussed are mediated by the observation that deciding logical validity (and related properties) is generally a computationally difficult task. For on the one hand, a traditional view in epistemology is that logical knowledge – e.g. knowledge that certain statements are logical validities – is a paradigm case of a priori knowledge. If this is correct, then it would seem that we should credit normal epistemic agents with knowledge of the class of logically valid sentences of systems such as propositional and first-order logic which are commonly taken to underlie everyday reasoning. But on the other hand, the results presented in 
 Section 4.2  make clear that the problem of deciding whether ϕ\phi is a validity of one of these systems is computationally intractable.
The apparent clash between philosophical views about knowledge which predict that logical validities ought to be easy to come to know (e.g. in virtue of the fact that they do not require empirical confirmation) and the technical fact that the validity and satisfiability problems for even the weakest familiar systems are intractable is the origin of two developments which traditionally have been of interest to epistemologists: the problem of logical omniscience and the study of theories of minimal or bounded rationality.
The problem of logical omniscience is often presented as having originated from within the subject known as epistemic logic. In this setting, knowledge is treated as a modal operator KiK_i where sentences of the form KiϕK_i \phi are assigned the intended interpretation agent ii knows that ϕ\phi. The goal is to then set down axioms and rules which characterize how we should reason about statements of this form. For instance the principle that knowledge of a statement entails its truth is rendered as Kiϕ→ϕK_i \phi \rightarrow \phi and the principle that knowledge of a conjunction entails knowledge of its conjuncts is rendered as Ki(ϕ∧ψ)→(Kiϕ∧Kiψ)K_i(\phi \wedge \psi) \rightarrow (K_i \phi \wedge K_i \psi). Other principles – e.g. ¬Kiϕ→Ki¬Kiϕ\neg K_i \phi \rightarrow K_i \neg K_i \phi, which expresses that ii’s failure to know ϕ\phi entails that he knows of this failure – are considered more controversial. However the consensus view (e.g., Hintikka 1962; Lenzen 1978; Fagin et al. 1995) is that the most defensible choices of logics of knowledge lie between the modal systems S4\textsf{S4} and S5\textsf{S5}.
The precise axiomatization which is adopted for KiK_i notwithstanding, the use of modal logic to model reasoning about knowledge has the following two consequences:


KiϕK_i \phi for all ϕ∈VALID\phi \in \sc{VALID}

if Γ⊨ϕ\Gamma \models \phi and KiψK_i \psi for all ψ∈Γ\psi \in \Gamma, then KiϕK_i \phi

(i) and (ii) respectively report that an agent’s knowledge includes all propositional tautologies and that the set of sentences he knows is closed under logical consequence.[56] Note, however, that both of these results seem prima facie implausible relative to our everyday understanding of knowledge. For on the one hand, not only are there infinitely many distinct propositional tautologies, but there are even relatively short ones which many otherwise normal epistemic agents will fail to recognize as such. And on the other, it is generally acknowledged that the set of sentences which we know or believe (at least in an explicit sense) is not closed under logical consequence. This is often illustrated by observing that an otherwise competent epistemic agent might know the axioms of a mathematical theory (e.g. PA\mathsf{PA}) without knowing all of its theorems (e.g. the Infinitude of Primes).
The results summarized in Sections 
 4.2 and 
 4.3 underscore just how severe these problems are. Note first that the problem of deciding whether a given formula is a tautology is coNP\textbf{coNP}-complete, and hence very likely to be intractable. But we have also seen that results from proof complexity strongly suggest that any proof system which an agent is likely to adopt for reasoning about propositional logic (e.g. natural deduction) will be such that there are infinitely many statements whose simplest proofs are of length at least exponential relative to their size. It is thus likely that there will exist ‘short’ tautologies – e.g. with fewer than 100 propositional letters – whose shortest proof in a conventional natural deduction system will be astronomically long. Similar remarks apply to the problem of determining whether a finite set of sentences {ψ1,…,ψn}\{\psi_1, \ldots, \psi_n\} is consistent – a task which is surely of epistemic import in everyday reasoning. In particular, it is readily seen that consistency checking is simply a re-description of the canonical NP\textbf{NP}-complete problem SAT\sc{SAT}.
It is already a consequence of the Cobham-Edmonds Thesis (together with the expected positive answers to Open Questions 1 and 2) that such problems are computationally intractable. But if we abandon the simplifying assumption that everyday reasoning is based on classical propositional logic, then validity and consistency checking only become harder. For instance, if we think that agents can reason about their own knowledge or that of others using a modal logic such as S4\mathsf{S4} or S52\mathsf{S5}_2, then the corresponding validity and satisfiability problems become PSPACE\textbf{PSPACE}-complete. And if we assume that they reason in propositional relevance logic, classical first-order logic, or intuitionistic first-order logic, then the validity problem becomes RE\textbf{RE}-complete (i.e. as hard as the Halting Problem).
One common reaction to these observations is to acknowledge that as plausible as the axioms of traditional epistemic logics may appear, they formalize not our everyday understanding of knowledge but rather an idealized or ‘implicit’ notion more closely resembling ‘knowability in principle’ (e.g. Levesque 1984; Stalnaker 1991; 1999). Another reaction has been to attempt to modify the interpretation of the language of epistemic logic to mitigate the effects of (i) and (ii). This can be achieved by altering the account of semantic validity for the modal language in question – e.g. through the use of so-called impossible worlds (Rantala 1982), awareness models (Fagin and Halpern 1988), or local models (Fagin and Halpern 1988). But although these techniques can be used to circumvent particular cases of (i) and (ii), they also typically complicate the task of explaining why the set of sentences known by an agent may exhibit some potentially desirable closure properties (e.g. that knowledge of a conjunction entails knowledge of its conjuncts).
An approach to logical omniscience which explicitly takes computational complexity into account is proposed by Artemov and Kuznets (2014). They suggest that a logic of knowledge should formalize not just statements of the form ‘ϕ\phi is knowable by agent ii’ but also ones of the form ‘ϕ\phi is known by agent ii on the basis of evidence tt’. They then exhibit a family of Justification Logics for reasoning about statements of the latter form which satisfy the so-called Strong Logical Omniscience Test – i.e. if a statement of the form ‘ϕ\phi is known by agent ii on the basis of evidence tt’ is derivable, then a proof of ϕ\phi can be found in time polynomial in the size of tt and ϕ\phi.
The foregoing modifications to epistemic logic fall broadly within the scope of the subject which has come to be known as bounded rationality – c.f., e.g., (Simon 1957; Simon 1972; Rubinstein 1998; Gigerenzer and Selten 2002; Kahneman 2003). This development can be traced to work in decision theory and cognitive psychology which attempts to take into account that human agents face resource limitations in everyday decision making. The attempt to develop models of decision which take resource limitations into account is sometimes presented as a counterpoint to the normative models of rational choice which are often employed in economics and political science. In this context, decision making is often modeled as a form of optimization. In particular, it is traditionally required that fully rational agents be able to find an optimal choice among an array of alternatives,  the potential infeasibility of carrying out a search amongst them notwithstanding.
These developments serve as part of the background to a debate in epistemology which originated with Cherniak’s (1981; 1984; 1986) theory of minimal rationality. Cherniak attempts to provide a characterization of rationality which is responsive to both traditional normative characterizations as well as complexity theoretic results about the difficulty of deductive inference of the sort discussed above. According to his account, a minimally rational agent need only make some of the valid inferences which follow from his beliefs in a proof system for classical logic. He further suggests that the inferences which are selected may depend on the sorts of heuristics considered by cognitive psychologists such as Tversky and Kahneman (1974). As such, Cherniak proposes that the use of heuristics which may be of benefit to an agent in certain circumstances – even potentially unsound ones – should be regarded as falling under a generalized account of rationality informed by computational complexity theory. Related themes are discussed by Harman (1986) and Morton (2004; 2012).
5. Further reading
Historical surveys of the early development of computational complexity theory by the Western and Soviet schools are respectively provided in (Fortnow and Homer 2003) and (Trakhtenbrot 1984). (Wagner and Wechsung 1986) and (Emde Boas 1990) provide detailed treatments of machine models, simulation results, the status of the Invariance Thesis, and the distinction between the first and second machine classes. Basic topics in complexity theory are presented in semi-popular form in (Allender and McCartin 2000), (Harel 2006), (Cook 2012), and (Fortnow 2013). Introductory presentations can also be found in many general undergraduate textbooks on theoretical computer science such as (Hopcroft and Ulman 1979), (Lewis and Papadimitriou 1998), and (Sipser 2006). More advanced textbook treatments of complexity theory include (Papadimitriou 1994), (Du and Ko 2000), (Hemaspaandra and Ogihara 2002), (Arora and Barak 2009), and (Goldreich 2010). Many of these books cover topics which are not surveyed here but which may be of interest to philosophers – e.g. randomized algorithms, probabilistically checkable proofs, natural proofs, zero-knowledge proofs, and interactive proof systems. (Van Leeuwen 1990) contains survey articles on several complexity-related topics. The standard reference on NP\textbf{NP}-completeness remains (Garey and Johnson 1979) which contains a list of over 300 NP\textbf{NP}-complete problems. A similar reference on P\textbf{P}-completeness is (Greenlaw, Hoover, and Ruzzo 1995). Structural complexity theory – i.e. the study of different reduction notions and the corresponding degree structures – is developed in (Balcázar, Diaz, and Gabarró 1988) and (Odifreddi 1999). Textbook treatments of proof complexity, descriptive complexity, and bounded arithmetic are respectively provided by (Cook and Nguyen 2010), (Immerman 1999), and (Hájek and Pudlák 1998). (Moore and Mertens 2011) and (Aaronson 2013b) also provide recent philosophically-oriented surveys of complexity theory.
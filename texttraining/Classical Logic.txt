Typically, a logic consists of a formal or informal language
together with a deductive system and/or a model-theoretic semantics.
The language has components that correspond to a part of a natural language like
English or Greek. The deductive system is to capture, codify, or
simply record arguments that are valid for the given
language, and the semantics is to capture, codify, or record the
meanings, or truth-conditions for at least part of the language.

 The following sections provide the basics of a typical logic,
sometimes called “classical elementary logic” or “classical
first-order logic”. Section 2 develops a formal language, with a
rigorous syntax and grammar. The formal language is a recursively
defined collection of strings on a fixed alphabet. As such, it has
no meaning, or perhaps better, the meaning of its formulas is given
by the deductive system and the semantics. Some of the symbols have
counterparts in ordinary language. We define an argument to
be a non-empty collection of sentences in the formal language, one of
which is designated to be the conclusion. The other sentences (if
any) in an argument are its premises. Section 3 sets up a deductive
system for the language, in the spirit of natural deduction. An
argument is derivable if there is a deduction from some or all of
its premises to its conclusion. Section 4 provides a model-theoretic
semantics. An argument is valid if there is no
interpretation (in the semantics) in which its premises are all true
and its conclusion false. This reflects the longstanding view that a
valid argument is truth-preserving.

 In Section 5, we turn to relationships between the deductive system
and the semantics, and in particular, the relationship between
derivability and validity. We show that an argument is derivable only
if it is valid. This pleasant feature, called soundness,
entails that no deduction takes one from true premises to a false
conclusion. Thus, deductions preserve truth. Then we establish a converse, called
completeness, that an argument is valid only if it is
derivable. This establishes that the deductive system is rich enough
to provide a deduction for every valid argument. So there are enough
deductions: all and only valid arguments are derivable. We briefly
indicate other features of the logic, some of which are corollaries to
soundness and completeness.

The final section, Section 6, is devoted to the a brief examination of
the philosophical position that classical logic is “the one right
logic”.
 
1. Introduction


Today, logic is a branch of mathematics and a branch of philosophy.
In most large universities, both departments offer courses in logic,
and there is usually a lot of overlap between them. Formal languages,
deductive systems, and model-theoretic semantics are mathematical
objects and, as such, the logician is interested in their mathematical
properties and relations. Soundness, completeness, and most of the
other results reported below are typical examples. Philosophically,
logic is at least closely related to the study of correct
reasoning. Reasoning is an epistemic, mental activity. So logic
is at least closely allied with epistemology. Logic is also a central
branch of computer science, due, in part, to interesting computational
relations in logical systems, and, in part, to the close connection
between formal deductive argumentation and reasoning (see the entries
on
 recursive functions,
 computability and complexity, and
 philosophy of computer science).

This raises questions concerning the philosophical relevance of the
various mathematical aspects of logic. How do deducibility and
validity, as properties of formal languages--sets of strings on a
fixed alphabet--relate to correct reasoning? What do the mathematical
results reported below have to do with the original philosophical
issues concerning valid reasoning? This is an instance of the
philosophical problem of explaining how mathematics applies to
non-mathematical reality.
 
 Typically, ordinary deductive reasoning takes place in a natural language, or
perhaps a natural language augmented with some mathematical symbols. So
our question begins with the relationship between a natural language and a
formal language. Without attempting to be comprehensive, it may help to
sketch several options on this matter.

 One view is that the formal languages accurately exhibit actual
features of certain fragments of a natural language. Some
philosophers claim that declarative sentences of natural language
have underlying logical forms and that these forms are
displayed by formulas of a formal language. Other writers hold that
(successful) declarative sentences express propositions; and
formulas of formal languages somehow display the forms of these
propositions. On views like this, the components of a logic provide
the underlying deep structure of correct reasoning. A chunk of
reasoning in natural language is correct if the forms underlying the
sentences constitute a valid or deducible argument. See for example,
Montague [1974], Davidson [1984], Lycan [1984] (and the
entry on 
 logical form).

 Another view, held at least in part by Gottlob Frege and Wilhelm
Leibniz, is that because natural languages are fraught with vagueness
and ambiguity, they should be replaced by formal languages. A
similar view, held by W. V. O. Quine (e.g., [1960], [1986]), is that a
natural language should be regimented, cleaned up for serious
scientific and metaphysical work. One desideratum of the enterprise is
that the logical structures in the regimented language should be
transparent. It should be easy to “read off” the logical
properties of each sentence. A regimented language is similar to a
formal language regarding, for example, the explicitly presented rigor
of its syntax and its truth conditions.

 On a view like this, deducibility and validity represent
idealizations of correct reasoning in natural language. A
chunk of reasoning is correct to the extent that it corresponds to,
or can be regimented by, a valid or deducible argument in a formal
language.

When mathematicians and many philosophers engage in deductive
reasoning, they occasionally invoke formulas in a formal language to
help disambiguate, or otherwise clarify what they mean. In other
words, sometimes formulas in a formal language are used in
ordinary reasoning. This suggests that one might think of a formal
language as an
addendum to a natural language. Then our present question
concerns the relationship between this addendum and the original
language. What do deducibility and validity, as sharply defined on
the addendum, tell us about correct deductive reasoning in general?

 Another view is that a formal language is a mathematical
model of a natural language in roughly the same sense as, say, a
collection of point masses is a model of a system of physical objects,
and the Bohr construction is a model of an atom. In other words, a
formal language displays certain features of natural languages, or
idealizations thereof, while ignoring or simplifying other
features. The purpose of mathematical models is to shed light on what
they are models of, without claiming that the model is accurate in all
respects or that the model should replace what it is a model of. On a
view like this, deducibility and validity represent mathematical
models of (perhaps different aspects of) correct reasoning in natural
languages. Correct chunks of deductive reasoning correspond, more or
less, to valid or deducible arguments; incorrect chunks of reasoning
roughly correspond to invalid or non-deducible arguments. See, for
example, Corcoran [1973], Shapiro [1998], and Cook [2002].

 There is no need to adjudicate this matter here. Perhaps the truth
lies in a combination of the above options, or maybe some other
option is the correct, or most illuminating one. We raise the matter
only to lend some philosophical perspective to the formal treatment
that follows.
2. Language


Here we develop the basics of a formal language, or to be precise, a
class of formal languages. Again, a formal language is a recursively
defined set of strings on a fixed alphabet. Some aspects of the
formal languages correspond to, or have counterparts in, natural
languages like English. Technically, this “counterpart relation” is
not part of the formal development, but we will mention it from time
to time, to motivate some of the features and results.
2.1 Building blocks


We begin with analogues of singular terms, linguistic items
whose function is to denote a person or object. We call these
terms. We assume a stock of individual constants.
These are lower-case letters, near the beginning of the Roman
alphabet, with or without numerical subscripts:
a,a1,b23,c,d22,etc.a,a1,b23,c,d22,etc.
a, a_1, b_{23}, c, d_{22}, \text{etc}.


 We envisage a potential infinity of individual constants. In the
present system each constant is a single character, and so individual
constants do not have an internal syntax. Thus we have an infinite
alphabet. This could be avoided by taking a constant like
d22d22d_{22}, for example, to consist of three characters,
a lowercase “ddd” followed by a pair of subscript
“2”s.
 
 We also assume a stock of individual variables. These are
lower-case letters, near the end of the alphabet, with or without
numerical subscripts:
w,x,y12,z,z4,etc.w,x,y12,z,z4,etc.
w, x, y_{12}, z,  z_4, \text{etc}. 


In ordinary mathematical reasoning, there are two functions terms need
to fulfill. We need to be able to denote specific, but unspecified (or
arbitrary) objects, and sometimes we need to express generality. In
our system, we use some constants in the role of unspecified reference
and variables to express generality. Both uses are recapitulated in
the formal treatment below. Some logicians employ different symbols
for unspecified objects (sometimes called “individual
parameters”) and variables used to express generality.


Constants and variables are the only terms in our formal language, so
all of our terms are simple, corresponding to proper names and some
uses of pronouns. We call a term closed if it contains no
variables. In general, we use vvv to represent variables, and ttt
to represent a closed term. Some authors also introduce function
letters, which allow complex terms corresponding to:
“7+47+47+4” and “the wife of Bill Clinton”, or
complex terms containing variables, like “the father of
xxx” and “x/yx/yx/y”. Logic books aimed at
mathematicians are likely to contain function letters, probably due to
the centrality of functions in mathematical discourse. Books aimed at
a more general audience (or at philosophy students), may leave out
function letters, since it simplifies the syntax and theory. We follow
the latter route here. This is an instance of a general tradeoff
between presenting a system with greater expressive resources, at the
cost of making its formal treatment more complex.


 For each natural number nnn, we introduce a stock of
nnn-place predicate letters. These are upper-case
letters at the beginning or middle of the alphabet. A superscript
indicates the number of places, and there may or may not be a
subscript. For example, 
A3,B32,P3,etc.A3,B32,P3,etc.
A^3, B^{3}_2,  P^3, \text{etc}.


are three-place predicate letters. We often omit the superscript, when
no confusion will result. We also add a special two-place predicate
symbol “===” for identity.

Zero-place predicate letters are sometimes called “sentence
letters”. They correspond to free-standing sentences whose
internal structure does not matter. One-place predicate letters,
called “monadic predicate letters”, correspond to
linguistic items denoting properties, like “being a man”,
“being red”, or “being a prime
number”. Two-place predicate letters, called “binary
predicate letters”, correspond to linguistic items denoting
binary relations, like “is a parent of” or “is
greater than”. Three-place predicate letters correspond to
three-place relations, like “lies on a straight line
between”. And so on. 

 The non-logical terminology of the language consists of its
individual constants and predicate letters. The symbol “===”, for
identity, is not a non-logical symbol. In taking identity to be
logical, we provide explicit treatment for it in the deductive system
and in the model-theoretic semantics. Most authors do the same, but
there is some controversy over the issue (Quine [1986, Chapter
5]). If KKK is a set of constants and predicate letters, then
we give the fundamentals of a language L1K=L1K=\LKe 
 built on this set of non-logical terminology. It may be called the
first-order language with identity on KKK. A similar
language that lacks the symbol for identity (or which takes identity
to be non-logical) may be called
 L1KL1K\mathcal{L}1K,
the first-order language without identity on KKK.
2.2 Atomic formulas


If VVV is an nnn-place predicate letter in KKK,
and t1,…,tnt1,…,tnt_1, \ldots,t_n
are terms of KKK,
then Vt1…tnVt1…tnVt_1 \ldots t_n
is an atomic formula of
 L1K=L1K=\LKe. 
 Notice that the terms t1,…,tnt1,…,tnt_1, \ldots,t_n need not be distinct. Examples of
atomic formulas include:
P4xaab,C1x,C1a,D0,A3abc.P4xaab,C1x,C1a,D0,A3abc.
P^4 xaab, C^1 x, C^1 a, D^0, A^3 abc.


 The last one is an analogue of a statement that a certain relation
(A)(A)(A) holds between three objects (a,b,c)(a,b,c)(a, b,
c). If t1t1t_1 and t2t2t_2 are
terms, then t1=t2t1=t2t_1 =t_2 is also an
atomic formula of
 L1K=L1K=\LKe. It corresponds
to an assertion that t1t1t_1 is identical to
t2t2t_2.

If an atomic formula has no variables, then it is called an
atomic sentence. If it does have variables, it is
called open. In the above list of examples, the first and
second are open; the rest are sentences. 
2.3 Compound formulas


We now introduce the final items of the lexicon:
¬,&,∨,→,∀,∃,(,)¬,&,∨,→,∀,∃,(,)
\neg, \amp, \vee, \rightarrow, \forall, \exists, (, )


 We give a recursive definition of a formula of
 L1K=L1K=\LKe:

All atomic formulas of L1K=L1K=\LKe are formulas of L1K=L1K=\LKe.
If θθ\theta is a formula of L1K=L1K=\LKe, then so is ¬θ¬θ\neg \theta.


 A formula corresponding to ¬θ¬θ\neg \theta thus says that it is not the
 case that θθ\theta. The symbol “¬¬\neg” is called
 “negation”, and is a unary connective.

If θθ\theta and ψψ\psi are formulas of L1K=L1K=\LKe,
 then so is (θ&ψ)(θ&ψ)(\theta \amp \psi).


 The ampersand “&&\amp” corresponds to the English
“and” (when “and” is used to connect
sentences). So (θ&ψ)(θ&ψ)(\theta \amp \psi) can be read “θθ\theta and
ψψ\psi”. The formula (θ&ψ)(θ&ψ)(\theta \amp \psi) is called the
“conjunction” of θθ\theta and ψψ\psi.

If θθ\theta and ψψ\psi are formulas of L1K=L1K=\LKe, then so is (θ∨ψ)(θ∨ψ)(\theta \vee \psi).


The wedge “∨∨\vee” corresponds to “either …
or … or both”, so (θ∨ψ)(θ∨ψ)(\theta \vee \psi) can be read
“θθ\theta or ψψ\psi”.  The formula (θ∨ψ)(θ∨ψ)(\theta \vee
\psi) is called the “disjunction” of θθ\theta and
ψψ\psi.

 5. If θθ\theta and ψψ\psi are formulas of L1K=L1K=\LKe,
 then so is (θ→ψ)(θ→ψ)(\theta \rightarrow \psi).


The arrow “→→\rightarrow” roughly corresponds to
“if … then … ”, so (θ→ψ)(θ→ψ)(\theta \rightarrow
\psi) can be read “if θθ\theta then ψψ\psi” or
“θθ\theta only if ψψ\psi”.

 The symbols “&&\amp”, “∨∨\vee”, and
 “→→\rightarrow” are called “binary connectives”,
 since they serve to “connect” two formulas into
 one. Some authors introduce (θ↔ψ)(θ↔ψ)(\theta \leftrightarrow \psi) as an abbreviation
 of ((θ→ψ)&(ψ→θ))((θ→ψ)&(ψ→θ))((\theta \rightarrow \psi) \amp(\psi \rightarrow \theta)). The symbol
 “↔↔\leftrightarrow” is an analogue of the locution “if and
 only if”.

If θθ\theta is a formula of L1K=L1K=\LKe and vvv is a variable,
then ∀vθ∀vθ\forall v \theta is a formula of L1K=L1K=\LKe.


The symbol “∀∀\forall” is called a universal
quantifier, and is an analogue of “for all”; so
∀vθ∀vθ\forall v\theta can be read “for all v,θv,θv,
\theta”.

If θθ\theta is a formula of L1K=L1K=\LKe and vvv is a variable,
then ∃vθ∃vθ\exists v \theta is a formula of L1K=L1K=\LKe.


 The symbol “∃∃\exists” is called an
existential quantifier, and is an analogue of “there
exists” or “there is”; so ∃vθ∃vθ\exists v \theta
can be read “there is a vvv such that θθ\theta”.

That’s all folks. That is, all formulas are constructed in
accordance with rules (1)–(7).


 Clause (8) allows us to do inductions on the complexity of
formulas. If a certain property holds of the atomic formulas and is
closed under the operations presented in clauses (2)–(7), then the
property holds of all formulas. Here is a simple example:


Theorem 1. Every formula of
 L1K=L1K=\LKe
 has the same number of left and right parentheses. Moreover, each
left parenthesis corresponds to a unique right parenthesis, which
occurs to the right of the left parenthesis. Similarly, each right
parenthesis corresponds to a unique left parenthesis, which occurs to
the left of the given right parenthesis. If a parenthesis occurs
between a matched pair of parentheses, then its mate also occurs
within that matched pair. In other words, parentheses that occur
within a matched pair are themselves matched.

Proof: By clause (8), every formula is built up
from the atomic formulas using clauses (2)–(7). The atomic formulas
have no parentheses. Parentheses are introduced only in clauses (3)–(5), and
each time they are introduced as a matched set. So at any stage in
the construction of a formula, the parentheses are paired off.


We next define the notion of an occurrence of a variable being
free or bound in a formula. A variable that
immediately follows a quantifier (as in “∀x∀x\forall x”
and “∃y∃y\exists y”) is neither free nor bound. We do not
even think of those as occurrences of the variable. All variables that
occur in an atomic formula are free. If a variable occurs free (or
bound) in θθ\theta or in ψψ\psi, then that same occurrence is free
(or bound) in ¬θ,(θ&ψ),(θ∨ψ)¬θ,(θ&ψ),(θ∨ψ)\neg \theta, (\theta \amp \psi), (\theta \vee \psi),
and (θ→ψ)(θ→ψ)(\theta \rightarrow \psi). That is, the (unary and binary)
connectives do not change the status of variables that occur in
them. All occurrences of the variable vvv in θθ\theta are bound in
∀vθ∀vθ\forall v \theta and ∃vθ∃vθ\exists v \theta. Any free
occurrences of vvv in θθ\theta are bound by the initial
quantifier. All other variables that occur in θθ\theta are free or
bound in ∀vθ∀vθ\forall v \theta and ∃vθ∃vθ\exists v \theta, as they are in
θθ\theta.

 For example, in the formula 
 (∀(∀(\forallx(Axy
∨Bx)&Bx)∨Bx)&Bx)\vee Bx) \amp Bx), the occurrences of “xxx” in 
 Axy and in the first BxBxBx are bound by the
quantifier. The occurrence of “yyy” and last
occurrence of “xxx” are free. In
∀x(Ax→∃∀x(Ax→∃\forall x(Ax \rightarrow \existsxBx), the
“xxx” in AxAxAx is bound by the initial
universal quantifier, while the other occurrence of xxx is
bound by the existential quantifier. The above syntax allows this
“double-binding”. Although it does not create any
ambiguities (see below), we will avoid such formulas, as a matter of
taste and clarity.

The syntax also allows so-called vacuous binding, as in
∀∀\forallxBcBcBc. These, too, will be avoided in what follows.
Some treatments of logic rule out vacuous binding and double binding
as a matter of syntax. That simplifies some of the treatments below,
and complicates others.

 Free variables correspond to place-holders, while bound variables
are used to express generality. If a formula has no free variables,
then it is called a sentence. If a formula has free
variables, it is called open. 
2.4 Features of the syntax


Before turning to the deductive system and semantics, we mention a few
features of the language, as developed so far. This helps draw the
contrast between formal languages and natural languages like English.
 
 We assume at the outset that all of the categories are disjoint. For
example, no connective is also a quantifier or a variable, and the
non-logical terms are not also parentheses or connectives. Also, the
items within each category are distinct. For example, the sign for
disjunction does not do double-duty as the negation symbol, and
perhaps more significantly, no two-place predicate is also a
one-place predicate.

 One difference between natural languages like English and formal
languages like
 L1K=L1K=\LKe is that the latter are not
supposed to have any ambiguities. The policy that the different
categories of symbols do not overlap, and that no symbol does
double-duty, avoids the kind of ambiguity, sometimes called
“equivocation”, that occurs when a single word has two meanings:
“I’ll meet you at the bank.” But there are other kinds of
ambiguity. Consider the English sentence:

 John is married, and Mary is single, or Joe is crazy.


 It can mean that John is married and either Mary is single or Joe is
crazy, or else it can mean that either both John is married and Mary
is single, or else Joe is crazy. An ambiguity like this, due to
different ways to parse the same sentence, is sometimes called an
“amphiboly”. If our formal language did not have the
parentheses in it, it would have amphibolies. For example, there would
be a “formula” A&B∨A&B∨A \amp B \vee
C. Is this supposed to be ((A&B)∨C)((A&B)∨C)((A \amp B)
\vee C), or is it (A&(B∨C))(A&(B∨C))(A \amp(B \vee C))? The parentheses resolve what would be an
amphiboly.

 Can we be sure that there are no other amphibolies in our language?
That is, can we be sure that each formula of
 L1K=L1K=\LKe can be put
together in only one way? Our next task is to answer this question.
 
 Let us temporarily use the term “unary marker” for the negation
symbol (¬)(¬)(\neg) or a quantifier followed by a variable (e.g.,
 ∀x,∃z)∀x,∃z)\forall x,
 \exists z).


Lemma 2. Each formula consists of a string of zero
or more unary markers followed by either an atomic formula or a
formula produced using a binary connective, via one of clauses
(3)–(5).

Proof: We proceed by induction on the complexity of
the formula or, in other words, on the number of formation rules that
are applied. The Lemma clearly holds for atomic formulas. Let nnn be
a natural number, and suppose that the Lemma holds for any formula
constructed from nnn or fewer instances of clauses
(2)–(7). Let θθ\theta be a formula constructed from n+1n+1n+1
instances. The Lemma holds if the last clause used to construct
θθ\theta was either (3), (4), or (5). If the last clause used to
construct θθ\theta was (2), then θθ\theta is ¬ψ¬ψ\neg \psi.  Since
ψψ\psi was constructed with nnn instances of the rule, the Lemma
holds for ψψ\psi (by the induction hypothesis), and so it holds for
θθ\theta.  Similar reasoning shows the Lemma to hold for θθ\theta
if the last clause was (6) or (7). By clause (8), this exhausts the
cases, and so the Lemma holds for θθ\theta, by induction.

Lemma 3. If a formula
 θθ\theta contains a left parenthesis,
then it ends with a right parenthesis, which matches the leftmost left
parenthesis in
 θθ\theta.

Proof: Here we also proceed by induction on the
number of instances of (2)–(7) used to construct the
formula. Clearly, the Lemma holds for atomic formulas, since they
have no parentheses. Suppose, then, that the Lemma holds for formulas
constructed with nnn or fewer instances of (2)–(7), and let
 θθ\theta be constructed with
n+1n+1n+1
 instances. If the last clause applied was (3)–(5), then the Lemma
holds since
 θθ\theta itself begins with a left
parenthesis
 and ends with the matching right parenthesis. If the last clause
applied was
 (2), then
 θθ\theta is
 ¬ψ¬ψ\neg \psi,
 and the induction hypothesis applies to
 ψψ\psi.
 Similarly, if the last clause applied was (6) or (7), then
 θθ\theta
 consists of a quantifier, a variable, and a formula to which we can
apply the
 induction hypothesis. It follows that the Lemma holds for
 θθ\theta.

Lemma 4. Each formula contains at least one atomic
formula.


 The proof proceeds by induction on the number of instances of (2)–(7)
used to construct the formula, and we leave it as an exercise.



Theorem 5. Let α,βα,β\alpha, \beta be nonempty
sequences of characters on our alphabet, such that αβαβ\alpha \beta
(i.e αα\alpha followed by β)β)\beta) is a formula. Then αα\alpha
is not a formula.

Proof: By Theorem 1 and Lemma 3, if αα\alpha
contains a left parenthesis, then the right parenthesis that matches
the leftmost left parenthesis in αβαβ\alpha \beta comes at the end of
αβαβ\alpha \beta, and so the matching right parenthesis is in
ββ\beta.  So, αα\alpha has more left parentheses than right
parentheses. By Theorem 1,α1,α1, \alpha is not a formula. So now suppose
that αα\alpha does not contain any left parentheses. By Lemma 2,αβ2,αβ2,
\alpha \beta consists of a string of zero or more unary markers
followed by either an atomic formula or a formula produced using a
binary connective, via one of clauses (3)–(5). If the latter
formula was produced via one of clauses (3)–(5), then it begins
with a left parenthesis. Since αα\alpha does not contain any
parentheses, it must be a string of unary markers. But then αα\alpha
does not contain any atomic formulas, and so by Lemma 4,α4,α4, \alpha is
not a formula. The only case left is where αβαβ\alpha \beta consists
of a string of unary markers followed by an atomic formula, either in
the form t1=t2t1=t2t_1 =t_2 or Pt1…tnPt1…tnPt_1 \ldots t_n.  Again, if αα\alpha
just consisted of unary markers, it would not be a formula, and so
αα\alpha must consist of the unary markers that start αβαβ\alpha
\beta, followed by either t1t1t_1 by itself, t1=t1=t_1 = by itself, or
the predicate letter PPP, and perhaps some (but not all) of the
terms t1,…,tnt1,…,tnt_1, \ldots,t_n.  In the first two cases, αα\alpha does
not contain an atomic formula, by the policy that the categories do
not overlap. Since PPP is an nnn-place predicate letter, by the
policy that the predicate letters are distinct, PPP is not an
mmm-place predicate letter for any m≠nm≠nm \ne n. So the part of
αα\alpha that consists of PPP followed by the terms is not an
atomic formula. In all of these cases, then, αα\alpha does not
contain an atomic formula. By Lemma 4,α4,α4, \alpha is not a
formula.

We are finally in position to show that there is no amphiboly in our
language.


Theorem 6. Let θθ\theta be any formula of
L1K=L1K=\LKe.  If θθ\theta is not atomic, then there is one and only one
among (2)–(7) that was the last clause applied to construct
θθ\theta.  That is, θθ\theta could not be produced by two
different clauses. Moreover, no formula produced by clauses
(2)–(7) is atomic.

Proof: By Clause (8), either θθ\theta is atomic or
it was produced by one of clauses (2)–(7). Thus, the first
symbol in θθ\theta must be either a predicate letter, a term, a
unary marker, or a left parenthesis. If the first symbol in θθ\theta
is a predicate letter or term, then θθ\theta is atomic. In this
case, θθ\theta was not produced by any of (2)–(7), since all
such formulas begin with something other than a predicate letter or
term. If the first symbol in θθ\theta is a negation sign
“¬¬\neg”, then was θθ\theta produced by clause (2),
and not by any other clause (since the other clauses produce formulas
that begin with either a quantifier or a left parenthesis). Similarly,
if θθ\theta begins with a universal quantifier, then it was produced
by clause (6), and not by any other clause, and if θθ\theta begins
with an existential quantifier, then it was produced by clause (7),
and not by any other clause. The only case left is where θθ\theta
begins with a left parenthesis. In this case, it must have been
produced by one of (3)–(5), and not by any other clause. We only
need to rule out the possibility that θθ\theta was produced by more
than one of (3)–(5). To take an example, suppose that θθ\theta
was produced by (3) and (4). Then θθ\theta is (ψ1&ψ2)(ψ1&ψ2)(\psi_1 \amp
\psi_2) and θθ\theta is also (ψ3∨ψ4)(ψ3∨ψ4)(\psi_3 \vee \psi_4), where
ψ1,ψ2,ψ3ψ1,ψ2,ψ3\psi_1, \psi_2, \psi_3, and ψ4ψ4\psi_4 are themselves
formulas. That is, (ψ1&ψ2)(ψ1&ψ2)(\psi_1 \amp \psi_2) is the very same formula
as (ψ3∨ψ4)(ψ3∨ψ4)(\psi_3 \vee \psi_4). By Theorem 5,ψ15,ψ15, \psi_1 cannot be a
proper part of ψ3ψ3\psi_3, nor can ψ3ψ3\psi_3 be a proper part of
ψ1ψ1\psi_1. So ψ1ψ1\psi_1 must be the same formula as ψ3ψ3\psi_3. But
then “&&\amp” must be the same symbol as
“∨∨\vee”, and this contradicts the policy that all of
the symbols are different. So θθ\theta was not produced by both
Clause (3) and Clause (4). Similar reasoning takes care of the other
combinations.


 This result is sometimes called “unique readability”. It
shows that each formula is produced from the atomic formulas via the
various clauses in exactly one way. If θθ\theta was produced by
clause (2), then its main connective is the initial
“¬¬\neg”. If θθ\theta was produced by clauses (3),
(4), or (5), then its main connective is the introduced
“&&\amp”, “∨∨\vee”, or
“→→\rightarrow”, respectively. If θθ\theta was
produced by clauses (6) or (7), then its main connective is
the initial quantifier. We apologize for the tedious details. We
included them to indicate the level of precision and rigor for the
syntax.
3. Deduction


We now introduce a deductive system, DDD, for our
languages. As above, we define an argument to be a non-empty
collection of sentences in the formal language, one of which is
designated to be the conclusion. If there are any other
sentences in the argument, they are its
 premises.[1]
 By convention, we use “ΓΓ\Gamma”,
“Γ′Γ′\Gamma'”, “Γ1Γ1\Gamma_1”, etc, to range
over sets of formulas, and we use the letters “ϕϕ\phi”,
“ψψ\psi”, “θθ\theta”, uppercase or
lowercase, with or without subscripts, to range over single
formulas. We write “Γ,Γ′Γ,Γ′\Gamma, \Gamma'” for the union of
ΓΓ\Gamma and Γ′Γ′\Gamma', and “Γ,ϕΓ,ϕ\Gamma, \phi” for the
union of ΓΓ\Gamma with {ϕ}{ϕ}\{\phi\}.

We write an argument in the form ⟨Γ,ϕ⟩⟨Γ,ϕ⟩\langle \Gamma, \phi \rangle,
where ΓΓ\Gamma is a set of sentences, the premises, and ϕϕ\phi is
a single sentence, the conclusion. Remember that ΓΓ\Gamma may be
empty. We write Γ⊢ϕΓ⊢ϕ\Gamma \vdash \phi to indicate that ϕϕ\phi is
deducible from ΓΓ\Gamma, or, in other words, that the argument
⟨Γ,ϕ⟩⟨Γ,ϕ⟩\langle \Gamma, \phi \rangle is deducible in DDD. We may write
Γ⊢DϕΓ⊢Dϕ\Gamma \vdash_D \phi to emphasize the deductive system DDD. We
write ⊢ϕ⊢ϕ\vdash \phi or ⊢Dϕ⊢Dϕ\vdash_D \phi to indicate that ϕϕ\phi
can be deduced (in D)D)D) from the empty set of premises.

 The rules in DDD are chosen to match logical relations
concerning the English analogues of the logical terminology in the
language. Again, we define the deducibility relation by recursion. We
start with a rule of assumptions:



(As)

If ϕϕ\phi is a member of ΓΓ\Gamma, then Γ⊢ϕΓ⊢ϕ\Gamma \vdash \phi.



 
We thus have that {ϕ}⊢ϕ{ϕ}⊢ϕ\{\phi \}\vdash \phi; each premise follows
from itself. We next present two clauses for each connective and
quantifier. The clauses indicate how to “introduce” and
“eliminate” sentences in which each symbol is the main
connective.
 
First, recall that “&&\amp” is an analogue of the
English connective “and”. Intuitively, one can deduce a
sentence in the form (θ&ψ)(θ&ψ)(\theta \amp \psi) if one has deduced
θθ\theta and one has deduced ψψ\psi. Conversely, one can deduce
θθ\theta from (θ&ψ)(θ&ψ)(\theta \amp \psi) and one can deduce ψψ\psi
from (θ&ψ)(θ&ψ)(\theta \amp \psi):


(&I)(&I)(\amp \mathrm{I})

If Γ1⊢θΓ1⊢θ\Gamma_1 \vdash \theta and Γ2⊢ψΓ2⊢ψ\Gamma_2 \vdash \psi, then
Γ1,Γ2⊢(θ&ψ)Γ1,Γ2⊢(θ&ψ)\Gamma_1, \Gamma_2 \vdash(\theta \amp \psi).



(&E)(&E)(\amp \mathrm{E}) 

If Γ⊢(θ&ψ)Γ⊢(θ&ψ)\Gamma \vdash(\theta \amp \psi) then Γ⊢θΓ⊢θ\Gamma \vdash \theta;
and if Γ⊢(θ&ψ)Γ⊢(θ&ψ)\Gamma \vdash(\theta \amp \psi) then Γ⊢ψΓ⊢ψ\Gamma \vdash
\psi.




The name “&I” stands for
“&-introduction”; “&E” stands for
“&-elimination”.

Since, the symbol “∨∨\vee” corresponds to the English
“or”, (θ∨ψ)(θ∨ψ)(\theta \vee \psi) should be deducible from
θθ\theta, and (θ∨ψ)(θ∨ψ)(\theta \vee \psi) should also be deducible from
ψψ\psi:



(∨I)(∨I)(\vee \mathrm{I}) 

If Γ⊢θΓ⊢θ\Gamma \vdash \theta then Γ⊢(θ∨ψ)Γ⊢(θ∨ψ)\Gamma
\vdash(\theta \vee \psi); if Γ⊢ψΓ⊢ψ\Gamma \vdash \psi then Γ⊢(θ∨ψ)Γ⊢(θ∨ψ)\Gamma
\vdash(\theta \vee \psi).




The elimination rule is a bit more complicated. Suppose that
“θθ\theta or ψψ\psi” is true. Suppose also that
ϕϕ\phi follows from θθ\theta and that ϕϕ\phi follows from
ψψ\psi. One can reason that if θθ\theta is true, then ϕϕ\phi is
true. If instead ψψ\psi is true, we still have that ϕϕ\phi is
true.  So either way, ϕϕ\phi must be true.


(∨E)(∨E)(\vee \mathrm{E}) 

If Γ1⊢(θ∨ψ),Γ2,θ⊢ϕΓ1⊢(θ∨ψ),Γ2,θ⊢ϕ\Gamma_1 \vdash(\theta \vee \psi),
\Gamma_2, \theta \vdash \phi and Γ3,ψ⊢ϕΓ3,ψ⊢ϕ\Gamma_3, \psi \vdash \phi,
then Γ1,Γ2,Γ3⊢ϕΓ1,Γ2,Γ3⊢ϕ\Gamma_1, \Gamma_2, \Gamma_3 \vdash \phi.




 For the next clauses, recall that the symbol, “→→\rightarrow”, is
 an analogue of the English “if … then … ”
 construction. If one knows, or assumes (θ→ψ)(θ→ψ)(\theta \rightarrow \psi) and
 also knows, or assumes θθ\theta, then one can conclude
 ψψ\psi. Conversely, if one deduces ψψ\psi from an assumption θθ\theta,
 then one can conclude that (θ→ψ)(θ→ψ)(\theta \rightarrow \psi).


(→I)(→I)({\rightarrow}\mathrm{I}) 

If Γ,θ⊢ψΓ,θ⊢ψ\Gamma, \theta \vdash \psi, then Γ⊢(θ→ψ)Γ⊢(θ→ψ)\Gamma \vdash(\theta
\rightarrow \psi).



(→E)(→E)({\rightarrow}\mathrm{E}) 

If Γ1⊢(θ→ψ)Γ1⊢(θ→ψ)\Gamma_1 \vdash(\theta \rightarrow \psi) and Γ2⊢θΓ2⊢θ\Gamma_2 \vdash
\theta, then Γ1,Γ2⊢ψΓ1,Γ2⊢ψ\Gamma_1, \Gamma_2 \vdash \psi.




This elimination rule is sometimes called “modus ponens”.
In some logic texts, the introduction rule is proved as a
“deduction theorem”.

Our next clauses are for the negation sign, “¬¬\neg”. The
underlying idea is that a sentence ψψ\psi is inconsistent with its
negation ¬ψ¬ψ\neg \psi. They cannot both be true. We call a pair of
sentences ψ,¬ψψ,¬ψ\psi, \neg \psi contradictory opposites. If one
can deduce such a pair from an assumption θθ\theta, then one can
conclude that θθ\theta is false, or, in other words, one can conclude
¬θ¬θ\neg \theta.


(¬I)(¬I)(\neg \mathrm{I})

If Γ1,θ⊢ψΓ1,θ⊢ψ\Gamma_1, \theta \vdash \psi and Γ2,θ⊢¬ψΓ2,θ⊢¬ψ\Gamma_2, \theta \vdash \neg
\psi, then Γ1,Γ2⊢¬θΓ1,Γ2⊢¬θ\Gamma_1, \Gamma_2 \vdash \neg \theta.




By (As), we have that {A,¬A}⊢A{A,¬A}⊢A\{A,\neg A\}\vdash A and
{{\{A,¬¬\negA}⊢¬A}⊢¬A\}\vdash \neg A. So by ¬¬\negI we have
that {A}⊢¬¬A{A}⊢¬¬A\{A\}\vdash \neg \neg A. However, we do not have the converse
yet. Intuitively, ¬¬θ¬¬θ\neg \neg \theta corresponds to “it is not
the case that it is not the case that” . One might think that
this last is equivalent to θθ\theta, and we have a rule to that
effect:



(DNE)

If Γ⊢¬¬θΓ⊢¬¬θ\Gamma \vdash \neg \neg \theta, then Γ⊢θΓ⊢θ\Gamma \vdash \theta.




The name DNE stands for “double-negation
elimination”. There is some controversy over this inference. It
is rejected by philosophers and mathematicians who do not hold that
each meaningful sentence is either true or not
true. Intuitionistic logic does not sanction the inference in
question (see, for example Dummett [2000], or the entry on
 intuitionistic logic, or
 history of intuitionistic logic),
but, again, classical logic does.

 To illustrate the parts of the deductive system DDD presented
thus far, we show that ⊢(A∨¬A)⊢(A∨¬A)\vdash(A \vee \neg A):

{¬(A∨¬A),A}⊢¬(A∨¬A){¬(A∨¬A),A}⊢¬(A∨¬A)\{\neg(A \vee \neg A),  A\}\vdash \neg(A \vee \neg A), 
 by (As)
{¬(A∨¬A),A}⊢A{¬(A∨¬A),A}⊢A\{\neg(A \vee \neg A), A\}\vdash A,
 by (As).
{¬(A∨¬A),A}⊢(A∨¬A){¬(A∨¬A),A}⊢(A∨¬A)\{\neg(A \vee \neg A), A\}\vdash(A \vee \neg A), 
 by (∨(∨(\veeI), from (ii).
{¬(A∨¬A)}⊢¬A{¬(A∨¬A)}⊢¬A\{\neg(A \vee \neg A)\}\vdash \neg A, 
 by  (¬(¬(\negI), from (i) and (iii).
{¬(A∨¬A),¬A}⊢¬(A∨¬A){¬(A∨¬A),¬A}⊢¬(A∨¬A)\{\neg(A \vee \neg A), \neg A\}\vdash \neg(A \vee \neg A), 
 by (As)
{¬(A∨¬A),¬A}⊢¬A{¬(A∨¬A),¬A}⊢¬A\{\neg(A \vee \neg A), \neg A\}\vdash \neg A,
 by (As)
{¬(A∨¬A),¬A}⊢(A∨¬A){¬(A∨¬A),¬A}⊢(A∨¬A)\{\neg(A \vee \neg A), \neg A\}\vdash(A \vee \neg A), 
 by (∨(∨(\veeI), from (vi).
{¬(A∨¬A)}⊢¬¬A{¬(A∨¬A)}⊢¬¬A\{\neg(A \vee \neg A)\}\vdash \neg \neg A, 
 by (¬(¬(\negI), from (v) and (vii).
⊢¬¬(A∨¬A)⊢¬¬(A∨¬A)\vdash \neg \neg(A \vee \neg A), 
 by (¬(¬(\negI), from (iv) and (viii).
⊢(A∨¬A)⊢(A∨¬A)\vdash(A \vee \neg A), 
 by (DNE), from (ix).


The principle (θ∨¬θ)(θ∨¬θ)(\theta \vee \neg \theta) is sometimes called the 
law of excluded middle. It is not valid in intuitionistic
logic.
 Let θ,¬θθ,¬θ\theta, \neg \theta be a pair of contradictory opposites,
and let ψψ\psi be any sentence at all. By (As) we have {θ,¬θ,¬ψ}⊢θ{θ,¬θ,¬ψ}⊢θ\{\theta,
\neg \theta, \neg \psi \}\vdash \theta and {θ,¬θ,¬ψ}⊢¬θ{θ,¬θ,¬ψ}⊢¬θ\{\theta, \neg \theta,
\neg \psi \}\vdash \neg \theta. So by (¬(¬(\negI), {θ,¬θ}⊢¬¬ψ{θ,¬θ}⊢¬¬ψ\{\theta, \neg
\theta \}\vdash \neg \neg \psi. So, by (DNE) we have {θ,¬θ}⊢ψ{θ,¬θ}⊢ψ\{\theta ,
\neg \theta \}\vdash \psi . That is, anything at all follows from a
pair of contradictory opposites. Some logicians introduce a rule to
codify a similar inference:
 
If Γ1⊢θΓ1⊢θ\Gamma_1 \vdash \theta and Γ2⊢¬θΓ2⊢¬θ\Gamma_2 \vdash \neg \theta,
then for any sentence ψ,Γ1,Γ2⊢ψ\psi, \Gamma_1, \Gamma_2 \vdash \psi


The inference is sometimes called ex falso quodlibet or, more
colorfully, explosion. Some call it
“¬\neg-elimination”, but perhaps this stretches the notion
of “elimination” a bit. We do not officially
include ex falso quodlibet as a separate rule in DD,
but as will be shown below (Theorem 10), each instance of it is
derivable in our system DD.

 Some logicians object to ex falso quodlibet, on the ground
that the sentence ψ\psi may be irrelevant to any of the
premises in Γ\Gamma. Suppose, for example, that one starts with some
premises Γ\Gamma about human nature and facts about certain people,
and then deduces both the sentence “Clinton had extra-marital
sexual relations” and “Clinton did not have extra-marital
sexual relations”. One can perhaps conclude that there is
something wrong with the premises Γ\Gamma. But should we be allowed to
then deduce anything at all from Γ\Gamma? Should we be
allowed to deduce “The economy is sound”?
A small minority of logicians, called dialetheists, hold
that some contradictions are actually true. For them, ex falso
quodlibet is not truth-preserving.

Deductive systems that demur from ex falso quodlibet are
called paraconsistent. Most relevant logics are
paraconsistent. See the entries on 
 relevance logic, 
 paraconsistent logic,
 and
 dialetheism. 
Or see Anderson and Belnap [1975], Anderson, Belnap, and Dunn [1992],
and Tennant [1997] for fuller overviews of relevant logic; and Priest
[2006],[2006a] for dialetheism. Deep philosophical issues concerning
the nature of 
 logical consequence
 are involved. Far be it for
an article in a philosophy encyclopedia to avoid philosophical issues,
but space considerations preclude a fuller treatment of this issue
here. Suffice it to note that the inference ex falso
quodlibet is sanctioned in systems of classical logic,
the subject of this article. It is essential to establishing the
balance between the deductive system and the semantics (see §5
below).

 The next pieces of DD are the clauses for the quantifiers. Let
 θ\theta be a formula, vv a
variable, and tt a term (i.e., a variable or a constant). Then
define
 θ(v|t)\theta(v|t) to be
the result of substituting tt for each free occurrence of
vv in
 θ\theta. So, if
 θ\theta is (Qx&∃(Qx \amp \existsxPxy), then
 θ(x|c)\theta(x|c) is
(Qc&∃(Qc \amp \existsxPxy). The last
occurrence of xx is not free.

A sentence in the form ∀vθ\forall v \theta is an analogue of the
English “for every v,θv, \theta holds”. So one should be
able to infer θ(v|t)\theta(v|t) from ∀vθ\forall v \theta for any closed
term tt. Recall that the only closed terms in our system are
constants.


(∀E)(\forall \mathrm{E})

If Γ⊢∀vθ\Gamma \vdash \forall v \theta, then Γ⊢θ(v|t)\Gamma \vdash
\theta(v|t), for any closed term tt.




The idea here is that if ∀vθ\forall v \theta is true, then θ\theta
should hold of tt, no matter what tt is.

The introduction clause for the universal quantifier is a bit more
complicated. Suppose that a sentence θ\theta contains a closed term
tt, and that θ\theta has been deduced from a set of premises
Γ\Gamma. If the closed term tt does not occur in any member of
Γ\Gamma, then θ\theta will hold no matter which object tt may
denote. That is, ∀vθ\forall v \theta follows. 


(∀I)(\forall \mathrm{I}) 

For any closed term tt, if Γ⊢θ(v|t)\Gamma\vdash\theta (v|t), then
Γ⊢∀vθ\Gamma\vdash\forall v\theta provided that tt is not in
Γ\Gamma or θ\theta.




This rule (∀I)(\forall \mathbf{I}) corresponds to a common inference in
mathematics. Suppose that a mathematician says “let nn
be a natural number” and goes on to show that nn has a
certain property PP, without assuming anything
about nn (except that it is a natural number). She then
reminds the reader that nn is “arbitrary”, and
concludes that PP holds for all natural numbers. The
condition that the term
tt not occur in any premise is what guarantees that it is
indeed “arbitrary”. It could be any object, and so anything we
conclude about it holds for all objects. 

 The existential quantifier is an analogue of the English expression
“there exists”, or perhaps just “there is”. If
we have established (or assumed) that a given object tt has a
given property, then it follows that there is something that has that
property.  


(∃I)(\exists \mathrm{I})

For any closed term tt, if Γ⊢θ(v|t)\Gamma\vdash\theta (v|t) then
Γ⊢∃vθ\Gamma\vdash\exists v\theta.




The elimination rule for ∃\exists is not quite as simple:


(∃E)(\exists \mathrm{E})

For any closed term tt, if Γ1⊢∃vθ\Gamma_1\vdash\exists v\theta and
Γ2,θ(v|t)⊢ϕ\Gamma_2, \theta(v|t)\vdash\phi, then Γ1,Γ2⊢ϕ\Gamma_1
,\Gamma_2\vdash\phi, provided that tt does not occur in ϕ\phi,
Γ2\Gamma_2 or θ\theta.




 This elimination rule also corresponds to a common inference. Suppose
that a mathematician assumes or somehow concludes that there is a
natural number with a given property PP. She then says “let
nn be such a natural number, so that PnPn”, and goes on to
establish a sentence ϕ\phi, which does not mention the number
nn. If the derivation of ϕ\phi does not invoke anything about
nn (other than the assumption that it has the given property
P)P), then nn could have been any number that has the property
PP. That is, nn is an arbitrary number with property
PP (this is where we invoke constants which “denote”
arbitrary objects). It does not matter which number nn is. Since
ϕ\phi does not mention nn, it follows from the assertion that
something has property PP. The provisions added to (∃(\existsE)
are to guarantee that tt is “arbitrary”.

 The final items are the rules for the identity sign “=”. The
introduction rule is about a simple as can be:


(=I)({=}\mathrm{I})

Γ⊢t=t\Gamma \vdash t=t, where tt is any closed term.




 This “inference” corresponds to the truism that everything is
identical to itself. The elimination rule corresponds to a principle
that if aa is identical to bb, then anything true of
aa is also true of bb. 


(=E)({=}\mathrm{E})

For any closed terms t1t_1 and t2t_2, if Γ1⊢t1=t2\Gamma_1 \vdash t_1
=t_2 and Γ2⊢θ\Gamma_2 \vdash \theta, then Γ1,Γ2⊢θ′\Gamma_1, \Gamma_2
\vdash \theta', where θ′\theta' is obtained from θ\theta by
replacing one or more occurances of t1t_1 with t2t_2.




 The rule (=E)({=}\mathrm{E}) indicates a certain restriction in the
expressive resources of our language. Suppose, for example, that Harry
is identical to Donald (since his mischievous parents gave him two
names). According to most people’s intuitions, it would not
follow from this and “Dick knows that Harry is wicked”
that “Dick knows that Donald is wicked”, for the reason
that Dick might not know that Harry is identical to Donald. Contexts
like this, in which identicals cannot safely be substituted for each
other, are called “opaque”. We assume that our language
L1K=\LKe has no opaque contexts.

 One final clause completes the description of the deductive system
DD:


(*)

That’s all folks.  Γ⊢θ\Gamma \vdash \theta only if θ\theta
follows from members of Γ\Gamma by the above rules.




 Again, this clause allows proofs by induction on the rules used to
establish an argument. If a property of arguments holds of all
instances of (As) and (=I)({=}\mathrm{I}), and if the other rules
preserve the property, then every argument that is deducible in DD
enjoys the property in question.

Before moving on to the model theory for L1K=\LKe, we pause to note a
few features of the deductive system. To illustrate the level of
rigor, we begin with a lemma that if a sentence does not contain a
particular closed term, we can make small changes to the set of
sentences we prove it from without problems. We allow ourselves the
liberty here of extending some previous notation: for any terms tt
and t′t', and any formula θ\theta, we say that θ(t|t′)\theta(t|t')
is the result of replacing all free occurrences of tt in θ\theta
with t′t'.


Lemma 7.  If Γ1\Gamma_1 and Γ2\Gamma_2 differ
only in that wherever Γ1\Gamma_1 contains θ\theta, Γ2\Gamma_2
contains θ(t|t′)\theta(t|t'), then for any sentence ϕ\phi not
containing tt or t′t', if Γ1⊢ϕ\Gamma_1\vdash\phi then
Γ2⊢ϕ\Gamma_2\vdash\phi.


Proof: The proof proceeds by induction on the number
of steps in the proof of ϕ\phi. Crucial to this proof is the fact
that θ=θ(t|t′)\theta=\theta(t|t') whenever θ\theta does not contain
tt or t′t'. When the number of steps in the proof of ϕ\phi is
one, this means that the last (and only) rule applied is (As) or
(=I). Then, since ϕ\phi does not contain tt or t′t', if
Γ1⊢ϕ\Gamma_1\vdash\phi we simply apply the same rule ((As) or (=I)) to
Γ2\Gamma_2 to get Γ2⊢ϕ\Gamma_2\vdash\phi. Assume that there are
n>1n&gt;1 steps in the proof of ϕ\phi, and that Lemma 7 holds for any
proof with less than nn steps. Suppose that the nthn^{th} rule
applied to Γ1\Gamma_1 was (&I\amp I). Then ϕ\phi is
ψ&χ\psi\amp\chi, and Γ1⊢ϕ&χ\Gamma_1\vdash\phi\amp\chi. But then we know
that previous steps in the proof include Γ1⊢ψ\Gamma_1\vdash\psi and
Γ1⊢χ\Gamma_1\vdash\chi, and by induction, we have
Γ2⊢ψ\Gamma_2\vdash\psi and Γ2⊢χ\Gamma_2\vdash\chi, since neither
ψ\psi nor χ\chi contain tt or t′t'. So, we simply apply
(&I\amp I) to Γ2\Gamma_2 to get Γ2⊢ψ&χ\Gamma_2\vdash\psi\amp\chi as
required. Suppose now that the last step applied in the proof of
Γ1⊢ϕ\Gamma_1\vdash\phi was (&E\amp E). Then, at a previous step in
the proof of ϕ\phi, we know Γ1⊢ϕ&ψ\Gamma_1\vdash\phi\amp\psi for some
sentence ψ\psi. If ψ\psi does not contain tt, then we simply
apply (&E\amp E) to Γ2\Gamma_2 to obtain the desired result. The
only complication is if ψ\psi contains tt. Then we would have
that Γ2⊢(ϕ&ψ)(t|t′)\Gamma_2\vdash (\phi\amp\psi)(t|t'). But, since
(ϕ&ψ)(t|t′)(\phi\amp\psi)(t|t') is ϕ(t|t′)&ψ(t|t′)\phi(t|t')\amp\psi(t|t'), and
ϕ(t|t′)\phi(t|t') is just ϕ\phi, we can just apply (&E\amp E) to get
Γ2⊢ϕ\Gamma_2\vdash\phi as required. The cases for the other rules are
similar.

Theorem 8. The rule of Weakening. If Γ1⊢ϕ\Gamma_1
\vdash \phi and Γ1⊆Γ2\Gamma_1 \subseteq \Gamma_2, then Γ2⊢ϕ\Gamma_2
\vdash \phi.

Proof: Again, we proceed by induction on the number
of rules that were used to arrive at Γ1⊢ϕ\Gamma_1 \vdash \phi.
Suppose that n>0n\gt 0 is a natural number, and that the theorem
holds for any argument that was derived using fewer than nn
rules. Suppose that Γ1⊢ϕ\Gamma_1 \vdash \phi using exactly nn
rules. If n=1n=1, then the rule is either (As) or (=(=I). In these
cases, Γ2⊢ϕ\Gamma_2 \vdash \phi by the same rule. If the last rule
applied was (&I), then ϕ\phi has the form (θ&ψ)(\theta \amp
\psi), and we have Γ3⊢θ\Gamma_3 \vdash \theta and Γ4⊢ψ\Gamma_4 \vdash
\psi, with Γ1=Γ3,Γ4\Gamma_1 = \Gamma_3, \Gamma_4.  We apply the
induction hypothesis to the deductions of θ\theta and ψ\psi, to
get Γ2⊢θ\Gamma_2 \vdash \theta and Γ2⊢ψ\Gamma_2 \vdash \psi.  and then
apply (&I) to the result to get Γ2⊢ϕ\Gamma_2 \vdash \phi.  Most of
the other cases are exactly like this. Slight complications arise only
in the rules (∀(\forallI) and (∃(\existsE), because there we have
to pay attention to the conditions for the rules.

Suppose that the last rule applied to get Γ1⊢ϕ\Gamma_1 \vdash \phi is
(∀(\forallI). So ϕ\phi is a sentence of the form ∀vθ\forall
v\theta, and we have Γ1⊢θ(v|t)\Gamma_1 \vdash \theta (v|t) and tt does
occur in any member of Γ1\Gamma_1 or in θ\theta.  The problem is
that tt may occur in a member of Γ2\Gamma_2, and so we cannot
just invoke the induction hypothesis and apply (∀(\forallI) to the
result. So, let t′t' be a term not occurring in any sentence in
Γ2\Gamma_2. Let Γ′\Gamma' be the result of substituting t′t' for
all tt in Γ2\Gamma_2. Then, since tt does not occur in
Γ1\Gamma_1, Γ1⊆Γ′\Gamma_1\subseteq\Gamma'. So, the induction
hypothesis gives us Γ′⊢θ(v|t)\Gamma'\vdash\theta (v|t), and we know that
Γ′\Gamma' does not contain tt, so we can apply (∀I\forall I) to
get Γ′⊢∀vθ\Gamma'\vdash\forall v\theta. But ∀vθ\forall v\theta does not
contain tt or t′t', so Γ2⊢∀vθ\Gamma_2\vdash\forall v\theta by Lemma
7.

Suppose that the last rule applied was (∃(\existsE), we have
Γ3⊢∃vθ\Gamma_3 \vdash \exists v\theta and Γ4,θ(v|t)⊢ϕ\Gamma_4, \theta (v|t)
\vdash \phi, with Γ1\Gamma_1 being Γ3,Γ4\Gamma_3, \Gamma_4, and
tt not in ϕ\phi, Γ4\Gamma_4 or θ\theta.  If tt does not
occur free in Γ2\Gamma_2, we apply the induction hypothesis to get
Γ2⊢∃vθ\Gamma_2 \vdash \exists v\theta, and then (∃(\existsE) to end up
with Γ2⊢ϕ\Gamma_2 \vdash \phi. If tt does occur free in
Γ2\Gamma_2, then we follow a similar proceedure to ∀I\forall I,
using Lemma 7. 


 Theorem 8 allows us to add on premises at will. It follows that
 Γ⊢ϕ\Gamma \vdash \phi if and only if there is a subset
 Γ′⊆Γ\Gamma'\subseteq \Gamma such that
 Γ′⊢ϕ\Gamma'\vdash \phi. Some systems of relevant
 logic do not have weakening, nor does substructural logic (See the
 entries on
 relevance logic,
 substructural logics,
and
 linear logic).

By clause (*), all derivations are established in a finite number of steps. 
So we have


Theorem 9.  Γ⊢ϕ\Gamma \vdash \phi if and only if
there is a finite Γ′⊆Γ\Gamma'\subseteq \Gamma such that
Γ′⊢ϕ\Gamma'\vdash \phi.

Theorem 10. The rule of ex falso quodlibet
is a “derived rule” of DD: if Γ1⊢θ\Gamma_1 \vdash
\theta and Γ2⊢¬θ\Gamma_2 \vdash \neg \theta, then Γ1,Γ2⊢ψ\Gamma_1,\Gamma_2
\vdash \psi, for any sentence ψ\psi.

Proof: Suppose that Γ1⊢θ\Gamma_1 \vdash \theta and
Γ2⊢¬θ\Gamma_2 \vdash \neg \theta. Then by Theorem 8,Γ1,¬ψ⊢θ8, \Gamma_1,\neg
\psi \vdash \theta, and Γ2,¬ψ⊢¬θ\Gamma_2,\neg \psi \vdash \neg
\theta. So by (¬(\negI), Γ1,Γ2⊢¬¬ψ\Gamma_1, \Gamma_2 \vdash \neg \neg
\psi. By (DNE), Γ1,Γ2⊢ψ\Gamma_1, \Gamma_2 \vdash \psi. 

Theorem 11. The rule of Cut.  If Γ1⊢ψ\Gamma_1 \vdash
\psi and Γ2,ψ⊢θ\Gamma_2, \psi \vdash \theta, then Γ1,Γ2⊢θ\Gamma_1, \Gamma_2
\vdash \theta.

Proof: Suppose Γ1⊢ψ\Gamma_1 \vdash \psi and
Γ2,ψ⊢θ\Gamma_2, \psi \vdash \theta.  We proceed by induction on the
number of rules used to establish Γ2,ψ⊢θ\Gamma_2, \psi \vdash \theta.
Suppose that nn is a natural number, and that the theorem holds for
any argument that was derived using fewer than nn rules. Suppose
that Γ2,ψ⊢θ\Gamma_2, \psi \vdash \theta was derived using exactly nn
rules. If the last rule used was (=(=I), then Γ1,Γ2⊢θ\Gamma_1, \Gamma_2
\vdash \theta is also an instance of (=(=I). If Γ2,ψ⊢θ\Gamma_2, \psi
\vdash \theta is an instance of (As), then either θ\theta is
ψ\psi, or θ\theta is a member of Γ2\Gamma_2. In the former
case, we have Γ1⊢θ\Gamma_1 \vdash \theta by supposition, and get
Γ1,Γ2⊢θ\Gamma_1, \Gamma_2 \vdash \theta by Weakening (Theorem 8).  In the
latter case, Γ1,Γ2⊢θ\Gamma_1, \Gamma_2 \vdash \theta is itself an
instance of (As). Suppose that Γ2,ψ⊢θ\Gamma_2, \psi \vdash \theta was
obtained using (&E). Then we have Γ2,ψ⊢(θ&ϕ)\Gamma_2, \psi \vdash(\theta
\amp \phi).  The induction hypothesis gives us Γ1,Γ2⊢(θ&ϕ)\Gamma_1, \Gamma_2
\vdash(\theta \amp \phi), and (&E) produces Γ1,Γ2⊢θ\Gamma_1, \Gamma_2
\vdash \theta.  The remaining cases are similar. 


Theorem 11 allows us to chain together inferences. This fits the
practice of establishing theorems and lemmas and then using those
theorems and lemmas later, at will. The cut principle is, some think,
essential to reasoning. In some logical systems, the cut principle is
a deep theorem; in others it is invalid. The system here was designed,
in part, to make the proof of Theorem 11 straightforward.

 If Γ⊢Dθ\Gamma \vdash_D \theta, then
 we say that the sentence θ\theta is a deductive consequence
 of the set of sentences Γ\Gamma, and that the argument
 ⟨Γ,θ⟩\langle \Gamma,\theta \rangle is deductively valid. A sentence
 θ\theta is a logical theorem, or a deductive logical
 truth,
 if ⊢Dθ\vdash_D \theta. That is,
 θ\theta is a logical theorem if it is a deductive consequence of the
 empty set. A set Γ\Gamma of sentences is consistent if there
 is no sentence θ\theta such that
 Γ⊢Dθ\Gamma \vdash_D \theta and
 Γ⊢D¬θ\Gamma \vdash_D \neg \theta.
 That is, a set is consistent if it does not entail a pair of
 contradictory opposite sentencess.


Theorem 12. A set Γ\Gamma is consistent if and
only if there is a sentence θ\theta such that it is not the case
that Γ⊢θ\Gamma \vdash \theta.

Proof: Suppose that Γ\Gamma is consistent and let
θ\theta be any sentence. Then either it is not the case that
Γ⊢θ\Gamma \vdash \theta or it is not the case that Γ⊢¬θ\Gamma \vdash
\neg \theta.  For the converse, suppose that Γ\Gamma is
inconsistent and let ψ\psi be any sentence. We have that there is a
sentence such that both Γ⊢θ\Gamma \vdash \theta and Γ⊢¬θ\Gamma \vdash
\neg \theta. By ex falso quodlibet (Theorem 10), Γ⊢ψ\Gamma
\vdash \psi. 


Define a set Γ\Gamma of sentences of the language L1K=\LKe to
be maximally consistent if Γ\Gamma is consistent and for
every sentence θ\theta of L1K=\LKe, if θ\theta is not in
Γ\Gamma, then Γ,θ\Gamma,\theta is inconsistent. In other words,
Γ\Gamma is maximally consistent if Γ\Gamma is consistent, and
adding any sentence in the language not already in Γ\Gamma renders
it inconsistent. Notice that if Γ\Gamma is maximally consistent
then Γ⊢θ\Gamma \vdash \theta if and only if θ\theta is in
Γ\Gamma.


Theorem 13. The Lindenbaum Lemma.  Let Γ\Gamma be
any consistent set of sentences of L1K=.\LKe . Then there is a set
Γ′\Gamma' of sentences of L1K=\LKe such that Γ⊆Γ′\Gamma \subseteq
\Gamma' and Γ′\Gamma' is maximally consistent.

Proof: Although this theorem holds in general, we
assume here that the set KK of non-logical terminology is either
finite or denumerably infinite (i.e., the size of the natural numbers,
usually called ℵ0)\aleph_0).  It follows that there is an enumeration
θ0,θ1,…\theta_0, \theta_1,\ldots of the sentences of L1K=\LKe, such that
every sentence of L1K=\LKe eventually occurs in the list. Define a
sequence of sets of sentences, by recursion, as follows: Γ0\Gamma_0
is Γ\Gamma; for each natural number nn, if Γn,θn\Gamma_n,
\theta_n is consistent, then let Γn+1=Γn,θn\Gamma_{n+1} = \Gamma_n,
\theta_n. Otherwise, let Γn+1=Γn\Gamma_{n+1} = \Gamma_n. Let
Γ′\Gamma' be the union of all of the sets Γn\Gamma_n. Intuitively,
the idea is to go through the sentences of L1K=\LKe, throwing each one
into Γ′\Gamma' if doing so produces a consistent set. Notice that
each Γn\Gamma_n is consistent. Suppose that Γ′\Gamma' is
inconsistent. Then there is a sentence θ\theta such that
Γ′⊢θ\Gamma'\vdash \theta and Γ′⊢¬θ\Gamma'\vdash \neg \theta. By Theorem
9 and Weakening (Theorem 8), there is finite subset Γ″\Gamma'' of
Γ′\Gamma' such that Γ″⊢θ\Gamma''\vdash \theta and Γ″⊢¬θ\Gamma''\vdash
\neg \theta.  Because Γ″\Gamma'' is finite, there is a natural
number nn such that every member of Γ″\Gamma'' is in
Γn\Gamma_n. So, by Weakening again, Γn⊢θ\Gamma_n \vdash \theta and
Γn⊢¬θ\Gamma_n \vdash \neg \theta. So Γn\Gamma_n is inconsistent,
which contradicts the construction. So Γ′\Gamma' is consistent. Now
suppose that a sentence θ\theta is not in Γ′\Gamma'. We have to
show that Γ′,θ\Gamma', \theta is inconsistent. The sentence θ\theta
must occur in the aforementioned list of sentences; say that θ\theta
is θm\theta_m.  Since θm\theta_m is not in Γ′\Gamma', then it is
not in Γm+1\Gamma_{m+1}. This happens only if Γm,θm\Gamma_m,
\theta_m is inconsistent. So a pair of contradictory opposites can
be deduced from Γm,θm\Gamma_m,\theta_m.  By Weakening, a pair of
contradictory opposites can be deduced from Γ′,θm\Gamma', \theta_m. So
Γ′,θm\Gamma', \theta_m is inconsistent. Thus, Γ′\Gamma' is maximally
consistent.


Notice that this proof uses a principle corresponding to the law of
excluded middle. In the construction of Γ′\Gamma', we assumed that,
at each stage, either Γn\Gamma_n is consistent or it is
not. Intuitionists, who demur from excluded middle, do not accept the
Lindenbaum lemma.
4. Semantics


Let KK be a set of non-logical terminology. An
interpretation for the language L1K=\LKe is a structure M=⟨d,I⟩M =
\langle d,I\rangle, where dd is a non-empty set, called
the domain-of-discourse, or simply the
domain, of the interpretation, and II is an
interpretation function. Informally, the domain is what we
interpret the language
 L1K=\LKe to be
about. It is what the variables range over. The interpretation
function assigns appropriate extensions to the non-logical terms. In
particular,

If cc is a constant in KK, then I(c)I(c) is a member of the
domain dd.


 Thus we assume that every constant denotes something. Systems where
 this is not assumed are called free logics (see the entry
 on free logic). Continuing,


If P0P^0 is a zero-place predicate letter in KK, then I(P)I(P) is
a truth value, either truth or falsehood.

If Q1Q^1 is a one-place predicate letter in KK, then I(Q)I(Q) is
a subset of dd. Intuitively, I(Q)I(Q) is the set of members of the
domain that the predicate QQ holds of. If QQ represents
“red”, then I(Q)I(Q) is the set of red members of the
domain.

If R2R^2 is a two-place predicate letter in KK, then I(R)I(R) is
a set of ordered pairs of members of dd. Intuitively, I(R)I(R) is
the set of pairs of members of the domain that the relation RR
holds between. If RR represents “love”, then I(R)I(R)
is the set of pairs ⟨a,b⟩\langle a,b\rangle such that aa and bb
are the members of the domain for which aa loves bb.

In general, if Sn^n is an nn-place predicate letter in
KK, then I(S)I(S) is a set of ordered nn-tuples of members of
dd.


 Define ss to be a variable-assignment, or simply an
assignment, on an interpretation MM, if ss is a
function from the variables to the domain dd of MM. The role of
variable-assignments is to assign denotations to the free
variables of open formulas. (In a sense, the quantifiers determine the
“meaning” of the bound variables.) 

Let tt be a term of L1K=\LKe.  We define the denotation of
tt in MM under ss, in terms of the interpretation function
and variable-assignment:
 
If tt is a constant, then DM,s(t)D_{M,s}(t) is I(t)I(t), and if tt
is a variable, then DM,s(t)D_{M,s}(t) is s(t)s(t).


 That is, the interpretation MM assigns denotations to the
constants, while the variable-assignment assigns denotations to the
(free) variables. If the language contained function symbols, the
denotation function would be defined by recursion.

We now define a relation of satisfaction between
interpretations, variable-assignments, and formulas of L1K=\LKe. If
ϕ\phi is a formula of L1K=,M\LKe, M is an interpretation for
L1K=\LKe, and ss is a variable-assignment on MM, then we write
M,s⊨ϕM,s\vDash \phi for MM satisfies ϕ\phi under the
assignment ss. The idea is that M,s⊨ϕM,s\vDash \phi is an
analogue of “ϕ\phi comes out true when interpreted as in
MM via ss”.
 
We proceed by recursion on the complexity of the formulas of
 L1K=\LKe.
 
If t1t_1 and t2t_2 are terms, then M,s⊨t1=t2M,s\vDash t_1 =t_2 if and
only if DM,s(t1)D_{M,s}(t_1) is the same as DM,s(t2)D_{M,s}(t_2).


 This is about as straightforward as it gets. An identity
t1=t2t_1 =t_2 comes out true if and
only if the terms t1t_1 and t2t_2
denote the same thing.
 
If P0P^0 is a zero-place predicate letter in KK, then M,s⊨PM,s\vDash
P if and only if I(P)I(P) is truth.

 
If Sn^n is an nn-place predicate letter in KK and
t1,…,tnt_1, \ldots,t_n are terms, then M,s⊨St1…tnM,s\vDash St_1 \ldots t_n if
and only if the nn-tuple ⟨DM,s(t1),…,DM,s(tn)⟩\langle D_{M,s}(t_1),
\ldots,D_{M,s}(t_n)\rangle is in I(S)I(S).

This takes care of the atomic formulas. We now proceed to the
compound formulas of the language, more or less following the meanings of the
English counterparts of the logical terminology.

 M,s⊨¬θM,s\vDash \neg \theta if and only if it is not the case that
M,s⊨θM,s\vDash \theta.
 M,s⊨(θ&ψ)M,s\vDash(\theta \amp \psi) if and only if both M,s⊨θM,s\vDash
\theta and M,s⊨ψM,s\vDash \psi.
 M,s⊨(θ∨ψ)M,s\vDash(\theta \vee \psi) if and only if either M,s⊨θM,s\vDash
\theta or M,s⊨ψM,s\vDash \psi.
 M,s⊨(θ→ψ)M,s\vDash(\theta \rightarrow \psi) if and only if either it is
not the case that M,s⊨θM,s\vDash \theta, or M,s⊨ψM,s\vDash \psi.
 M,s⊨∀vθM,s\vDash \forall v\theta if and only if M,s′⊨θM,s'\vDash
\theta, for every assignment s′s' that agrees with ss except
possibly at the variable vv.


The idea here is that ∀vθ\forall v\theta comes out true if and only
if θ\theta comes out true no matter what is assigned to the
variable vv. The final clause is similar.

M,s⊨∃vθM,s\vDash \exists v\theta if and only if M,s′⊨θM,s'\vDash \theta,
for some assignment s′s' that agrees with ss except possibly at
the variable vv.


So ∃vθ\exists v\theta comes out true if there is an assignment to
vv that makes θ\theta true.
 
 Theorem 6, unique readability, assures us that this definition is
coherent. At each stage in breaking down a formula, there is exactly
one clause to be applied, and so we never get contradictory verdicts
concerning satisfaction.
 
 As indicated, the role of variable-assignments is to give
denotations to the free variables. We now show that
variable-assignments play no other role.


Theorem 14. For any formula θ\theta, if s1s_1
and s2s_2 agree on the free variables in θ\theta, then M,s1⊨θM,s_1
\vDash \theta if and only if M,s2⊨θM,s_2 \vDash \theta.

Proof: We proceed by induction on the complexity of
the formula θ\theta. The theorem clearly holds if θ\theta is
atomic, since in those cases only the values of the
variable-assignments at the variables in θ\theta figure in the
definition. Assume, then, that the theorem holds for all formulas less
complex than θ\theta. And suppose that s1s_1 and s2s_2 agree on
the free variables of θ\theta. Assume, first, that θ\theta is a
negation, ¬ψ\neg \psi. Then, by the induction hypothesis, M,s1⊨ψM,s_1
\vDash \psi if and only if M,s2⊨ψM,s_2 \vDash \psi.  So, by the clause
for negation, M,s1⊨¬ψM,s_1 \vDash \neg \psi if and only if M,s2⊨¬ψM,s_2 \vDash
\neg \psi. The cases where the main connective in θ\theta is
binary are also straightforward. Suppose that θ\theta is ∃vψ\exists
v\psi, and that M,s1⊨∃vψM,s_1 \vDash \exists v\psi.  Then there is an
assignment s′1s_1' that agrees with s1s_1 except possibly at vv
such that M,s′1⊨ψM,s_1'\vDash \psi.  Let s′2s_2' be the assignment that
agrees with s2s_2 on the free variables not in ψ\psi and agrees
with s′1s_1' on the others. Then, by the induction hypothesis,
M,s′2⊨ψM,s_2'\vDash \psi.  Notice that s′2s_2' agrees with s2s_2 on
every variable except possibly vv. So M,s2⊨∃vψM,s_2 \vDash \exists
v\psi.  The converse is the same, and the case where θ\theta
begins with a universal quantifier is similar.


By Theorem 14, if θ\theta is a sentence, and s1,s2s_1, s_2, are any two
variable-assignments, then M,s1⊨θM,s_1 \vDash \theta if and only if
M,s2⊨θM,s_2 \vDash \theta.  So we can just write M⊨θM\vDash \theta if
M,s⊨θM,s\vDash \theta for some, or all, variable-assignments ss. So we define 
 M⊨θM\vDash \theta where θ\theta is a sentence
just in case M,s⊨θM,s\vDash\theta for all variable assignments ss.


In this case, we call MM a  model  of θ\theta.

Suppose that K′⊆KK'\subseteq K are two sets of non-logical terms. If
M=⟨d,I⟩M = \langle d,I\rangle is an interpretation of L1K=\LKe, then we
define the restriction of MM to L1K′\mathcal{L}1K' be the
interpretation M′=⟨d,I′⟩M'=\langle d,I'\rangle such that I′I' is the
restriction of II to K′K'. That is, MM and M′M' have the
same domain and agree on the non-logical terminology in K′K'. A
straightforward induction establishes the following:


Theorem 15. If M′M' is the restriction of MM to
L1K′\mathcal{L}1K', then for every sentence θ\theta of
L1K′\mathcal{L}1K', M⊨θM\vDash\theta if and only if M′⊨θM'\vDash \theta.

Theorem 16. If two interpretations M1M_1 and
M2M_2 have the same domain and agree on all of the non-logical
terminology of a sentence θ\theta, then M1⊨θM_1\vDash\theta if and
only if M2⊨θM_2\vDash \theta.


 In short, the satisfaction of a sentence θ\theta only
depends on the domain of discourse and the interpretation of the
non-logical terminology in θ\theta.

We say that an argument ⟨Γ,θ⟩\langle \Gamma,\theta \rangle
is semantically valid, or just valid, written
Γ⊨θ\Gamma \vDash \theta, if for every interpretation MM of the
language, if M⊨ψM\vDash\psi, for every member ψ\psi of Γ\Gamma,
then M⊨θM\vDash\theta. If Γ⊨θ\Gamma \vDash \theta, we also say that
θ\theta is a logical consequence, or semantic
consequence, or
model-theoretic consequence of Γ\Gamma. The definition
corresponds to the informal idea that an argument is valid if it is
not possible for its premises to all be true and its conclusion
false. Our definition of logical consequence also sanctions the common
thesis that a valid argument is truth-preserving--to the extent that
satisfaction represents truth. Officially, an argument in L1K=\LKe is
valid if its conclusion comes out true under every interpretation of
the language in which the premises are true. Validity is the
model-theoretic counterpart to deducibility.

A sentence θ\theta is logically true, or
valid, if M⊨θM\vDash \theta, for every interpretation
MM. A sentence is logically true if and only if
it is a consequence of the empty set. If θ\theta is logically true,
then for any set Γ\Gamma of sentences, Γ⊨θ\Gamma \vDash \theta.
Logical truth is the model-theoretic counterpart of theoremhood.

A sentence θ\theta is satisfiable if there is an
interpretation MM such
that M⊨θM\vDash \theta.  That is, θ\theta is satisfiable if
there is an interpretation that satisfies it. A set
Γ\Gamma of sentences is satisfiable if there is an interpretation
MM such that M⊨θM\vDash\theta, for every sentence θ\theta in Γ\Gamma. If Γ\Gamma is
a set of sentences and if M⊨θM\vDash \theta for each sentence
θ\theta in Γ\Gamma, then we say that MM is a model
of Γ\Gamma. So a set of sentences is satisfiable if it has a
model. Satisfiability is the model-theoretic counterpart to
consistency.

Notice that Γ⊨θ\Gamma \vDash \theta if and only if the set
Γ,¬θ\Gamma,\neg \theta is not satisfiable. It follows that if a set
Γ\Gamma is not satisfiable, then if θ\theta is any sentence,
Γ⊨θ\Gamma \vDash \theta.  This is a model-theoretic counterpart
to ex falso quodlibet (see Theorem 10). We have the
following, as an analogue to Theorem 12:


Theorem 17. Let Γ\Gamma be a set of sentences. The
following are equivalent: (a) Γ\Gamma is satisfiable; (b) there is
no sentence θ\theta such that both Γ⊨θ\Gamma \vDash \theta and
Γ⊨¬θ\Gamma \vDash \neg \theta; (c) there is some sentence ψ\psi such
that it is not the case that Γ⊨ψ\Gamma \vDash \psi.

Proof: (a)⇒\Rightarrow(b): Suppose that Γ\Gamma
is satisfiable and let θ\theta be any sentence. There is an
interpretation MM such that M⊨ψM\vDash \psi
for every member ψ\psi of Γ\Gamma. By the clause for negations,
we cannot have both M⊨θM\vDash \theta and M⊨¬θM\vDash \neg
\theta.  So either ⟨Γ,θ⟩\langle \Gamma,\theta \rangle is not valid or
else ⟨Γ,¬θ⟩\langle \Gamma,\neg \theta \rangle is not valid.
(b)⇒\Rightarrow(c): This is immediate.  (c)⇒\Rightarrow(a):
Suppose that it is not the case that Γ⊨ψ\Gamma \vDash \psi. Then
there is an interpretation MM such that
M⊨θM\vDash \theta, for every sentence θ\theta in Γ\Gamma and
it is not the case that M⊨ψM\vDash \psi.  A fortiori, MM
satisfies every member of Γ\Gamma, and so Γ\Gamma is
satisfiable.

5. Meta-theory


We now present some results that relate the deductive notions to their
model-theoretic counterparts. The first one is probably the most
straightforward. We motivated both the various rules of the deductive
system DD and the various clauses in the definition of
satisfaction in terms of the meaning of the English counterparts to
the logical terminology (more or less, with the same simplifications
in both cases). So one would expect that an argument is deducible, or
deductively valid, only if it is semantically valid.


Theorem 18. Soundness. For any sentence θ\theta and
set Γ\Gamma of sentences, if Γ⊢Dθ\Gamma \vdash_D \theta, then
Γ⊨θ\Gamma \vDash \theta.

Proof: We proceed by induction on the number of
clauses used to establish Γ⊢θ\Gamma \vdash \theta.  So let nn be a
natural number, and assume that the theorem holds for any argument
established as deductively valid with fewer than nn steps. And
suppose that Γ⊢θ\Gamma \vdash \theta was established using exactly
nn steps. If the last rule applied was (=(=I) then θ\theta is
a sentence in the form t=tt=t, and so θ\theta is logically true. A
fortiori, Γ⊨θ\Gamma \vDash \theta.  If the last rule applied was
(As), then θ\theta is a member of Γ\Gamma, and so of course any
interpretation that satisfies every member of
Γ\Gamma also satisfies θ\theta. Suppose the last rule applied is
(&I). So θ\theta has the form (ϕ&ψ)(\phi \amp \psi), and we have
Γ1⊢ϕ\Gamma_1 \vdash \phi and Γ2⊢ψ\Gamma_2 \vdash \psi, with Γ=Γ1,Γ2\Gamma =
\Gamma_1, \Gamma_2. The induction hypothesis gives us Γ1⊨ϕ\Gamma_1
\vDash \phi and Γ2⊨ψ\Gamma_2 \vDash \psi.  Suppose that MM
satisfies every member of Γ\Gamma. Then MM satisfies every
member of Γ1\Gamma_1, and so MM satisfies ϕ\phi.  Similarly,
MM satisfies every member of Γ2\Gamma_2, and so MM
satisfies ψ\psi. Thus, by the clause for “&\amp” in
the definition of satisfaction, MM satisfies θ\theta. So
Γ⊨θ\Gamma \vDash \theta.  

Suppose the last clause applied was (∃E)(\exists\mathrm{E}). So we
have Γ1⊢∃vϕ\Gamma_1 \vdash \exists v\phi and Γ2,ϕ(v|t)⊢θ\Gamma_2, \phi(v|t)
\vdash \theta, where Γ=Γ1,Γ2\Gamma = \Gamma_1, \Gamma_2, and tt does
not occur in ϕ,θ\phi , \theta , or in any member of Γ2\Gamma_2.

We need to show that Γ⊨θ\Gamma\vDash\theta. By the induction
hypothesis, we have that Γ1⊨∃vϕ\Gamma_1\vDash\exists v\phi and
Γ2,ϕ(v|t)⊨θ\Gamma_2, \phi(v|t)\vDash\theta. Let MM be an interpretation
such that MM makes every member of Γ\Gamma true. So, MM makes
every member of Γ1\Gamma_1 and Γ2\Gamma_2 true. Then
M,s⊨∃vϕM,s\vDash\exists v\phi for all variable assignments ss, so
there is an s′s' such that M,s′⊨ϕM,s'\vDash\phi. Let M′M' differ
from MM only in that IM′(t)=s′(v)I_{M'}(t)=s'(v). Then,
M′,s′⊨ϕ(v|t)M',s'\vDash\phi(v|t) and M′,s′⊨Γ2M',s'\vDash\Gamma_2 since tt does
not occur in ϕ\phi or Γ2\Gamma_2. So,
M′,s′⊨θM',s'\vDash\theta. Since tt does not occur in θ\theta and
M′M' differs from MM only with respect to IM′(t)I_{M'}(t),
M,s′⊨θM,s'\vDash\theta. Since θ\theta is a sentence, s′s' doesn't
matter, so M⊨θM\vDash\theta as desired. Notice the role of the
restrictions on (∃(\existsE) here. The other cases are about as
straightforward. 

Corollary 19. Let Γ\Gamma be a set of sentences. If
Γ\Gamma is satisfiable, then Γ\Gamma is consistent. 

Proof: Suppose that Γ\Gamma is satisfiable. So let
MM be an interpretation such that MM
satisfies every member of Γ\Gamma. Assume that Γ\Gamma is
inconsistent. Then there is a sentence θ\theta such that Γ⊢θ\Gamma
\vdash \theta and Γ⊢¬θ\Gamma \vdash \neg \theta. By soundness
(Theorem 18), Γ⊨θ\Gamma \vDash \theta and Γ⊨¬θ\Gamma \vDash \neg
\theta. So we have that M⊨θM\vDash \theta and M⊨¬θM\vDash \neg
\theta. But this is impossible, given the clause for negation in the
definition of satisfaction.


 Even though the deductive system DD and the model-theoretic
semantics were developed with the meanings of the logical terminology
in mind, one should not automatically expect the converse to
soundness (or Corollary 19) to hold. For all we know so far, we may
not have included enough rules of inference to deduce every valid
argument. The converses to soundness and Corollary 19 are among the
most important and influential results in mathematical logic. We begin
with the latter.


Theorem 20. Completeness. Gödel [1930]. Let
Γ\Gamma be a set of sentences. If Γ\Gamma is consistent, then
Γ\Gamma is satisfiable.

Proof: The proof of completeness is rather
complex. We only sketch it here. Let Γ\Gamma be a consistent set of
sentences of L1K=\LKe. Again, we assume for simplicity that the set
KK of non-logical terminology is either finite or countably
infinite (although the theorem holds even if KK is
uncountable). The task at hand is to find an interpretation MM such
that MM satisfies every member of Γ\Gamma. Consider the language
obtained from L1K=\LKe by adding a denumerably infinite stock of new
individual constants c0,c1,…c_0, c_1,\ldots We stipulate that the
constants, c0,c1,…c_0, c_1,\ldots, are all different from each other and
none of them occur in KK. One interesting feature of this
construction, due to Leon Henkin, is that we build an interpretation
of the language from the language itself, using some of the constants
as members of the domain of discourse. Let θ0(x),θ1(x),…\theta_0 (x), \theta_1
(x),\ldots be an enumeration of the formulas of the expanded
language with at most one free variable, so that each formula with at
most one free variable occurs in the list eventually. Define a
sequence Γ0,Γ1,…\Gamma_0, \Gamma_1,\ldots of sets of sentences (of the
expanded language) by recursion as follows: Γ0=Γ\Gamma_0 = \Gamma; and
Γn+1=Γn,(∃xθn→θn(x|ci))\Gamma_{n+1} = \Gamma_n,(\exists x\theta_n \rightarrow
\theta_{n}(x|c_i)), where cic_i is the first constant in the above
list that does not occur in θn\theta_n or in any member of
Γn\Gamma_n. The underlying idea here is that if ∃xθn\exists
x\theta_nis true, then cic_i is to be one such xx. Let
Γ\Gamma be the union of the sets Γn\Gamma_n. 

We sketch a proof that Γ′\Gamma' is consistent. Suppose that
Γ′\Gamma' is inconsistent. By Theorem 9, there is a finite subset of
Γ\Gamma that is inconsistent, and so one of the sets Γm\Gamma_m
is inconsistent. By hypothesis, Γ0=Γ\Gamma_0 = \Gamma is
consistent. Let nn be the smallest number such that Γn\Gamma_n is
consistent, but Γn+1=Γn,(∃xθn→θn(x|ci))\Gamma_{n+1} = \Gamma_n,(\exists x\theta_n
\rightarrow \theta_{n}(x|c_i)) is inconsistent. By (¬(\negI), we
have that
Γn⊢¬(∃xθn→θn(x|ci)).\tag{1}
\Gamma_n \vdash \neg(\exists x\theta_n \rightarrow \theta_n(x|c_i)).

 By ex falso quodlibet (Theorem 10), Γn,¬∃xθn,∃xθn⊢θn(x|ci)\Gamma_n, \neg
\exists x\theta_n, \exists x\theta_n \vdash \theta_n (x|c_i).  So by
(→(\rightarrowI), Γn,¬∃xθn⊢(∃xθn→θn(x|ci))\Gamma_n, \neg \exists x\theta_n \vdash(\exists
x\theta_n \rightarrow \theta_n (x|c_i)).  From this and (1), we have
Γn⊢¬¬∃xθn\Gamma_n \vdash \neg \neg \exists x\theta_n, by (¬(\negI), and
by (DNE) we have
Γn⊢∃xθn.\tag{2}
\Gamma_n \vdash \exists x\theta_n .


By (As), Γn,θn(x|ci),∃xθn⊢θn(x|ci)\Gamma_n, \theta_n (x|c_i), \exists x\theta_n \vdash
\theta_n (x|c_i).  So by (→(\rightarrowI), Γn,θn(x|ci)⊢(∃xθn→θn(x|ci))\Gamma_n, \theta_n
(x|c_i)\vdash(\exists x\theta_{n} \rightarrow
\theta_{n}(x|c_i)).  From this and (1), we have Γn⊢¬θn(x|ci)\Gamma_n \vdash
\neg \theta_n (x|c_i), by (¬(\negI). Let tt be a term that
does not occur in θn\theta_n or in any member of
Γn\Gamma_n. By uniform substitution of tt for cic_i, we can
turn the derivation of Γn⊢¬θn(x|ci)\Gamma_n \vdash \neg \theta_n (x|c_i) into
Γn⊢¬θn(x|t)\Gamma_n \vdash \neg \theta_n (x|t).  By (∀(\forallI), we
have
Γn⊢∀v¬θn(x|v).\tag{3}
\Gamma_n \vdash \forall v\neg \theta_n (x|v).


By (As) we have {∀v¬θn(x|v),θn}⊢θn\{\forall v\neg \theta_n (x|v),\theta_n\}\vdash
\theta_n and by (∀(\forallE) we have {∀v¬θn(x|v),θn}⊢¬θn\{\forall v\neg \theta_n
(x|v), \theta_n\}\vdash \neg \theta_n.  So {∀v¬θn(x|v),θn}\{\forall v\neg
\theta_n (x|v), \theta_n\} is inconsistent. Let ϕ\phi be any
sentence of the language. By ex falso quodlibet (Theorem 10), we have that
{∀v¬θn(x|v),θn}⊢ϕ\{\forall v\neg \theta_n (x|v),\theta_n\}\vdash \phi and
{∀v¬θn(x|v),θn}⊢¬ϕ\{\forall v\neg \theta_n (x|v), \theta_n\}\vdash \neg \phi. So
with (2), we have that Γn,∀v¬θn(x|v)⊢ϕ\Gamma_n, \forall v\neg \theta_n (x|v)\vdash
\phi and Γn,∀v¬θn(x|v)⊢¬ϕ\Gamma_n, \forall v\neg \theta_n (x|v)\vdash \neg \phi,
by (∃(\existsE). By Cut (Theorem 11), Γn⊢ϕ\Gamma_n \vdash \phi and
Γn⊢¬ϕ\Gamma_n \vdash \neg \phi. So Γn\Gamma_n is inconsistent,
contradicting the assumption. So Γ′\Gamma' is consistent.
 Applying the Lindenbaum Lemma (Theorem 13), let Γ″\Gamma'' be a
maximally consistent set of sentences (of the expanded language) that
contains Γ′\Gamma'. So, of course, Γ″\Gamma'' contains
Γ\Gamma. We can now define an interpretation MM such that MM satisfies every
member of Γ″\Gamma''.

 If we did not have a sign for identity in the language, we would let
the domain of MM be the collection of new constants {c0,c1,…}\{c_0, c_1,
\ldots \}. But as it is, there may be a sentence in the form
ci=cjc_{i}=c_{j}, with i≠ji\ne j, in Γ″\Gamma''. If so, we
cannot have both cic_i and cjc_j in the domain of the
interpretation (as they are distinct constants). So we define the
domain dd of MM to be the set {ci\{c_i | there is no j<ij\lt i
such that ci=cjc_{i}=c_{j} is in Γ″}\Gamma''\}. In other words, a
constant cic_i is in the domain of MM if Γ″\Gamma'' does not
declare it to be identical to an earlier constant in the list. Notice
that for each new constant cic_i, there is exactly one j≤ij\le i
such that cjc_j is in dd and the sentence ci=cjc_{i}=c_{j} is
in Γ″\Gamma''.

We now define the interpretation function II. Let aa be any
constant in the expanded language. By (=(=I) and (∃(\existsI),
Γ″⊢∃xx=a\Gamma''\vdash \exists x x=a, and so ∃xx=a∈Γ″\exists x x=a \in
\Gamma''.  By the construction of Γ′\Gamma', there is a sentence
in the form (∃xx=a→ci=a)(\exists x x=a \rightarrow c_i =a) in Γ″\Gamma''. We
have that ci=ac_i =a is in Γ″\Gamma''.  As above, there is exactly
one cjc_j in dd such that ci=cjc_{i}=c_{j} is in
Γ″\Gamma''. Let I(a)=cjI(a)=c_j. Notice that if cic_i is a constant
in the domain dd, then II(ci)=ci_i)=c_i.  That is each cic_i in
dd denotes itself.

Let PP be a zero-place predicate letter in KK.  Then I(P)I(P) is
truth if PP is in Γ″\Gamma'' and I(P)I(P) is falsehood otherwise.
Let QQ be a one-place predicate letter in KK. Then I(Q)I(Q) is
the set of constants {\{ci|ci_i | c_i is in dd and the sentence
QcQc is in Γ″}\Gamma''\}. Let RR be a binary predicate letter in
KK. Then I(R)I(R) is the set of pairs of constants {⟨ci,cj⟩|ci\{\langle
c_i,c_j\rangle | c_i is in d,cjd, c_j is in dd, and the sentence
RcicjRc_{i}c_{j} is in Γ″}\Gamma''\}. Three-place predicates,
etc. are interpreted similarly. In effect, II interprets the
non-logical terminology as they are in Γ″\Gamma''.

The variable assignments are similar. If vv is a variable, then
 s(v)=cis(v)=c_i, where cic_i is the first constant in dd such that
 ci=vc_i=v is in Γ″\Gamma ''.

The final item in this proof is a lemma that for every formula
θ\theta in the expanded language, M⊨θM\vDash \theta if and only
if θ\theta is in Γ″\Gamma''. This proceeds by induction on the
complexity of θ\theta. The case where θ\theta is atomic follows
from the definitions of MM (i.e., the domain dd and the
interpretation function II, and the variable assignment ss). The
other cases follow from the various clauses in the definition of
satisfaction.

Since Γ⊆Γ″\Gamma \subseteq \Gamma'', we have that MM satisfies
every member of Γ\Gamma. By Theorem 15, the restriction of MM to
the original language L1K=\LKe and ss also satisfies every member
of Γ\Gamma. Thus Γ\Gamma is satisfiable. 


A converse to Soundness (Theorem 18) is a straightforward
corollary:


Theorem 21. For any sentence θ\theta and set
Γ\Gamma of sentences, if Γ⊨θ\Gamma \vDash \theta, then Γ⊢Dθ\Gamma
\vdash_D \theta.

Proof: Suppose that Γ⊨θ\Gamma \vDash \theta.  Then
there is no interpretation MM such
that M satisfies every member of Γ\Gamma but does not
satisfy θ\theta. So the set Γ,¬θ\Gamma,\neg \theta is not
satisfiable. By Completeness (Theorem 20), Γ,¬θ\Gamma,\neg \theta is
inconsistent. So there is a sentence ϕ\phi such that Γ,¬θ⊢ϕ\Gamma,\neg
\theta \vdash \phi and Γ,¬θ⊢¬ϕ\Gamma,\neg \theta \vdash \neg \phi. By
(¬(\negI), Γ⊢¬¬θ\Gamma \vdash \neg \neg \theta, and by (DNE) Γ⊢θ\Gamma
\vdash \theta.


Our next item is a corollary of Theorem 9, Soundness (Theorem 18),
and Completeness:


Corollary 22. Compactness. A set Γ\Gamma of
sentences is satisfiable if and only if every finite subset of
Γ\Gamma is satisfiable.

Proof: If MM satisfies every member of
Γ\Gamma, then MM satisfies every member of each finite subset
of Γ\Gamma. For the converse, suppose that Γ\Gamma is not
satisfiable. Then we show that some finite subset of Γ\Gamma is not
satisfiable. By Completeness (Theorem 20), Γ\Gamma is
inconsistent. By Theorem 9 (and Weakening), there is a finite subset
Γ′⊆Γ\Gamma'\subseteq \Gamma such that Γ′\Gamma' is inconsistent.  By
Corollary 19,Γ′19, \Gamma' is not satisfiable.


 Soundness and completeness together entail that an argument is
deducible if and only if it is valid, and a set of sentences is
consistent if and only if it is satisfiable. So we can go back and
forth between model-theoretic and proof-theoretic notions,
transferring properties of one to the other. Compactness holds in the
model theory because all derivations use only a finite number of
premises.

 Recall that in the proof of Completeness (Theorem 20), we made the
simplifying assumption that the set KK of non-logical
constants is either finite or denumerably infinite. The
interpretation we produced was itself either finite or denumerably
infinite. Thus, we have the following:

Corollary 23. Löwenheim-Skolem Theorem. Let
Γ\Gamma be a satisfiable set of sentences of the language
L1K=\LKe. If Γ\Gamma is either finite or denumerably infinite, then
Γ\Gamma has a model whose domain is either finite or denumerably
infinite.


In general, let Γ\Gamma be a satisfiable set of sentences of
L1K=\LKe, and let κ\kappa be the larger of the size of Γ\Gamma
and denumerably infinite. Then Γ\Gamma has a model whose domain is
at most size κ\kappa.

There is a stronger version of Corollary 23. Let M1=⟨d1,I1⟩M_1 =\langle
d_1,I_1\rangle and M2=⟨d2,I2⟩M_2 =\langle d_2,I_2\rangle be
interpretations of the language L1K=\LKe. Define M1M_1 to be
a submodel of M2M_2 if d1⊆d2,I1(c)=I2(c)d_1 \subseteq d_2, I_1 (c) = I_2
(c) for each constant cc, and I1I_1 is the restriction of
I2I_2 to d1d_1. For example, if RR is a binary relation letter
in KK, then for all a,ba,b in d1d_1, the pair ⟨a,b⟩\langle
a,b\rangle is in I1(R)I_1 (R) if and only if ⟨a,b⟩\langle a,b\rangle
is in I2(R)I_2 (R). If we had included function letters among the
non-logical terminology, we would also require that d1d_1 be closed
under their interpretations in M2M_2. Notice that if M1M_1 is a
submodel of M2M_2, then any variable-assignment on M1M_1 is also a
variable-assignment on M2M_2.

Say that two interpretations M1=⟨d1,I1⟩,M2=⟨d2,I2⟩M_1 =\langle d_1,I_1\rangle, M_2
=\langle d_2,I_2\rangle are equivalent if one of them is a
submodel of the other, and for any formula of the language and any
variable-assignment ss on the submodel, M1,s⊨θM_1,s\vDash \theta if
and only if M2,s⊨θM_2,s\vDash \theta.  Notice that if two
interpretations are equivalent, then they satisfy the same
sentences.


Theorem 25. Downward Löwenheim-Skolem Theorem.
Let M=⟨d,I⟩M = \langle d,I\rangle be an interpretation of the language
L1K=\LKe. Let d1d_1 be any subset of dd, and let κ\kappa be
the maximum of the size of KK, the size of d1d_1, and denumerably
infinite. Then there is a submodel M′=⟨d′,I′⟩M' = \langle d',I'\rangle of
MM such that (1) d′d' is not larger than κ\kappa, and (2)
MM and M′M' are equivalent. In particular, if the set KK of
non-logical terminology is either finite or denumerably infinite, then
any interpretation has an equivalent submodel whose domain is either
finite or denumerably infinite.

Proof: Like completeness, this proof is complex, and
we rest content with a sketch. The downward Löwenheim-Skolem
theorem invokes the axiom of choice, and indeed, is equivalent to the
axiom of choice (see the entry on
 the axiom of choice). 
So let CC be a choice function on the powerset of dd, so that
for each non-empty subset e⊆d,C(e)e\subseteq d, C(e) is a member of
ee. We stipulate that if ee is the empty set, then C(e)C(e) is
C(d)C(d).

Let ss be a variable-assignment on MM, let θ\theta be a
formula of L1K=\LKe, and let vv be a variable. Define the
vv-witness of θ\theta over s, written wv(θ,s)w_v
(\theta,s), as follows: Let qq be the set of all elements c∈dc\in
d such that there is a variable-assignment s′s' on MM that
agrees with ss on every variable except possibly vv, such that
M,s′⊨θM,s'\vDash \theta, and s′(v)=cs'(v)=c. Then wv(θ,s)=C(q)w_v (\theta,s) =
C(q). Notice that if M,s⊨∃vθM,s\vDash \exists v\theta, then qq is
the set of elements of the domain that can go for vv in
θ\theta. Indeed, M,s⊨∃vθM,s\vDash \exists v\theta if and only if qq
is non-empty. So if M,s⊨∃vθM,s\vDash \exists v\theta, then wv(θ,s)w_v
(\theta,s) (i.e., C(q))C(q)) is a chosen element of the domain that
can go for vv in θ\theta. In a sense, it is a
“witness” that verifies M,s⊨∃vθM,s\vDash \exists v\theta.

If ee is a non-empty subset of the domain dd, then define a
variable-assignment ss to be an ee-assignment if for
all variables u,s(u)u, s(u) is in ee. That is, ss is an
ee-assignment if ss assigns an element of ee to each
variable. Define sk(e)sk(e), the
Skolem-hull of ee, to be the set:
e∪{wv(θ,s)|θ is a formula in L1K=,v is a variable, and s is an e-assignment}.\begin{align*}
e \cup \{w_v (\theta,s)|&amp; \theta  \text{ is a formula in } \LKe, \\
  &amp; v \text{ is a variable, and } \\
  &amp; s \text{ is an } e\text{-assignment} \}.
\end{align*}

 That is, the Skolem-Hull of ee is the set ee together with
every vv-witness of every formula over every
ee-assignment. Roughly, the idea is to start with ee and then
throw in enough elements to make each existentially quantified formula
true. But we cannot rest content with the Skolem-hull, however. Once
we throw the “witnesses” into the domain, we need to deal
with sk(e)sk(e) assignments. In effect, we need a set which is its own
Skolem-hull, and also contains the given subset d1d_1.

 We define a sequence of non-empty sets e0,e1,…e_0, e_1,\ldots as
follows: if the given subset d1d_1 of dd is empty and there are
no constants in KK, then let e0e_0 be C(d)C(d), the choice
function applied to the entire domain; otherwise let e0e_0 be the
union of d1d_1 and the denotations under II of the constants in
KK. For each natural number n,en+1n, e_{n+1} is sk(en)sk(e_n). Finally,
let d′d' be the union of the sets ene_n, and let I′I' be the
restriction of II to d′d'. Our interpretation is M′=⟨d′,I′⟩M' = \langle
d',I'\rangle.

Clearly, d1d_1 is a subset of d′d', and so M′M' is a submodel of
MM. Let κ\kappa be the maximum of the size of KK, the size of
d1d_1, and denumerably infinite. A calculation reveals that the size
of d′d' is at most κ\kappa, based on the fact that there are at
most κ\kappa-many formulas, and thus, at most κ\kappa-many
witnesses at each stage. Notice, incidentally, that this calculation
relies on the fact that a denumerable union of sets of size at most
κ\kappa is itself at most κ\kappa. This also relies on the axiom
of choice.

The final item is to show that M′M' is equivalent to MM: For
every formula θ\theta and every variable-assignment ss on
M′M',
M,s⊨θ if and only if M′,s⊨θ.
M,s\vDash \theta \text{ if and only if }
  M',s\vDash \theta.


The proof proceeds by induction on the complexity of
θ\theta. Unfortunately, space constraints require that we leave
this step as an exercise.


 Another corollary to Compactness (Corollary 22) is the opposite
of the Löwenheim-Skolem theorem:


Theorem 26. Upward Löwenheim-Skolem Theorem.
Let Γ\Gamma be any set of sentences of L1K=,\LKe, such that for each
natural number nn, there is an interpretation Mn=⟨dn,In⟩M_n = \langle
d_n,I_n\rangle, and an assignment sns_n on MnM_n, such that
dnd_n has at least nn elements, and Mn,snM_n,s_n satisfies every
member of Γ\Gamma. In other words, Γ\Gamma is satisfiable and
there is no finite upper bound to the size of the interpretations that
satisfy every member of Γ\Gamma. Then for any infinite cardinal
κ\kappa, there is an interpretation M=⟨d,I⟩M=\langle d,I\rangle and
assignment ss on MM, such that the size of dd is at
least κ\kappa and M,sM,s satisfies every member of
Γ\Gamma. In particular, if Γ\Gamma is a set of sentences, then
it has arbitrarily large models.

Proof: Add a collection of new constants
{cα|α<κ}\{c_{\alpha} | \alpha \lt \kappa \}, of size κ\kappa, to the
language, so that if cc is a constant in KK, then cαc_{\alpha}
is different from cc, and if α<β<κ\alpha \lt \beta \lt \kappa, then
cαc_{\alpha} is a different constant than cβc_{\beta}.  Consider
the set of formulas Γ′\Gamma' consisting of Γ\Gamma together with
the set {¬cα=cβ|α≠β}\{\neg c_{\alpha}=c_{\beta} | \alpha \ne \beta \}.
That is, Γ′\Gamma' consists of Γ\Gamma together with statements
to the effect that any two different new constants denote different
objects. Let Γ″\Gamma'' be any finite subset of Γ′\Gamma', and let
mm be the number of new constants that occur in Γ″\Gamma''.  Then
expand the interpretation MmM_m to an interpretation M′mM_m' of the
new language, by interpreting each of the new constants in
Γ″\Gamma'' as a different member of the domain dmd_m. By
hypothesis, there are enough members of dmd_m to do this. One can
interpret the other new constants at will. So MmM_m is a restriction
of M′mM_m'. By hypothesis (and Theorem 15), M′m,smM'_m,s_m satisfies
every member of Γ\Gamma. Also M′m,smM'_m,s_m satisfies the members of
{¬cα=cβ|α≠β}\{\neg c_{\alpha}=c_{\beta} | \alpha \ne \beta \} that are in
Γ″\Gamma''. So M′m,smM'_m,s_m satisfies every member of Γ″\Gamma''.
By compactness, there is an interpretation M=⟨d,I⟩M = \langle d,I\rangle
and an assignment ss on MM such that M,sM,s satisfies every
member of Γ′\Gamma'. Since Γ′\Gamma' contains every member of
{¬cα=cβ|α≠β}\{\neg c_{\alpha}=c_{\beta} | \alpha \ne \beta \}, the domain
dd of MM must be of size at least κ\kappa, since each of the
new constants must have a different denotation. By Theorem 15, the
restriction of MM to the original language L1K=\LKe satisfies every
member of Γ\Gamma, with the variable-assignment ss.


Combined, the proofs of the downward and upward Löwenheim-Skolem
theorems show that for any satisfiable set Γ\Gamma of sentences, if
there is no finite bound on the models of Γ\Gamma, then for any
infinite cardinal κ\kappa, there is a model of Γ\Gamma whose
domain has size exactly κ\kappa. Moreover, if MM is any
interpretation whose domain is infinite, then for any infinite
cardinal κ\kappa, there is an interpretation M′M' whose domain
has size exactly κ\kappa such that MM and M′M' are
equivalent.

These results indicate a weakness in the expressive resources of
first-order languages like L1K=\LKe. No satisfiable set of sentences
can guarantee that its models are all denumerably infinite, nor can
any satisfiable set of sentences guarantee that its models are
uncountable. So in a sense, first-order languages cannot express the
notion of “denumerably infinite”, at least not in the
model theory. (See the entry on
 second-order and higher-order logic.)

 Let AA be any set of sentences in a first-order language L1K=\LKe,
where KK includes terminology for arithmetic, and assume that every
member of AA is true of the natural numbers. We can even let AA
be the set of all sentences in L1K=\LKe that are true of the natural
numbers. Then AA has uncountable models, indeed models of any
infinite cardinality. Such interpretations are among those that are
sometimes called unintended, or non-standard models
of arithmetic. Let BB be any set of first-order sentences that are
true of the real numbers, and let CC be any first-order
axiomatization of set theory. Then if BB and CC are satisfiable
(in infinite interpretations), then each of them has denumerably
infinite models. That is, any first-order, satisfiable set theory or
theory of the real numbers, has (unintended) models the size of the
natural numbers. This is despite the fact that a sentence (seemingly)
stating that the universe is uncountable is provable in most
set-theories. This situation, known as the
Skolem paradox, has generated much discussion, but we must
refer the reader elsewhere for a sample of it (see the entry on
 Skolem’s paradox and
 Shapiro 1996).
6. The One Right Logic?

Logic and reasoning go hand in hand. We say that someone has reasoned
poorly about something if they have not reasoned logically, or that an
argument is bad because it is not logically valid. To date, research
has been devoted to exactly just what types of logical systems are
appropriate for guiding our reasoning. Traditionally, classical logic
has been the logic suggested as the ideal for guiding reasoning (for
example, see Quine [1986], Resnik [1996] or Rumfitt [2015]). For this
reason, classical logic has often been called “the one right
logic”. See Priest [2006a] for a description of how being the best
reasoning-guiding logic could make a logic the one right logic. 

That classical logic has been given as the answer to which logic ought
to guide reasoning is not unexpected. It has rules which are more or
less intuitive, and is surprisingly simple for how strong it is. Plus,
it is both sound and complete, which is an added bonus. There are some
issues, though. As indicated in Section 5, there are certain
expressive limitations to classical logic. Thus, much literature has
been written challenging this status quo. This literature in general
stems from three positions. The first is that classical logic is not
reason-guiding because some other single logic is. Examples of this
type of argument can be found in Brouwer [1949], Heyting [1956] and
Dummett [2000] who argue that intuitionistic logic is correct, and
Anderson and Belnap [1975], who argue relevance logic is correct,
among many others. Further, some people propose that an extension of
classical logic which can express the notion of “denumerably infinite”
(see Shapiro [1991]). The second objection to the claim that classical
logic is the one right logic comes from a different perspective:
logical pluralists claim that classical logic is not the (single) one
right logic, because more than one logic is right. See Beall and
Restall [2006] and Shapiro [2014] for examples of this type of view
(see also the entry on logical
pluralism). Finally, the last objection to the claim that
classical logic is the one right logic is that logic(s) is not
reasoning-guiding, and so there is no one right logic. 

Suffice it to say that, though classical logic has traditionally been
thought of as “the one right logic”, this is not accepted by
everyone. An interesting feature of these debates, though, is that
they demonstrate clearly the strengths and weaknesses of various
logics (including classical logic) when it comes to capturing
reasoning.
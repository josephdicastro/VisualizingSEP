Hermeneutics as the methodology of interpretation is concerned with
problems that arise when dealing with meaningful human actions and the
products of such actions, most importantly texts. As a methodological
discipline, it offers a toolbox for efficiently treating problems of
the interpretation of human actions, texts and other meaningful
material. Hermeneutics looks back at a long tradition as the set of
problems it addresses have been prevalent in human life, and have
repeatedly and consistently called for consideration: interpretation
is a ubiquitous activity, unfolding whenever humans aspire to grasp
whatever interpretanda they deem significant. Due to its long
history, it is only natural that both its problems, and the tools
designed to help solve them, have shifted considerably over time,
along with the discipline of hermeneutics itself. The article focuses
on the main problem areas and presents some proposals that have been put
forward for tackling them effectively.
 
1. Introduction

There has been a highly developed practice of interpretation in Greek
antiquity, aiming at diverse interpretanda like oracles, dreams,
myths, philosophical and poetical works, but also laws and contracts.
The beginning of ancient hermeneutics as a more systematic activity
goes back to the exegesis of the Homeric epics. The most remarkable
characteristic of ancient exegesis was allegorisis
(allegoría, from alla agoreuein, i.e., saying
something different). This was a method of nonliteral interpretation
of the authoritative texts which contained claims and statements that
seemed theologically and morally inappropriate or false (Tate 1934).
Such exegetical attempts were aiming at a deeper sense, hidden under
the surface—hypónoia, i.e., underlying meaning.
Allegorisis was practiced widely from the sixth century BCE to the
Stoic and Neoplatonistic schools and even later (Scholz 2016: 18ff).
In the Middle Ages the most remarkable characteristic of the
interpretative praxis was the so-called accessus ad auctores;
this was a standardized introduction that preceded the editions and
commentaries of (classical) authors. There were many versions of the
accessus, but one of the more widely used was the following
typology of seven questions (Detel 2011: 84f.):

Who (is the author) (quis/persona)?
What (is the subject matter of the text)
(quid/materia)?
Why (was the text written) (cur/causa)?
How (was the text composed) (quomodo/modus)?
When (was the text written or published)
(quando/tempus)?
Where (was the text written or published)
(ubi/loco)?
By which means (was the text written or published) (quibus
faculatibus/facultas)?


Johann Conrad Dannhauer was the first to present a systematic textbook
on general hermeneutics (Jaeger 1974), the Idea boni interpretis
et malitiosi calumniatoris (1630) introducing the Latin neologism
hermeneutica as the title of a general modus
sciendi. The intention of this work was to supplement the
Aristotelian Organon and its subject matter to distinguish
between the true and false meaning of any text (verum sensum a
falso discernere). It is explicitly general in scope, relevant
for all scientific domains (una generalis omnibus scientiis
communis) and applicable to the oral discourse and texts of all
authors (in omnibus auctorum scriptis et orationibus). A
series of authors followed the lead of Dannhauer who established the
systematic locus of hermeneutics within logic (Schönert and
Vollhardt 2005). Most remarkable is the work of Johann Clauberg
(1654), who introduced sophisticated distinctions between the rules of
interpretation with respect to their generality and clarified the
capturing of the intention of the author as a valuable aim of
interpretative praxis. Thus, a general hermeneutics had existed at
least two centuries before Schleiermacher offered his own conception
at the beginning of the 19th century—so his claim
that such a discipline did not already exist before him is simply
false (Schönert and Vollhardt 2005: 9; Detel 2011: 119ff., Scholz
2016: 68ff.)

The scope of the more recent discussions on interpretation has become
broader, often starting with the question whether human actions are to
be viewed as physical phenomena or not and how they should be treated.
Naturalists since Mill (1843/1974, Book VI), have contended that
actions have to be viewed as phenomena on a continuum with other
phenomena in nature, and that they should be studied accordingly.
Issues of interpretation hardly emerge if one adopts such a view.
Interpretivists like Dilthey (1883/1990; 1924/1990;1927/1992), on the
contrary, have argued forcefully that human actions cannot be viewed
as natural phenomena since their meaningfulness makes them
categorically distinct. Unstructured bodily movements, i.e., purely
physiological reactions, are not constitutive of a human
action—there is a consensus on that. The disagreement concerns
the issue as to whether it is constitutive for a human action to have
meaning or not (Mantzavinos 2012). If one adopts the interpretivist
view, then issues of interpretation necessarily arise in the space of
the mental. Human actions are meaningful, and the outcomes of these
actions constitute meaningful material which calls for
interpretation.

It is important to distinguish carefully between two levels of
analysis, the ontological and the epistemological. Heidegger has
proposed a hermeneutic phenomenology as a Hermeneutik der
Faktizität (1923/1995) that should replace traditional
ontology: its centerpiece being an existential analytic of
Dasein, i.e., human existence (1927/1993). The meaning of
Being should be disclosed as a result of analyzing the unique features
of Dasein, and Auslegung (interpretation) is
proposed as a concrete way of being in the world. Gadamer (1960/1990;
1986/1993; 2000) partly adopted this view of ontology, so that the
so-called philosophical hermeneutics emerged as a
philosophical program largely based on the work of these two
protagonists (Malpas and Gander 2014). Although epistemological
studies on hermeneutics can, they need not share these or any
other commitments with respect to ontology. Epistemological
approaches, either descriptive or normative, can start with
problems of interpretation and propose solutions to the problems
independently of the ontological constitution and structure that
underlies each problem area.

Even when the distinction between the ontological and epistemological
level is largely acknowledged, it has been a matter of dispute whether
it is indeed fruitful to completely neglect the constitution and
structure of the material that one deals with, when one is engaged in
the activity of interpretation. In fact, the age-old
“Verstehen vs. Erklären” debate is
largely about this question: whether there is a distinct method for
the apprehension of meaningful material, employable in the social
sciences and the humanities (Geisteswissenschaften;
Kulturwissenschaften), which deal with such material, i.e.,
Verstehen (understanding), or whether the general method
employed in the natural sciences is successfully employable in the
social sciences and humanities as well, i.e., Erklären
(explanation). Methodological dualists like Dilthey famously
pleaded for the autonomy of the social sciences and humanities which
must follow the method of Verstehen. The neo-Kantian
philosophers Wilhelm Windelband and Heinrich Rickert focused on the
methods of concept formation and judgment in the different groups of
sciences, the Kulturwissenschaften and the natural sciences.
For Windelband (1894) the logic of the Kulturwissenschafen is
characterized by an idiographic interest in singular
judgments about the past opposed to the natural sciences’
nomothetic interest in formulating laws. For Rickert (1929)
the Kulturwissenschaften are characterized by an
individualizing form of concept formation which solved the problem of
how the general concepts essential to any scientific representation
could capture an individual object, without simply subsuming it under
a general law in the fashion of natural scientific concept
formation.

By contrast to this dualistic approach, methodological monists like
Mill reject the dichotomy and plead for a single method applicable to
all sciences, convinced as he is that discovering and establishing
lawlike hypotheses is also possible in the social sciences and
humanities. At the heart of this controversy (Ricoeur 1981; L.
Anderson 2003) lies a question about the acceptance of what can be
called “the method-object-argument”, i.e., the position
that the scientific method has to be suited to its object. If the
object of the scientific analysis demonstrates a certain ontological
constitution and structure, then we must use a method that is suitable
for dealing with that constitution and structure. The argument
postulates the primacy of the object of inquiry over the method of
inquiry, and depending on one’s view regarding the acceptability
of the argument, one normally adopts either Verstehen or
Erklären, although other ingenious attempts like the
possibility of a “verstehendes Erklären” (an
understanding explanation) has been proposed by Max Weber
(1922/1985).

In any case, the ontological and epistemological levels are not
consistently segregated in the discussion. This is notably the case
with respect to the hermeneutic circle which serves as the dominant
argument for all those who raise a claim to the autonomy of the
humanities, and to which we turn now.
2. The Hermeneutic Circle

The hermeneutic circle is a prominent and recurring theme in the
discussion ever since the philologist Friedrich Ast (1808: 178), who
was probably the first to do so, drew attention to the circularity of
interpretation: “The foundational law of all understanding and
knowledge”, he claimed, is “to find the spirit of the
whole through the individual, and through the whole to grasp the
individual”. Friedrich Schleiermacher in a lecture of 1829
adopts as a principle the notion 


that the same way that the whole is, of course, understood in
reference to the individual, so too, the individual can only be
understood in reference to the whole. (1999: 329ff.) 


Emilio Betti (1962: 15ff.) designates the principle as
“Grundsatz der Ganzheit” and Charles Taylor
(1985: 18) states: 


This is one way of trying to express what has been called the
“hermeneutical circle”. What we are trying to establish is
a certain reading of text or expressions, and what we appeal to as our
grounds for this reading can only be other readings. The circle can
also be put in terms of part-whole relations: we are trying to
establish a reading for the whole text, and for this we appeal to
readings of its partial expressions; and yet because we are dealing
with meaning, with making sense, where expressions only make sense or
not in relation to others, the readings of partial expressions depend
on those of others, and ultimately of the whole.


Many philosophers follow the lead of Heidegger who conceptualizes the
hermeneutical circle as an ontological issue (1927/1962: 195): 


The “circle” in understanding belongs to the structure of
meaning, and the latter phenomenon is rooted in the existential
constitution of Dasein—that is, in the understanding
which interprets. An entity for which, as Being-in-the-world, its
Being is itself an issue, has, ontologically, a circular structure.



This conceptualization has been severely criticized as a fruitless
attempt to immunize his conception from criticism by deliberately
sheltering it under a mantle of apriorism (Albert 1994: 19).

Others view the hermeneutic circle as a logical or methodological
problem. To begin with, it is clear that the hermeneutic circle is not
a logical problem in a strict sense: it is neither concerned with
circular argumentation in a deduction arising from proving
something by using the statement that one was supposed to prove nor
with a circular definition, arising from the concept to be
defined already having been used in the text. Stegmüller
(1979/1988) contends that the hermeneutic circle constitutes a dilemma
of a methodological nature or, more particularly, one of six specific
forms of dilemmas depending on what exactly is meant when one speaks
of a “hermeneutic circle”. He maintains that, in its most
important variations, the circle is by no means a narrow
epistemological problem of the humanities, but a problem to be
confronted in all disciplines. This is the case, for example, in what
is known as the dilemma regarding the appropriate distinction between
background knowledge and facts. Using examples from astronomy and
literature, Stegmüller shows that similar difficulties arise for
both when testing hypotheses concerning the differentiation between
facts and background knowledge. Testing of a hypothesis requires a
clear separation between hypothetical components in the observational
data, on the one hand, and the theoretical background knowledge, on
the other—a problem that by no means arises only in the
humanities and characterizes according to Stegmüller the natural
sciences as well. It can only be solved if, through critical
discussion the members of the relevant community of inquirers agree on
what should count as fact and what as background knowledge in respect
to the specific hypothesis tested. Føllesdal, Walløe and
Elster (1996: 116ff.) also hold that the hermeneutic circle is a
methodological problem. They discuss a series of methodological issues
that arise during the processes of understanding, and claim that they
all appear in the context of the justification of an interpretation.
They distinguish four variations: the whole-and-part circle, the
subject-object circle, the Hypothetico-Deductive-Method circle and the
question-answer circle.

Instead of viewing the hermeneutic circle as a methodological problem
that emerges when testing an interpretative hypothesis, one
can take it that the problem of the relationship between the
meaningful whole and its elements emerges in the process of
formulating a hypothesis. In this case, the hermeneutic
circle is an empirical phenomenon that arises when one does not manage
to understand a linguistic expression (or other signs) immediately,
i.e., more or less automatically (Mantzavinos 2009). It is then
necessary to create interpretative hypotheses, and it is during this
activity that one gets confronted with the problem of the meaningful
whole and its elements. Language processing is a complex skill which
has become routinized once one has gained experience in all levels
which are important when understanding expressions: the phonologic,
the semantic, the syntactic and the pragmatic. Over the course of
time, sounds, words, sentences, and entire texts are automatically
classified in one’s cognitive system (Nehamas 1987: 275f.) and
therefore language processing takes place largely unconsciously under
standard conditions. If a difficulty arises in the language
comprehension process, and if one cannot understand one or more
linguistic expressions immediately, then cognitive resources in the
form of attention are activated, and an interpretative hypothesis is
generated. In psycholinguistics this conscious process is often
modeled as an interactive process of all relevant levels of
information processing: the phonologic, the semantic, the syntactic,
and the pragmatic. There is enough evidence that supports the claim
that the discourse on the hermeneutic circle can be appropriately
viewed as the search process that is activated if the interpreter of a
linguistic expression does not understand something immediately (J.
Anderson 2005: ch. 12; Danks, Bohn, and Fears 1983; Simon 1986). The
process of parsing during which the words in a linguistic
expression are transformed into a mental representation with the
combined meaning of the words, as studied by cognitive scientists, is
especially relevant: during this procedure the meaning of a sentence
is processed phrase-by-phrase and people tend to integrate both
semantic and syntactic cues, in order to achieve an incremental
understanding of a statement or a text (Pinker 1994).
3. Text Interpretation

It is prima facie plausible to postulate that there is
nothing beyond understanding a text, than understanding the
sentences which compose it; and that there is nothing beyond
understanding a sentence than understanding the words which
compose it. This widespread view is based on the belief in the
validity of the principle of compositionality (Szabo
2013): the meaning of a
complex expression is supposed to be fully determined by its structure
and the meanings of its constituents. Gottlob Frege has famously
declared in section 60 of his Grundlagen der Arithmetik
(1884) that only within complete sentences do words have meaning. This
different, but related principle to the principle of compositionality
is usually referred to as the context principle. He writes:



Es genügt, wenn der Satz als Ganzes einen Sinn hat; dadurch
erhalten auch seine Theile ihren Inhalt.

(It is enough if the sentence as whole has meaning; thereby also its
parts obtain their meanings.) 


There is a consensus in many contemporary theories that the semantic
value of a sentence is a function of the semantic value of its
constituents, insofar the principle of compositionality is applicable.
However, the temptation to assume an analogous principle for texts
should be resisted: the semantic value of a text is not a
function of the semantic value of its constituents and its structure.
Whereas a sentence may express a thought which is a plausible mental
correlate, a text expresses a sequence of thoughts which
cannot be grasped directly: the meaning of a sentence can be grasped,
memorized and processed; the meaning of a text as a whole on the
macro-level requires for its comprehension a more complex cognitive
process (Scholz 2012).

Acknowledging the complexity of text comprehension as a
process is the first step towards looking for models that can
successfully come to grips with that complexity. Such models have been
proposed and discussed in cognitive psychology. A prominent example of
such a model has been put forward by Kintsch and van Deijk (1978) and
focuses on the information processing taking place once syntactic and
semantic analysis have been undertaken. In other words, the focus of
the model is directly on the comprehension of the whole text, after
the initial set of propositions have been identified and after
parsing processes have been applied to them. A crucial factor is
the capacity limit of the cognitive system, namely the number of
propositions that can be kept active in working memory. The
consequence of this is that sets of propositions are cognitively
processed in cycles, i.e., the first n1 propositions
are processed together in one cycle, then the next
n2 propositions and so on. Thus, it becomes
necessary to use criteria of relevance according to which propositions
are kept active, so that the meaning of the entire text can be
conveyed. The suggested criteria are temporal proximity and the
importance of the information conveyed. In accordance with what is
called “leading-edge strategy”, subjects keep active the
proposition that has most recently been processed and the propositions
that, in the hierarchical representation of the text, have priority
over the rest. This is done under the presupposition that there is a
hierarchical relationship between the propositions in the text. In a
parallel process of elaboration “bridge inferences” are
made in which the interpreter adds inferences in order to associate
otherwise unrelated terms, and “macro-propositions” are
established that contain a summary of the gist of the text. During
this complex process, the interpreter actively construes the meaning
of the whole text and grasps its meaning (Kintsch 1998).

Such models of text comprehension are empirically tested and amount to
a significant step forward towards the formulation of an account of
text interpretation based on solid empirical evidence. However, a
standard philosophical critique questions the possibility of providing
testable models of text comprehension without appropriately
acknowledging the normative presuppositions underlying all
interpretative praxis. There are two lines of argument that have been
influential in this context. The first has been propagated most
emphatically in the Anglo-Saxon philosophical discussion of the second
half of the twentieth century with respect to what is known as
“radical interpretation”. In an imaginary situation, an
interpreter is confronted with the (verbal) behavior of a human being,
in an entirely alien culture, without any kind of knowledge about his
or her beliefs, desires or the meanings of what he or she expresses.
The problem consists of getting to know the beliefs, desires and
meanings of this person starting from scratch, i.e., viewing this
person as a physical system without any help in translation (Lewis
1983: 108). In the context of this largely artificial problem, it is
contended that one is inclined to or bound to adopt a general
interpretative principle of a normative nature, which is supposed to
be imperative for correct (translation and) interpretation. According
to Quine (1960: 59) the assertions of the native 


startlingly false on the face of them are likely to turn on hidden
differences of language. […] The common sense behind the maxim
is that one’s interlocutor’s silliness, beyond a certain
point, is less likely than bad translation. 


Davidson in a similar vein contends that the interpretation is bounded
by a “principle of charity” (1984: 27): 


Charity in interpreting the words and thoughts of others is
unavoidable in another direction as well: just as we must maximize
agreement, or risk not making sense of what the alien is talking
about, so we must maximize self-consistency we attribute to him, on
pain of not understanding him. 


Grandy (1973: 443) views the “principle of humanity” as a
guide: the requirement that the pattern of relations among beliefs,
desires and the world ascribed to the author be as similar to our own
patterns as possible.

In fact, none of the principles proposed in this discussion is new. As
early as 1654 Johannes Clauberg has worked out in admirable detail
principles of “in bonam partem interpretari” in
Chapter XIII of the third part of his Logica, Vetus &
Nova, the principle of
charity—“benignitas”—being the most
important one. And 1757 Georg Friedrich Meier proposed the principle
of hermeneutic equity as the most general principle of all
interpretive rules of a hermeneutica universalis (Meier
1757/1996: §39):



Hermeneutic equity (aequitas hermeneutica) is the tendency of
the interpreter to hold that meaning for hermeneutically true that
best comports with the flawlessness of the originator of the sign,
until the opposite is shown. 


It is important to stress that the principle of hermeneutic equity is
explicitly formulated as a presumption: a rule which can fail to stand
up to evidence. In the Anglo-Saxon discussion on radical
interpretation referred to above, the general thrust of the argument
is that these rules are constitutive for the practice of
interpretation; they occupy a specific status that must accordingly be
recognized as an important presupposition of all interpretation.
However, their apparent indispensability can simply be traced to the
fact that they have been particularly well corroborated, as they have
often been employed with success. Accordingly, it is only their
greater corroboration that leads to a presumption that they are
indispensable to every interpretation (Mantzavinos 2005: 134).

The second line of argument regarding the normative presuppositions of
interpretative praxis, centers around the indispensability of a
rationality assumption in all interpretation (Livingston 1993).
According to this argument, it is possible to apprehend linguistic
expressions only if it is assumed that speakers or authors manifest
complex features that are appropriately conceptualized as rational.
Most importantly, deductive rationality plays an important role: it is
assumed that in bringing about linguistic expressions, the rules of
inference of propositional and predicate logic must be
respected. Only in this case is the appropriation of the meaning of
texts and linguistic expressions in general possible (Føllesdal
1982: 311). So, according to this view, rationality is
constitutive of the beliefs of the author which give rise to
his or her linguistic expressions and, thus, rationality is a (or the)
normative presupposition which must underlie all interpretative
praxis. However, the rationality assumption is surely not an
uncontested principle (Mantzavinos 2001: ch. 4), and many questions
regarding whether rationality is indeed constitutive and how much
rationality is necessary if (successful) interpretation is to take
place remain (Scholz 2016: 228ff.).

Thus, the process of text interpretation which lies in the center of
hermeneutics as the methodological discipline dealing with
interpretation can and has been analyzed empirically with the help of
testable models. The question whether there are certain normative
presuppositions of the interpretative praxis—like specific
principles of interpretation that are constitutive of this praxis and
indispensable rationality principles—is a focal issue of obvious
philosophical importance (Detel 2014). Regardless of the position that
is assumed with respect to this issue, it is hardly possible to deny
that the interpretative praxis can take on multiple forms and can take
place according to diverse aims, an issue to which we turn next.
4. Aims of Text Interpretation

We have seen that text interpretation goes beyond the interpretation
of simple or complex sentences since it crucially includes a number of
inferences that are necessary in order to glean the meaning of a text.
Text interpretation as a goal-directed activity can assume different
forms, but must be distinguished from highlighting the
significance of a text. In fact, a series of serious
misunderstandings and confusions can be easily avoided, if a clear
distinction is made between interpretation as an activity
directed at the appropriation of the meaning of a text and
textual criticism as an activity that is concerned with the
significance of a text with respect to different values. As
Hirsch (1967: 7f.) has correctly pointed out:


Probably the most extreme examples of this phenomenon are cases of
authorial self-repudiation, such as Arnold’s public attack on
his masterpiece, Empedocles on Etna, or Schelling’s
rejection of all the philosophy he had written before 1809. In these
cases there cannot be the slightest doubt that the author’s
later response to his work was quite different from his original
response. Instead of seeming beautiful, profound, or brilliant, the
work seemed misguided, trivial, and false, and its meaning was no
longer one that the author wished to convey. However, these examples
do not show that the meaning of the work had changed, but precisely
the opposite. If the work’s meaning had changed (instead of the
author himself and his attitudes), then the author would not have
needed to repudiate his meaning and could have spared himself the
discomfort of a public recantation. No doubt the significance
of the work to the author had changed a great deal, but its meaning
had not changed at all.

[…] Meaning is that which is represented by a text; it
is what the author meant by his use of a particular sign sequence; it
is what the signs represent. Significance, on the other hand,
names a relationship between that meaning and a person, or a
conception, or a situation, or indeed anything imaginable. […]
Significance always implies a relationship, and one constant,
unchanging pole of that relationship is what the text means. Failure
to consider this simple and essential distinction has been the source
of enormous confusion in hermeneutic theory.


Even if one acknowledges the difference between meaning and
significance, and decides to honor the distinction between text
interpretation and textual criticism, it is undisputable that
interpretation can be directed at many different goals. For a long
time the discussion has centered around the appropriate objective of
interpretation and a focal point has been the so-called intentional
fallacy, influentially formulated by Wimsatt and Beardsley (1946:
468), which states that “the design or intention of the author
is neither available nor desirable as a standard for judging the
success of literary work of art”. The crux of the matter in the
debate has been whether grasping the intention of the author of a text
is the only aim of interpretation or not and assuming that authorial
intention is indeed the goal of interpretation, how exactly it can be
tracked. The essential question with which we are confronted in
studying any given text, as Quentin Skinner (1969: 48f.) influentially
argued, is 


what its author, in writing at the time he did write for the audience
he intended to address, could in practice have been intending to
communicate by the utterance of this given utterance. It follows that
the essential aim, in any attempt to understand the utterances
themselves, must be to recover this complex intention on the part of
the author. And it follows from this that the appropriate methodology
for the history of ideas must be concerned, first of all, to delineate
the whole range of communications which could have been conventionally
performed on the given occasion by the utterance of the given
utterance, and, next, to trace the relations between the given
utterance and this wider linguistic context as a means of
decoding the actual intention of the given writer. 


Besides Quentin Skinner (1972, 1975), Axel Bühler, among others,
has contended that it is possible to identify the author’s
intentions, as long as the sources and the transmission of the text
allows this (1999a: 62ff.); and that it is even possible to specify
the communicative intention of the author in fictional texts, in
highlighting how the author moves those he or she is addressing to
“act as if” the contents of fictional speech were real
(1999a: 66ff.). This position, broadly known as Hermeneutic
Intentionalism (Bühler 1993, 1999b, 2003; see also 2010, in
Other Internet Resources), provides arguments designed to show that
capturing the intention of the author is perfectly desirable and fully
accessible as an aim of interpretation and that the intentional
fallacy is not a fallacy at all.

Whereas the notion of intention is certainly useful in providing a
methodological account of interpretation, its use is surely part of a
later development; and it has been largely imported into hermeneutic
methodology from discussions in philosophy of mind and language that
took place in the analytic tradition in the 20th century.
It was itself a reaction against two orthodoxies prevailing at the
time. On the one hand, that interpretation should aim only at the
concrete text itself; and on the other, that interpretation
should aim at the social context which gave rise (or caused)
the creation of the concrete text (Skinner 1969).

The term “nexus of meaning” (Sinnzusammenhang)
used by Dilthey and others in the tradition of classical hermeneutics
is, however, more appropriate as a terminus technicus than
the notion of intention. A nexus of meaning, connected with a specific
linguistic expression or a specific text, is construed by the author
against the background of his goals, beliefs, and other mental states
while interacting with his natural and social environment: such a
construal of meaning is a complex process and involves both the
conscious and unconscious use of symbols. Text interpretation can be
conceptualized as the activity directed at correctly identifying the
meaning of a text by virtue of accurately reconstructing the nexus
of meaning that has arisen in connection with that text. One way
to describe the nexus of meaning is by using the notion of
intention—a legitimate but surely not an exclusive way. It may
well be that the specification of the author’s intention is
adequate for the description of the nexus of meaning but the
reconstruction of the nexus of meaning can also be more complex than
that. In other words, in reconstructing the nexus of meaning, it is
not necessary to comply with a specific descriptive system: the
process of reconstruction need not be committed to the use of the
concept of intention. Since what is to be reconstructed is a whole
nexus of meaning, a completely different descriptive system can be
used. It is possible to use the intention of the author as well
as to incorporate an analysis of the grammatical elements and other
elements in order to produce an adequate reconstruction.

The notion of the nexus of meaning is central for the methodology of
hermeneutics, mainly because it can accommodate the hermeneutic
practices of a series of disciplines. Coseriu (1994/2006) in his
influential Textlinguistik used the notion of
“Umfeld” in order to delineate the same
phenomenon that the notion of the nexus of meaning does. The
reconstruction of the “Umfeld”—in the
tradition of the Organon Model of Karl Bühler who spoke
of “sympraktischem, symphysischem and
synsemantischem Umfeld” (1934/1965: 154ff.)—aims
at the appropriation of the meaning of a text by virtue of describing
its whole context, as far as possible. It is obvious, then, that
interpretation in the hermeneutic tradition is conceptualized as a
process of reconstructing nexuses of meaning and represents a process
diametrically opposite to the process of deconstruction as proposed
for example by Derrida and his followers. As Rescher (1997: 201)
points out: 


The crucial point, then, is that any text has an envisioning
historical and cultural context and that the context of a
text is itself not simply textual—not something that can be
played out solely and wholly in the textual domain. This context of
the texts that concern us constrains and limits the viable
interpretations that these texts are able to bear. The process of
deconstruction—of interpretatively dissolving any and
every text into a plurality of supposedly merit-equivalent
construction—can and should be offset by the process of
reconstruction which calls for viewing texts within their
larger contexts. After all, texts inevitably have a
setting—historical, cultural, authorial—on which their
actual meaning is critically dependent.


Viewing interpretation as a process of reconstructing the nexus of
meaning of a text does pay due attention to the context of the text,
without assuming that the social and historical context had caused
the production of the text. This view also enables the
reconciliation in a different facet of the age-old controversy
regarding the aims of interpretation. We have seen that it has long
been an object of fierce dispute whether capturing the intention of
the author is the only legitimate objective of an interpretation or
not. However, this dispute can be successfully arbitrated if one bears
in mind the character of hermeneutics as a technological discipline
(Albert 2003). Its technological character manifests itself in
positively acknowledging the plurality of aims towards which
interpretative activities can aim. These objectives need not
necessarily be reduced to a common denominator nor do some of them
need to be sacrificed for the sake of others. A critical discussion of
the significance of the different aims of interpretation is, of
course, possible, but it need not end up with definite results that
are binding for everyone. In fact, this will hardly ever be the case,
since consensus on appropriate aims of interpretation will typically
be of a provisional character: it is sufficient to provisionally
accept a series of objectives that have emerged in the discussion and
then formulate and test alternative hypotheses in relation to every
one of them. In other words, one only needs to accept an aim of
interpretation hypothetically, and then inquire into the ways that it
can be accomplished. Such a technology operates with
hypothetical rather than categorical imperatives.
Stated differently, the standards for the comparative evaluation of
interpretative hypotheses can be oriented towards various regulative
ideals. For example, the reconstruction of the nexus of meaning of a
text can take place in relation to the idea of accuracy:
interpretative activities would then aim at accurately depicting the
nexus of meaning of a text. But such a reconstruction of the nexus of
meaning could also take place with respect to other objectives, for
example aesthetic ones, like beauty. Whether accuracy or beauty should
be a legitimate aim of interpretation with respect to a specific text,
for example, is a discourse which can take place at another level and
need not be concluded via a dogmatic decision once and for all. In
opposition to “authorial intention” the “nexus of
meaning” is a complex phenomenon and the interpreters can opt to
highlight and apprehend it with respect to different aims and
standards—indeed this is very often the case. What lies at the
heart of this epistemic activity, i.e., of inventing interpretations
as reconstructions of nexuses of meaning with respect to different
aims, and how it can be best methodically captured is the subject of
the following section.
5. The Hypothetico-Deductive Method

The application of the hypothetico-deductive method in the case of
meaningful material has been proposed as a plausible way to account
for the epistemic activity of text interpretation (Føllesdal
1979; Tepe 2007). Hypothetico-deductivism has been originally debated
in connection with the philosophical theory of scientific explanation
and it has indeed been the case that the main protagonists, Hempel and
Popper (Popper1959/2003; 1963/1989), have portrayed scientific
activity as exclusively an explanatory activity—largely aiming
at answering “why?”–questions. This influential and,
very often, only implicitly shared view that all scientific activity
is explanatory need not be followed, however. Moreover, answers to
“what was the case?”–questions rather than only to
“why?”–questions can be allowed to enter the field
of science, appropriately accommodating the activities of all those
whose daily work consists in text interpretation. The application of
the hypothetico-deductive method is a way to show that the standards
currently used when dealing with problems of
explanation—intersubjective intelligibility, testability with
the use of evidence, rational argumentation and objectivity—can
also apply to problems of interpretation. It will be very briefly
shown how this method can be applied in five steps (Mantzavinos
2014).

In order to reconstruct the nexus of meaning which is connected with a
specific text, interpretative hypotheses need to be established as a
first step. The system of propositions that constitutes these
interpretative hypotheses is in principle hypothetical, because it is
not certain whether it will accomplish its epistemic aim, i.e., the
identification of the meaning of the text. In the construction of such
hypotheses, diverse hermeneutic principles can be employed like the
already discussed “principle of charity” or
“principle of humanity”, as presumptive rules that can
break down in the light of experience. These interpretative hypotheses
can partly consist of not directly observable “theoretical
terms”, which could, for example, refer to the intentions of the
author. In such cases one can, in a second step, deduce from
such interpretative hypotheses, in conjunction with other statements,
consequences which could be more observable, that is, consequences
that could be (more easily) testable. In a third step, these
observable consequences can be tested with the help of evidence
primarily provided with the help of research techniques from the
social sciences and humanities. The evidence can include what the
author claims about his or her own work, his or her other works,
details of rhyme, rhythm, frequency of occurrence of words, other
linguistic or biographical considerations (Nehamas 1981: 145) and so
on. In a fourth step, the different interpretative hypotheses
are checked against the evidence. A comparative evaluation is
necessary here, in order to distinguish good from bad interpretations.
Such an evaluation can take place with respect to different values, so
that a reconstruction of a nexus of meaning of a text can be oriented
towards diverse ideals. One such ideal can be truth,
which can be conceptualized as the accurate depiction of the
nexus of meaning, and interpretations are hypotheses precisely by
virtue of the fact that one searches for reasons for their truth and
falsity. Other values, for example aesthetic ones, can also be deemed
important, and the comparative evaluation of the offered
interpretations can also take place with respect to such
values—for example, beauty. In the fifth step of the
application of the hypothetico-deductive method, a multi-dimensional
evaluation of the same interpretative hypothesis with respect to
different values or of a set of hypotheses with respect to one value
is possible. Such evaluations do not take place according to any kind
of algorithmic procedures. The employment of specific calculi which
can supposedly lead to determinate evaluations and choices is not
possible either in textual interpretation or, indeed, in scientific
explanation. Human choices involving imagination are at work in this
kind of cognitive praxis, choices that are bound to be fallible. It is
only the institutionalization of the possibility of criticism that can
lead to the correction of errors when these evaluations and choices
are involved. Our fallible judgments are all what we have
here as elsewhere and enabling a critical discussion is the
prerequisite of making informed choices.

It is important to stress that the fifth step of this method has the
important consequence of impeding a serious problem which has been
exhaustively elaborated in the theory of confirmation. If contingent
evidence E confirms hypothesis H given background
beliefs B, then E also confirms the conjunction H
∧ X for any arbitrary X consistent with H.
This peculiarity can render the confirmation process extremely
permissive and so the whole method useless. A substantial critical
discussion conducted by arguments among the various interpreters of a
text is therefore a conditio sine qua non for the
fruitfulness of the hypothetico-deductive method. Scholz (2015) has in
fact questioned the productivity of this method precisely on these
grounds—he calls this “the relevance problem”—, and
has suggested that it be solved by employing an inference to the best
explanation (Lipton 2004). According to this alternative, the
hypothesis that best explains the evidence should be accepted from
among the various hypotheses proposed. However, this alternative move
is problematic since it is based on the assumption that it is possible
to provide necessary and sufficient conditions of what constitutes an
explanation and that there is a universal agreement on what counts as
“the best explanation”—both assumptions being in
fact untenable (Mantzavinos 2013, 2016).

In conclusion, the hypothetico-deductive method can help establish
hermeneutic objectivity, ultimately based on a critical discussion
among the participants to the discourse on the appropriateness of
different interpretations regarding the fulfillment of the diverse
aims of interpretation. Intersubjective intelligibility, testability
with the use of evidence, rational argumentation and objectivity are,
thus, feasible also in the case of text interpretation. A series of
examples from diverse disciplines demonstrate this (Føllesdal
1979; Mantzavinos 2005: ch. 6; Detel 2011: 394ff; Detel 2016).
6. Epilogue

Hermeneutics as the methodology of interpretation can provide guidance
for solving problems of interpretation of human actions, texts and
other meaningful material by offering a toolbox based on solid
empirical evidence. Throughout its historical development hermeneutics
has dealt with specific problems of interpretation, arising
within specific disciplines like jurisprudence, theology and
literature, which have not been the focus of this article. The aim was
indeed to show what kind of general problems of
interpretation are treated by the discipline of hermeneutics and to
identify some important procedures leading to their efficacious
solution—always keeping in mind that these procedures, like all
epistemological procedures, are bound to remain fallible.
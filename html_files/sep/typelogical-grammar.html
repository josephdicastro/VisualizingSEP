<!DOCTYPE html><!--[if lt IE 7]> <html class="ie6 ie"> <![endif]--><!--[if IE 7]>    <html class="ie7 ie"> <![endif]--><!--[if IE 8]>    <html class="ie8 ie"> <![endif]--><!--[if IE 9]>    <html class="ie9 ie"> <![endif]--><!--[if !IE]> --><html xmlns="http://www.w3.org/1999/xhtml"><!-- <![endif]--><head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Typelogical Grammar (Stanford Encyclopedia of Philosophy/Winter 2019 Edition)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Typelogical Grammar" />
<meta property="citation_author" content="Moortgat, Michael" />
<meta property="citation_publication_date" content="2010/09/07" />
<meta name="DC.title" content="Typelogical Grammar" />
<meta name="DC.creator" content="Moortgat, Michael" />
<meta name="DCTERMS.issued" content="2010-09-07" />
<meta name="DCTERMS.modified" content="2010-09-07" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="archive article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP logo" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy Archive<div id="site-subtitle">Winter 2019 Edition</div></a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu collapse">
              <ul class="nav">
                <li class="dropdown"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">New in this Archive</a></li>
                    
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../../../archives/">Archives <i class="icon-external-link"></i></a></li>
                  </ul>
                </li>
                <li class="dropdown"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    
                    <li><a href="../../../../contact.html">Contact <i class="icon-external-link"></i></a></li>
                  </ul>
                </li>
                <li class="dropdown"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../../../support/">Support the SEP</a></li>
                    <li><a href="../../../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="../../../../search/searcher.py">
        <input type="search" name="query" placeholder="Search this archive" />
<input type="hidden" name="archive" value="win2019" />

        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky" style="z-index: 999;">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar collapsed" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/typelogical-grammar/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=typelogical-grammar&amp;archive=win2019">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div><div></div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Typelogical Grammar</h1><div id="pubinfo"><em>First published Tue Sep 7, 2010</em></div>

<div id="preamble">

<p>

Typelogical grammars are substructural logics, designed for reasoning
about the composition of form and meaning in natural language. At
the core of these grammars are residuated families of type-forming
operations; a hierarchy of typelogical grammars results from the
choices one makes with respect to the structural properties of the
type-forming operations, and the means one introduces to control the
grammatical resource management.  Computational semantics is obtained
from the Curry-Howard interpretation of categorial derivations.
Parsing and natural language processing is modeled in terms of
suitably refined versions of the proof nets of linear logic.</p>

</div>

<div id="toc">
<!--Entry Contents-->
<ul>
<li><a href="#BitHis">1. A bit of history</a></li>
<li><a href="#LamSys">2.   The Lambek systems</a>
   <ul>
   <li><a href="#ModGraCom">2.1   Modelling grammatical composition</a></li>
   <li><a href="#SynSemInt">2.2 The syntax-semantics interface</a></li>
   </ul></li>
<li><a href="#ExtTypGra">3.   Extended typelogical grammars</a>
   <ul>
   <li><a href="#MulSysStrCon">3.1   Multimodal systems, structural control</a></li>
   <li><a href="#LogDis">3.2   The logic of discontinuity</a></li>
   <li><a href="#SymCatGra">3.3   Symmetric categorial grammar</a></li>
   <li><a href="#FleIntConSem">3.4   Flexible interpretation, continuation semantics</a></li>
   </ul></li>
<li><a href="#ProNetPro">4.  Proof nets and processing</a></li>
<li><a href="#RecCapCom">5.   Recognizing capacity, complexity</a></li>
<li><a href="#RelApp">6.   Related approaches</a></li>
<li><a href="#Bib">Bibliography</a></li>
<li><a href="#Aca">Academic Tools</a></li>
<li><a href="#Oth">Other Internet Resources</a></li>
<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->

<hr />

</div>

<div id="main-text">

<h2><a name="BitHis">1. A bit of history</a></h2>

<p>Typelogical grammar has its roots in two seminal papers written by
Jim Lambek half a century ago (Lambek 1958, 1961). In these papers,
the author set himself the aim “to obtain an effective rule (or
algorithm) for distinguishing sentences from non-sentences, which
works not only for the formal languages of interest to the
mathematical logician, but also for natural languages
[…]”. To realize this goal, the familiar parts of speech
(nouns, verbs, …) are turned into formulas of a logic — a
logic designed to reason about grammatical composition.  The judgement
whether a phrase is wellformed, under this view, is the outcome of a
process of deduction.  A decision procedure for such judgements is
obtained by casting the grammar logic in the format of a Gentzen-style
sequent calculus.  The sequent presentation is extremely simple: the
logic of grammar is a logic without structural rules.  Contraction and
Weakening are dropped; their presence would entail that wellformedness
is unaffected by arbitrary copying or deletion of grammatical
material.  To take into account also word order and phrase structure
information, Lambek further removes the structural rules of Exchange
and (in the 1961 paper) Associativity.</p>

<p>At the time of their publication, these ideas did not resonate;
their impact on the field of computational linguistics dates from
the 1980s. Two factors have played an important role in this belated
recognition.  The first was the addition of a computational semantics
for categorial derivations along the lines of the Curry-Howard
proofs-as-programs interpretation in van Benthem 1983, reprinted in
Buszkowski <em>et al</em>. 1988. Lambek's typelogical grammars, so
viewed, could be seen to realize Montague’s compositionality
program in an uncompromising way, with the lambda calculus and type
theory providing powerful tools to study derivational and lexical
semantics. The second factor was the introduction of linear logic
(Girard 1987), and with it, the surge of interest in substructural
logics. A key insight from linear logic injected into typelogical
grammar is the idea that structural rules can be reintroduced in a
controlled form by means of so-called modalities: in moving to more
discriminating type logics, no expressivity is lost. From a
computational point of view, variants of the proof nets of linear
logic have provided the basis for typelogical natural language
processing.</p>

<p>Typelogical grammar, in its current incarnations, keeps the general
architecture of Lambek's original calculi, but extends this to a more
articulate vocabulary of type-forming operations.  Of central
importance are the multiplicative operations, used to model
grammatical composition; these form the focus of this article. Next to
the multiplicatives one can consider additive or Boolean operations
and first or second order quantification to handle phenomena of
lexical ambiguity, type polymorphism and the management of feature
information. Morrill 1994 is a good source of examples for such
extensions.</p>

<p><em>Outline of this article</em>.  We start with the standard
Lambek systems. We study their model-theoretic and proof-theoretic
aspects (frame semantics, sequent calculus), and the relation between
the two (soundness, completeness). We characterize compositional
interpretation as a homomorphism relating a syntactic source calculus
and a target calculus for meaning assembly. The mapping associates
syntactic derivations with semantic readings, expressed as terms of
the simply typed linear lambda calculus.  In §3 we turn to the
expressive limitations of the standard Lambek systems, in syntax and
semantics, and study how they can be addressed by systematically
extending the categorial architecture in a modular way.
Generalizations concern the arity of the type-forming operations
(binary composition operations versus unary control operators);
multimodal extensions where multiple families of type-forming
operations co-exist and communicate via structural interaction
principles; one-sorted vs multi-sorted logics (discontinuous calculi);
single-conclusion vs multiple-conclusion systems (symmetric calculi);
and more structured views of the syntax- semantics interface
(continuation semantics). To close the panoramic tour, we present
proof nets for the various typelogical systems studied in §3, and
we compare these systems with respect to expressivity and
computational tractability.</p>

<h2><a name="LamSys">2.   The Lambek systems</a></h2>

<h3><a name="ModGraCom">2.1   Modelling grammatical composition</a></h3>

<p>The type language to be considered in this section consists of a
small set of atomic types, and is closed under the binary operations
product, left and right division. Some notational conventions: in the
specification below, lower case <em>p</em> ranges over atomic types,
upper case <em>A</em>, <em>B</em> over arbitrary types. We pronounce
<em>A</em>\<em>B</em> as ‘<em>A</em> under <em>B</em>’
and <em>B</em>/<em>A</em> as ‘<em>B</em> over <em>A</em>’
thus to make clear which part of the ‘fraction’ is the
denominator (<em>A</em>), and which is the numerator
(<em>B</em>).  </p>

<ul>
<li>Types: <em>A</em>,<em>B</em> ::= <em>p</em> | <em>A</em> ⊗ <em>B</em>
| <em>A</em>\<em>B</em> | <em>B</em>/<em>A</em></li>
</ul>

<p>In categorizing linguistic expressions, atomic types stand for
phrases that one can think of as grammatically ‘complete’.
Examples, for English, could be <em>s</em> for declarative sentences
(‘Mary dreams’,…), <em>np</em> for noun phrases
(‘Mary’, ‘the girl’,…), <em>n</em> for
common nouns (‘girl’, ‘smart girl’,…).
Division expresses incompleteness: an expression with
type <em>A</em>\<em>B</em> (<em>B</em>/<em>A</em>) will produce a
phrase of type <em>B</em> when it is put together with a phrase of
type <em>A</em> to its left (right).  Product types <em>A</em>
⊗ <em>B</em> explicitly express the formation of a complex
phrase out of constituent parts of type <em>A</em> and <em>B</em> in
that order.  So if ‘Mary’ is typed as <em>np</em> and the
verb ‘dreams’ as <em>np</em>\<em>s</em>, we
obtain <em>s</em> for the combination ‘Mary dreams’ by
multiplying out these types: <em>np</em> ⊗
(<em>np</em>\<em>s</em>). Similarly for the combination ‘the
girl dreams’, where one would start from a
type <em>np</em>/<em>n</em> for the determiner and <em>n</em> for
‘girl’: ((<em>np</em>/<em>n</em>) ⊗ <em>n</em>)
⊗ (<em>np</em>\<em>s</em>).</p>

<p>To make this informal description of the interpretation of the type
language precise, we turn to modal logic. Frames <em>F</em> =
(<em>W</em>,<em>R</em>), in the categorial setting, consist of a
set <em>W</em> of linguistic resources (expressions,
‘signs’), structured in terms of a ternary relation
<em>R</em>. This ternary relation stands for grammatical composition, or
‘Merge’ as it is known in the generative tradition: read
<em>Rxyz</em> as “the expression <em>x</em> is obtained by
putting together the subexpressions <em>y</em>
and <em>z</em>”. For comparison: in the Routley-Meyer semantics
for relevance logic, R would be the accessibility relation
interpreting the fusion operation.</p>

<p>A model <em>M</em> is a pair (<em>F</em>,<em>V</em>)
where <em>V</em> is an interpretation (valuation) sending atomic types
to subsets of <em>W</em>.  For complex types, the valuation respects
the clauses below.  We write <em>x</em> ∈ <em>V</em>(<em>A</em>)
as <em>M</em>,<em>x</em> ⊩ <em>A</em> or, if no confusion will
arise, <em>x</em> ⊩ <em>A</em>.</p>

<ul>

<li><em>x</em> ⊩ <em>A</em> ⊗ <em>B</em> iff
∃<em>yz</em> such that <em>Rxyz</em> and <em>y</em>
⊩ <em>A</em> and <em>z</em> ⊩ <em>B</em></li>

<li><em>y</em> ⊩ <em>C</em>/<em>B</em> iff ∀<em>xz</em>,
if <em>Rxyz</em> and <em>z</em> ⊩ <em>B</em>, then <em>x</em>
⊩ <em>C</em></li>

<li><em>z</em> ⊩ <em>A</em>\<em>C</em> iff ∀<em>xy</em>,
if <em>Rxyz</em> and <em>y</em> ⊩ <em>A</em>, then <em>x</em>
⊩ <em>C</em></li>

</ul>

<p>A syntactic calculus for a categorial type language is a deductive
system producing statements of the form <em>A</em> ⊢ <em>B</em>
(‘<em>B</em> is derivable from <em>A</em>’).  A
statement <em>A</em> ⊢ <em>B</em> holds in a model
(‘<em>M</em> ⊨ <em>A</em> ⊢ <em>B</em>’)
if <em>V</em>(<em>A</em>) ⊆ <em>V</em>(<em>B</em>); it is valid
(‘⊨ <em>A</em> ⊢
<em>B</em>’) if it holds in all models. For the minimal grammar
logic, the set of derivations in the syntactic calculus is freely
generated from the axiom and rules of inference below. This minimal
system, for historical reasons, is known as <b>NL</b> (for
‘<u>N</u>on-associative <u>L</u>ambek calculus’).  The
first line states that derivability is a reflexive, transitive
relation, i.e., a preorder.  The bidirectional (‘if and only
if’) inference rules of the second line characterize the product
and division operations as a residuated triple as it is known in
residuation theory. (Galatos <em>et al</em>. 2007 provides good
background reading.) The syntactic calculus, so defined, precisely
fits the intended interpretation of the type language, in the sense of
the soundness and completeness result below.</p>
 
<ul>

<li>preorder laws:
  <ul>
  <li><em>A</em> ⊢ <em>A</em> (reflexivity)</li>
  <li>from <em>A</em> ⊢ <em>B</em> and <em>B</em>
⊢ <em>C</em> infer <em>A</em> ⊢ <em>C</em> (transitivity)</li>
  </ul>
  </li>

<li>residuation laws: <em>A</em> ⊢
<em>C</em>/<em>B</em> iff <em>A</em> ⊗ <em>B</em>
⊢ <em>C</em> iff <em>B</em> ⊢ <em>A</em>\<em>C</em></li>
   
<li>soundness and completeness: <em>A</em> ⊢ <em>B</em> is
provable in the grammatical base logic <b>NL</b> iff
⊨ <em>A</em> ⊢ <em>B</em>, i.e., iff <em>V</em>(<em>A</em>)
⊆ <em>V</em>(<em>B</em>) for every valuation <em>V</em> on every
frame <em>F</em> (Došen 1992).</li>

</ul>

<p>The completeness result for <b>NL</b> does not impose any restrictions on
the interpretation of the Merge
relation <em>R</em>. This means that the theorems
and inference rules of the minimal grammar logic have the status of
grammatical invariants: properties of type combination that hold no
matter what the structural particularities of individual languages may
be.  Here are some examples of such universally valid principles. They
come in pairs, because of the left-right symmetry relating slash and
backslash.</p>
       

<ul>

<li>application:
 <ul>
 <li>(<em>A</em>/<em>B</em>) ⊗ <em>B</em> ⊢ <em>A</em></li>
 <li><em>B</em> ⊗ (<em>B</em>\<em>A</em>) ⊢ <em>A</em></li>
 </ul>
 </li>
       
<li>co-application: 
 <ul>
 <li><em>A</em> ⊢ (<em>A</em> ⊗ <em>B</em>)/<em>B</em></li>
 <li><em>A</em> ⊢ <em>B</em>\(<em>B</em> ⊗ <em>A</em>)</li>
 </ul>
 </li>

<li>lifting: 
 <ul>
 <li><em>A</em> ⊢ <em>B</em>/(<em>A</em>\<em>B</em>)</li>
 <li><em>A</em> ⊢ (<em>B</em>/<em>A</em>)\<em>B</em></li>
 </ul>
 </li>

<li>monotonicity: from <em>A</em> ⊢ <em>B</em> and <em>C</em>
⊢ <em>D</em> infer any of the following:
 <ul>
 <li><em>A</em> ⊗ <em>C</em> ⊢ <em>B</em> ⊗ <em>D</em></li>
 <li><em>A</em>/<em>D</em> ⊢ <em>B</em>/<em>C</em></li>
 <li><em>D</em>\<em>A</em> ⊢ <em>C</em>\<em>B</em></li>
 </ul>
 </li>
</ul>

<p>Given the universal freely generated syntactic calculus discussed
above, the task of providing a categorial grammar for
a <em>particular</em> language is reduced to specifying its lexicon:
the typelogical framework, in this sense, represents the culminating
point of ‘lexicalizing’ grammar formalisms.  A categorial
lexicon is a relation associating each word with a finite number of
types.  Given a lexicon <em>Lex</em>, a categorial grammar assigns
type <em>B</em> to a string of words <em>w</em><sub>1</sub> ··· 
<em>w<sub>n</sub></em> iff for 1 ≤ <em>i</em> ≤ <em>n</em>,
(<em>w<sub>i</sub></em>,<em>A<sub>i</sub></em>) ∈ <em>Lex</em>,
and <em>X</em> ⊢ <em>B</em> is provable in the syntactic
calculus where <em>X</em> is a product formula with
yield <em>A</em><sub>1</sub>
··· <em>A<sub>n</sub></em>. The grammar can be
considered adequate if the strings <em>w</em><sub>1</sub>
··· <em>w<sub>n</sub></em> are indeed judged to be the wellformed
phrases of type <em>B</em>.</p>

<p>Lexical type assignments don't have to be handcrafted: Buszkowski
and Penn (1990) model the <em>acquisition</em> of the lexicon as a
process of solving type equations. Their unification-based algorithms
take function-argument structures as input (binary trees with a
distinguised head daughter); one obtains variations depending on
whether the solution should assign a unique type to every vocabulary
item (so-called rigid grammars), or whether one accepts multiple
assignments (<em>k</em>-valued grammars, with <em>k</em> as the
maximal lexical ambiguity factor). Kanazawa (1998) studies learnable
classes of grammars from this perspective, in the sense of
Gold’s notion of identifiability ‘in the limit’.
This line of research is by now well established, for typelogical
grammars proper, and for the related pregroup formalism discussed
under §6.</p>

<p>The radical lexicalism of typelogical grammar means there is no
room for construction specific rule stipulations: central grammatical
notions, rather than being postulated, must emerge from the type
structure. Here are some examples.</p>
    

<ul>

<li><em>Valency</em>. Intransitive verbs (‘dreams’,
‘sleeps’) form a complete sentence with just a subject
noun phrase to their left; transitive verbs (‘sees’,
‘admires’) require both a subject to their left and a
direct object to their right. Such selectional requirements are
expressed in terms of the directional
implications: <em>np</em>\<em>s</em> for intransitive,
(<em>np</em>\<em>s</em>)/<em>np</em> for transitive verbs.  In a
context-free grammar, these distinctions require the postulation of
new non-terminals.</li>
 
<li><em>Case</em>. The distinction between phrases that can fulfill
any noun phrase selectional requirement (say, proper names) versus
case-marked pronouns insisting on playing the subject or direct object
role is expressed through higher-order type assignment: proper names
can be typed simply as <em>np</em>, subject pronouns
(‘he’, ‘she’)
as <em>s</em>/(<em>np</em>\<em>s</em>), direct object pronouns (and
reflexives) as
((<em>np</em>\<em>s</em>)/<em>np</em>)\(<em>np</em>\<em>s</em>).
Under such typing, one correctly distinguishes between the grammatical
‘he sees her’ versus the ill-formed ‘him likes
she’.</li>
    
<li><em>Complements vs modifiers</em>.  Compare exocentric types
(<em>A</em>/<em>B</em> or <em>B</em>\<em>A</em> with <em>A</em>
≠ <em>B</em>) versus endocentric
types <em>A</em>/<em>A</em>, <em>A</em>\<em>A</em>. The latter express
modification; optionality of modifier phrases follows. Compare
‘the girl’, ‘the smart girl’, ‘the girl
(who teases Bill)’, with ‘girl’ typed as <em>n</em>,
‘smart’ as <em>n</em>/<em>n</em> and the relative clause
‘who teases Bill’ as <em>n</em>\<em>n</em>.</li>
    
<li><em>Filler-gap dependencies</em>.  Types with nested implications
<em>A</em>/(<em>C</em>/<em>B</em>), <em>A</em>/(<em>B</em>\<em>C</em>),
etc., signal the withdrawal of a gap hypothesis of type <em>B</em> in
a domain of type <em>C</em>. The relative pronoun ‘who’ in
‘the girl who teases Bill’ is typed as
(<em>n</em>\<em>n</em>)/(<em>np</em>\<em>s</em>): it combines with a
sentence of which the subject must remain unexpressed, as one sees
from the ill-formed ‘the girl who Mary teases Bill’.</li>

</ul>

<p><b>Sequent calculus, decidability</b>. How can we decide whether a
statement <em>A</em> ⊢ <em>B</em> is indeed a theorem of the
syntactic calculus?  In the presence of <em>expanding</em> rules, such as
Lifting, this is not immediately obvious: the transitivity inference
from <em>A</em> ⊢ <em>B</em> and <em>B</em> ⊢ <em>C</em>
to <em>A</em> ⊢ <em>C</em> involves a ‘lemma’
formula <em>B</em> which disappears from the conclusion, and there is
an infinite number of candidate lemma formulas to consider.  A key
point of Lambek's original papers consists in the reformulation of the
syntactic calculus in an equivalent Gentzen-style sequent format,
which enjoys cut-elimination.</p>

<p>Sequents for the grammatical base logic <b>NL</b> are
statements <em>X</em> ⇒ <em>A</em>, where <em>X</em> (the
antecedent) is a structure, and <em>A</em> (the succedent) a type
formula. The antecedent structure is required to be non-empty. A
structure is either a single formula, or a binary branching tree with
formulas at the leaves.  In the sequent rules below, the tree
structure of the antecedent is indicated by bracketing
(–,–).  The notation <em>X</em>[<em>Y</em>] refers to a
structure <em>X</em> containing a distinguished
substructure <em>Y</em>.  For every connective (type-forming
operation), there are two inference rules, introducing the connective
to the left or to the right of the derivability arrow.  In this
sequent presentation, as in Gentzen's propositional logics, Cut
is an <em>admissible rule</em>: every proof of a theorem that makes use of Cut
inferences can be transformed into an equivalent cut-free derivation.
Backward-chaining proof search in the cut-free system respects the
subformula property and immediately yields a decision procedure.</p>

<blockquote>
<!--pdf exclude begin-->
<table width="80%" cellspacing="+5" cellpadding="+5" style="border:1px solid black; border-spacing: 0.5em 1em;">
<!--pdf exclude end-->
<!--pdf include
<table width="80%" cellspacing="+5" cellpadding="+5">
pdf include-->

<tbody><tr>
<td><table><tbody><tr>
 <td>
   <table style="border-spacing:0px;"> 
   <tbody><tr> 
   <td style="padding:0px;border-bottom:1px solid black; text-align:center;"> </td> 
   </tr> 

   <tr> 
   <td style="padding:0px; text-align:center;"><em>A</em>⇒<em>A</em></td>
   </tr>
   </tbody></table>
 </td>
 <td valign="middle">Ax</td>
  </tr></tbody></table>
</td>

<td><table><tbody><tr>
  <td>
     <table style="border-spacing:0px;"> 
     <tbody><tr> 
     <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>Y</em>⇒<em>A</em>  <em>X</em>[<em>A</em>]⇒<em>B</em></td> 
     </tr> 

     <tr> 
     <td style="padding:0px; text-align:center;"><em>X</em>[<em>Y</em>]⇒<em>B</em></td>
     </tr>
     </tbody></table>
  </td>
  <td valign="middle">Cut</td>
  </tr></tbody></table>
</td>
</tr>

<tr>
<td><table><tbody><tr>
 <td>
   <table style="border-spacing:0px;"> 
   <tbody><tr> 
   <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>X</em>⇒<em>A</em>  <em>Y</em>⇒<em>B</em></td> 
   </tr> 

   <tr> 
   <td style="padding:0px; text-align:center;">(<em>X</em>,<em>Y</em>)⇒<em>A</em>⊗<em>B</em></td>
   </tr>
   </tbody></table>
 </td>
 <td valign="middle">(⊗<em>R</em>)</td>
  </tr></tbody></table>
</td>

<td><table><tbody><tr>
  <td>
     <table style="border-spacing:0px;"> 
     <tbody><tr> 
     <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>X</em>[(<em>A</em>,<em>B</em>)]⇒<em>C</em></td> 
     </tr> 

     <tr> 
     <td style="padding:0px; text-align:center;"><em>X</em>[<em>A</em>⊗<em>B</em>]⇒<em>C</em></td>
     </tr>
     </tbody></table>
  </td>
  <td valign="middle">(⊗<em>L</em>)</td>
  </tr></tbody></table>
</td>
</tr>

<tr>
<td><table><tbody><tr>
 <td>
   <table style="border-spacing:0px;"> 
   <tbody><tr> 
   <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>Y</em>⇒<em>B</em>  <em>X</em>[<em>A</em>]⇒<em>C</em></td> 
   </tr> 

   <tr> 
   <td style="padding:0px; text-align:center;"><em>X</em>[(<em>Y</em>,<em>B</em>\<em>A</em>)]⇒<em>C</em></td>
   </tr>
   </tbody></table>
 </td>
 <td valign="middle">(\<em>L</em>)</td>
  </tr></tbody></table>
</td>

<td><table><tbody><tr>
  <td>
     <table style="border-spacing:0px;"> 
     <tbody><tr> 
     <td style="padding:0px;border-bottom:1px solid black; text-align:center;">(<em>B</em>,<em>X</em>)⇒<em>A</em></td> 
     </tr> 

     <tr> 
     <td style="padding:0px; text-align:center;"><em>X</em>⇒<em>B</em>\<em>A</em></td>
     </tr>
     </tbody></table>
  </td>
  <td valign="middle">(\<em>R</em>)</td>
  </tr></tbody></table>
 </td>
</tr>

<tr>
<td><table><tbody><tr>
 <td>
   <table style="border-spacing:0px;"> 
   <tbody><tr> 
   <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>Y</em>⇒<em>B</em>  <em>X</em>[<em>A</em>]⇒<em>C</em></td> 
   </tr> 

   <tr> 
   <td style="padding:0px; text-align:center;"><em>X</em>[(<em>A</em>/<em>B</em>,<em>Y</em>)]⇒<em>C</em></td>
   </tr>
   </tbody></table>
 </td>
 <td valign="middle">(/<em>L</em>)</td>
  </tr></tbody></table>
 </td>

<td><table><tbody><tr>
  <td>
     <table style="border-spacing:0px;"> 
     <tbody><tr> 
     <td style="padding:0px;border-bottom:1px solid black; text-align:center;">(<em>X</em>,<em>B</em>)⇒<em>A</em></td> 
     </tr> 

     <tr> 
     <td style="padding:0px; text-align:center;"><em>X</em>⇒<em>A</em>/<em>B</em></td>
     </tr>
     </tbody></table>
  </td>
  <td valign="middle">(/<em>R</em>)</td>
  </tr></tbody></table>
 </td>
</tr>

</tbody></table>
<b>NL</b>: Sequent Calculus 
</blockquote>

<p><b>A landscape of calculi</b>. From the pure residuation logic, one obtains
a hierarchy of calculi by attributing associativity and/or
commutativity properties to the composition operation ⊗. These
calculi reflect different views on how the grammatical material is
structured.  The table below summarizes the options. </p>

<blockquote>
<table border="1" cellspacing="0" cellpadding="+6">
<tbody><tr align="center">
<td align="right">logic</td>
<td>structure</td>
<td>associative</td>
<td>commutative</td>
</tr>

<tr align="center"> 
<td align="right"><b>NL</b></td>
<td>tree</td>
<td>–</td>
<td>–</td>
</tr>

<tr align="center">
<td align="right"><b>L</b></td>
<td>string</td>
<td>✓</td>
<td>–</td>
</tr>

<tr align="center">
<td align="right"><b>NLP</b></td>
<td>mobile</td>
<td>–</td>
<td>✓</td>
</tr>

<tr align="center">
<td align="right"><b>LP</b> (=MILL)</td>
<td>multiset</td>
<td>✓</td>
<td>✓</td>
</tr>
</tbody></table>
</blockquote>

<p>
For reasons of historical precedence, the system of Lambek 1958, with
an associative composition operation, is referred to as <b>L</b>; the
pure residuation logic <b>NL</b>, i.e., the non-associative version
of <b>L</b>, was introduced later in Lambek 1961.  Addition of
commutativity turns <b>L</b> and <b>NL</b> into <b>LP</b>
and <b>NLP</b>, respectively. <b>LP</b>, Lambek calculus with
Permutation, was introduced in van Benthem 1983; in retrospect, this
the implication/fusion fragment of Multiplicative Intuitionistic
Linear Logic (MILL). The commutative variant of <b>NL</b> has been
studied by Kandulski; it has not found significant linguistic
applications. For the four systems in the hierarchy, one writes
(<b>N</b>)<b>L</b>(<b>P</b>)* for the (non-conservative) extension
which allows empty antecedents.</p>

<p>
In the presentation of the syntactic calculus, the structural options
take the form of <em>non-logical axioms</em> (postulates), which one
adds to the residuation laws.  In the sequent presentation, one has
corresponding structural rules.  The sequent calculus
for <b>L</b>(<b>P</b>) also allows for a sugared presentation where
the antecedent is treated as a list or a multiset of formulas; the
inference steps for the structural rules can then be left implicit.
With respect to the frame semantics, each of the structural options
introduces a constraint on the interpretation of the Merge relation
<em>R</em><sub>⊗</sub>.  Completeness for the calculi with structural options is
then with respect to all frames respecting the relevant constraints
(Došen 1992). The associative calculus <b>L</b> in fact allows
more specific models where we read <em>Rxyz</em> as <em>x</em>
= <em>y</em> · <em>z</em>, with · a
binary <em>concatenation</em> operation.  Pentus (1995) presents a
quite intricate proof that <b>L</b> is indeed complete with respect to
such free semigroup models (or language models, as they are also
called).</p>

<ul>

<li>structural postulates: 
  <ul>
  <li><em>A</em> ⊗ <em>B</em> ⊢ <em>B</em>
  ⊗ <em>A</em> (commutativity)</li>
  <li><em>A</em> ⊗ (<em>B</em> ⊗ <em>C</em>) ≡
  (<em>A</em> ⊗ <em>B</em>) ⊗ <em>C</em> (associativity)</li>
  </ul>
  </li>
    
<li>corresponding structural rules for the sequent presentation:

  <blockquote>
  <table>
  <tbody><tr>
  <td>
    <table><tbody><tr>
    <td><table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>W</em>[(<em>Y</em>,<em>X</em>)]⇒<em>C</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>W</em>[(<em>X</em>,<em>Y</em>)]⇒<em>C</em></td>
</tr>
</tbody></table>
    </td>
 
    <td>(comm.)</td> 
    </tr></tbody></table>
  </td>
 
  <td>
    <table><tbody><tr>
    <td><table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:double;text-align:center;"><em>W</em>[((<em>X</em>,<em>Y</em>),<em>Z</em>)]⇒<em>C</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>W</em>[(<em>X</em>,(<em>Y</em>,<em>Z</em>))]⇒<em>C</em></td>
</tr>
</tbody></table>
    </td>
 
    <td>(assoc.)</td> 
    </tr></tbody></table>
  </td>
  </tr>
  </tbody></table>
  </blockquote>
</li>

<li>frame constraint for commutativity:
∀<em>xyz</em>(<em>Rxyz</em> implies <em>Rxzy</em>)</li>
    
<li>frame constraint for associativity (rebracketing to the left):
∀<em>xyzrs</em>(<em>Rrxs</em> and <em>Rsyz</em> imply
∃<em>t</em>(<em>Rrtz</em> and <em>Rtxy</em>)); similarly for the
other direction</li>

</ul>

<p>A characteristic property of <b>LP</b> is the collapse of the
distinction between left and right division: <em>A</em>\<em>B</em>
and <em>B</em>/<em>A</em> become interderivable.  From
a <em>syntactic</em> point of view, the global availability of
associativity and/or commutativity in <b>LP</b> would entail that
arbitrary changes in constituent structure and/or word order cannot
affect the well-formedness of an expression — a position that
few linguists will subscribe to.  Whether global associativity as we
find it in <b>L</b> is a viable option for syntax is open to dispute. Lambek
points to cases of overgeneration resulting from free rebracketing in
his 1961 paper, but later reverts to the associative view. In the
following section, we will see that <b>LP</b> is perfectly legitimate
as a calculus of meaning composition. In §3, we discuss
<em>controlled</em> forms of structural reasoning, anchored in lexical
type assignment, as an alternative to the problematic all-or-nothing
options.</p>

<h3><a name="SynSemInt">2.2 The syntax-semantics interface</a></h3>

<p>In order to give equal weight to syntactic and semantic
considerations, we can set up a categorial grammar as a pair of
calculi linked by a compositional interpretation: a
syntactic <em>source</em> calculus, a semantic <em>target</em>
calculus, and a homomorphism mapping types and derivations of the
source to the corresponding types and derivations of the target. In
this section, we work out this architecture for the standard Lambek
calculi. In the following section, we will see how the extended
categorial type logics reflect different views on the division of
labour between syntax and semantics, and hence on the amount of work
that has to be done to achieve a compositional mapping between the
two.</p>

<blockquote>
<table cellpadding="+8" cellspacing="+7">
<tbody><tr align="center">
<td valign="bottom">(<b>N</b>)<b>L</b><span style="margin:0;padding:0;width:-moz-min-content;width:min-content;vertical-align:text-bottom;display:inline-block;font-size:75%;text-align:left;position:relative;bottom:-0.3ex;line-height: 100%;">{<em>n</em>,<em>np</em>,<em>s</em>}<span class="lower">/,\</span></span></td>
<td align="center" valign="top">(·)′<br /><img src="longrightarrow.png" alt="homomorphically  maps to" /></td>
<td valign="bottom"><b>LP</b><span style="margin:0;padding:0;width:-moz-min-content;width:min-content;vertical-align:text-bottom;display:inline-block;font-size:75%;text-align:left;position:relative;bottom:-0.3ex;line-height: 100%;">{<em>e</em>,<em>t</em>}<span class="lower">→</span></span></td>
</tr>

<tr align="center">
<td>Syntax</td>
<td>homomorphism</td>
<td>Semantics</td>
</tr>
</tbody></table>
</blockquote>

<p>The semantic study of categorial deduction started with van Benthem
1983.  In line with the Curry-Howard ‘formulas-as-types,
proofs-as-programs’ method, derivations in the various
categorial calculi are associated with terms of suitable fragments of
the lambda calculus. For the sake of simplicity, we restrict attention
to the implicational vocabulary: the directional slashes for
(<b>N</b>)<b>L</b>, and the non-directional implication
of <b>LP</b>.</p>

<p>
Let us start with the Curry-Howard correspondence for <b>LP</b>. The
choice of <b>LP</b> as the calculus of natural language semantics captures
the fact that meaning composition is a resource-sensitive process: in
interpreting a derivation, no material can be copied or thrown away.
Let us refer to the terms of the simply typed <em>linear</em> lambda calculus
as Λ(Lin).  In the absence of Contraction and Weakening, this
is the smallest set such that, given a denumerably infinite set of
variables Var,</p>
    
<ul>

<li><em>x</em> ∈ Λ(Lin) if <em>x</em> ∈ Var;</li>
    
<li>λ<em>x</em>.<em>M</em> ∈ Λ(Lin) if <em>x</em>
∈ Var, <em>M</em> ∈ Λ(Lin), and there is exactly one
free variable occurrence of <em>x</em> in <em>M</em>;</li>
    
<li>(<em>M</em> <em>N</em>) ∈ Λ(Lin) if <em>M</em> ∈
Λ(Lin), <em>N</em> ∈ Λ(Lin), and the sets of free variables
of <em>M</em> and <em>N</em> are disjoint.</li>

</ul>

<p>From Λ(Lin), one obtains Λ(<b>LP</b>), the terms in
Curry-Howard correspondence with <b>LP</b> derivations, by adding one
extra restriction, reflecting the requirement that sequent antecedents
must be non-empty: every subterm contains at least one free variable.
The derivations of <b>LP</b> are now read as judgements of a type
inference system for Λ(<b>LP</b>).  Derivations are given in
the natural deduction style, with elimination and introduction rules
for the linear implication →. Judgements then are sequents of the
form Γ ⊢ <em>M</em> : <em>B</em>.  Γ is a non-empty
multiset of typing declarations <em>x</em><sub>1</sub>
: <em>A</em><sub>1</sub>, …,
<em>x<sub>n</sub></em> : <em>A<sub>n</sub></em>;
the <em>A<sub>i</sub></em> and <em>B</em> are linear implicative
types, and <em>M</em> is a Λ(<b>LP</b>) term of type <em>B</em>
built out of the <em>x<sub>i</sub></em> of
type <em>A<sub>i</sub></em>. Axiom sequents are of the
form <em>x</em> : <em>A</em> ⊢ <em>x</em> : <em>A</em>. Elimination
and Introduction rules for the linear implication are given below. In
the Introduction rule, <em>x</em> ∉ dom(Γ); in the
Elimination rule dom(Γ) ∩ dom(Δ) = ∅.</p>

<blockquote>
<table cellpadding="+7" cellspacing="+7">
<tbody><tr>
<td>
  <table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">Γ,<em>x</em> : <em>A</em> ⊢ <em>M</em> : <em>B</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">Γ ⊢
λ<em>x</em>.<em>M</em> : <em>A</em> → <em>B</em></td>
      </tr>
    </tbody></table>
  </td>

  <td>→<em>I</em></td>
  </tr></tbody></table>
</td>

<td>
  <table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">Γ ⊢ <em>M</em> : <em>A</em> → <em>B</em>       Δ ⊢ <em>N</em> : <em>A</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">Γ,Δ ⊢
(<em>M</em> <em>N</em>) : <em>B</em></td>
      </tr>
    </tbody></table>
  </td>

  <td>→<em>E</em></td>
  </tr></tbody></table>
</td>
</tr>
</tbody></table>
</blockquote>

<p>The syntactic calculi (<b>N</b>)<b>L</b>, similarly, are in
Curry-Howard correspondence with <em>directional</em> linear lambda
terms (Wansing 1992). There is no standard notation. In the
directional typing rules below, we use λ<sup><em>r</em></sup>
vs λ<sup><em>l</em></sup> for the image of the slash and
backslash Introduction rules; in the case of the Elimination rules, we
have right and left function application, with the triangle pointing
to the function. The antecedent is now a (bracketed) string of typing
declarations <em>x<sub>i</sub></em> : <em>A<sub>i</sub></em>.</p>

<blockquote>
<table cellpadding="+7" cellspacing="+7">
<tbody><tr align="center">
<td>
  <table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">(Γ,<em>x</em> : <em>A</em>) ⊢ <em>M</em> : <em>B</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">Γ ⊢ λ<sup><em>r</em></sup><em>x</em>.<em>M</em> : <em>B</em>/<em>A</em></td>
      </tr>
    </tbody></table>
  </td>

  <td><em>I</em>/</td>
  </tr></tbody></table>
</td>

<td>
  <table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">(<em>x</em> :
<em>A</em>,Γ) ⊢ <em>M</em> : <em>B</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">Γ ⊢ λ<sup><em>l</em></sup><em>x</em>.<em>M</em> : <em>A</em>\<em>B</em></td>
      </tr>
    </tbody></table>
  </td>

  <td><em>I</em>\</td>
  </tr></tbody></table>
</td>
</tr>

<tr>
<td>
  <table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">Γ
⊢ <em>M</em> : <em>B</em>/<em>A</em>       Δ ⊢ <em>N</em> : <em>A</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">(Γ,Δ) ⊢ (<em>M</em> ⊲ <em>N</em>) : <em>B</em></td>
      </tr>
    </tbody></table>
  </td>

  <td><em>E</em>/</td>
  </tr></tbody></table>
</td>

<td>
  <table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">Γ ⊢ <em>N</em> : <em>A</em>       Δ ⊢ <em>M</em> : <em>A</em>\<em>B</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">(Γ,Δ) ⊢ (<em>N</em> ⊳ <em>M</em>) : <em>B</em></td>
      </tr>
    </tbody></table>
  </td>

  <td><em>E</em>\</td>
  </tr></tbody></table>
</td>
</tr>
</tbody></table>
</blockquote>

<p>The source and target calculi each have their own set of basic
types, motivated by syntactic and semantic considerations
respectively.  At the syntactic end, one could discriminate between
sentences <em>s</em>, noun phrases <em>np</em>, common
nouns <em>n</em>, for example.  At the semantic end, for a simple
extensional interpretation, one could introduce two basic types, <em>e</em> and
<em>t</em>. Set-theoretic interpretation for the semantic types is
then given in terms of a non-empty set of
individuals/entities <em>E</em> and the truth values {0,1}.
Denotation domains Dom(<em>A</em>) for semantic types <em>A</em> are
set up such that Dom(<em>e</em>) = <em>E</em>, Dom(<em>t</em>) = {0,1}
and Dom(<em>A</em>\<em>B</em>) = Dom(<em>B</em>/<em>A</em>) is the set
of functions from Dom(<em>A</em>) to Dom(<em>B</em>).</p>

<p>
Consider next the homomorphic mapping from the syntactic source
calculus to the semantic target calculus. The mapping is specified at
the level of types and at the level of proofs (terms). Setting up the
mapping at the level of atomic types as below, sentences denote truth
values; (proper) noun phrases individuals; common nouns functions from
individuals to truth values. For complex syntactic types, the
interpretation function sends the two directional implications to the
linear implication of the target calculus. At the level of proofs
(terms), likewise, the directional constructs of the syntactic source
calculus are identified in the interpretation. (We use a primed
version for the target variable corresponding to a source variable, as
a reminder of the fact that the type of these expressions is
different.)</p>

<ul>
    
<li>types: (<em>np</em>)′ = <em>e</em>, (<em>s</em>)′
= <em>t</em>, (<em>n</em>)′ = <em>e</em> → <em>t</em>,
(<em>A</em>\<em>B</em>)′ = (<em>B</em>/<em>A</em>)′ =
<em>A</em>′ → <em>B</em>′</li>
  
<li>terms: (<em>x</em>)′ = <em>x</em>′,
(λ<sup><em>r</em></sup><em>x</em>.<em>M</em>)′ =
(λ<sup><em>l</em></sup><em>x</em>.<em>M</em>)′ =
λ<em>x</em>′.(<em>M</em>)′, (<em>N</em>
⊳ <em>M</em>)′ = (<em>M</em> ⊲ <em>N</em>)′ =
((<em>M</em>)′ (<em>N</em>)′)</li>

</ul>

<p>
In the previous sections, we have seen that the theorems of <b>NL</b>,
<b>L</b> and <b>LP</b> form a proper inclusion hierarchy.  The
semantic counterpart is that in moving to more discriminating
syntactic calculi, more and more <b>LP</b> terms are lost in
translation: desirable recipes for meaning assembly are often
unobtainable as the image of (<b>N</b>)<b>L</b> proofs.  For the three
calculi, the table below gives a characteristic type transition.  The
second row has the directional proof terms associated with the
(<b>N</b>)<b>L</b> type transitions; the third row gives the
corresponding <b>LP</b> term, obtained via the translation
homomorphism, in the case of (<b>N</b>)<b>L</b>, or directly for
the <b>LP</b> case.</p>

<!--pdf include 

<table width="100%" cellpadding="+5" cellspacing="+5" style="font-size:17px;">
pdf include-->
<!--pdf exclude begin-->
<table width="100%" cellpadding="+5" cellspacing="+5">
<!--pdf exclude end-->
<tbody><tr>
<td nowrap="nowrap"><b>NL</b>:  argument lowering</td>
<td><b>L</b>: composition</td>
<td><b>LP</b>:  argument raising</td>
</tr>

<tr>
<td nowrap="nowrap" valign="top">(<em>B</em>/(<em>A</em>\<em>B</em>))\<em>C</em> ⊢ <em>A</em>\<em>C</em></td>
<td nowrap="nowrap" valign="top"><em>A</em>\<em>B</em> ⊢ (<em>A</em>\<em>C</em>)/(<em>B</em>\<em>C</em>)</td>
<td><em>A</em> → (<em>B</em> → <em>C</em>) ⊢ <br />
 <span class="nw">((<em>A</em> → <em>C</em>) → <em>C</em>) → (<em>B</em> → <em>C</em>)</span></td>
</tr>

<tr>
<td nowrap="nowrap"><em>z</em> ⊢ λ<sup><em>l</em></sup><em>x</em>.((λ<sup><em>r</em></sup><em>y</em>.(<em>x</em> ⊳ <em>y</em>)) ⊳ <em>z</em>)</td>
<td nowrap="nowrap"><em>y</em> ⊢ λ<sup><em>r</em></sup><em>z</em>λ<sup><em>l</em></sup><em>x</em>.((<em>x</em> ⊳ <em>y</em>) ⊳ <em>z</em>)</td>
<td></td>
</tr>

<tr>
<td nowrap="nowrap"><em>z</em>′ ⊢ λ<em>x</em>′.(<em>z</em>′ λ<em>y</em>′.(<em>y</em>′ <em>x</em>′))</td>
<td nowrap="nowrap"><em>y</em>′ ⊢ λ<em>z</em>′λ<em>x</em>′.(<em>z</em>′ (<em>y</em>′ <em>x</em>′))</td>
<td nowrap="nowrap"><em>x</em> ⊢ λ<em>w</em>λ<em>z</em>.(<em>w</em> λ<em>y</em>.((<em>x</em> <em>y</em>) <em>z</em>))</td>
</tr>
</tbody></table>

<p>Argument lowering is valid in <b>NL</b>, hence also in
<b>L</b>, <b>LP</b>. Function composition depends on associativity; it
is invalid in <b>NL</b>, valid in <b>L</b>(<b>P</b>). Function
composition has been used to build up partial semantic
representations, running in parallel with the incremental
(left-to-right) processing of a string. Argument raising, finally, is
valid only in <b>LP</b>, i.e., this form of meaning assembly cannot be
obtained as the image of a derivation in any of the syntactic calculi
(<b>N</b>)<b>L</b>. But, as we will see below, this transition plays
an important role in the analysis of natural language scope
ambiguities.</p>

<p><b>Derivational versus lexical semantics</b>.  The constraints
imposed by resource-sensitivity put severe limitations on the
expressivity of the derivational semantics. To some extent, these can
be overcome at the level of <em>lexical</em> semantics. The
Curry-Howard term associated with a derivation, seen as a program for
meaning assembly, abstracts from the contribution of particular
lexical items.  The semantic specification for a lexical item of
type <em>A</em> takes the form of a lambda term of the corresponding
semantic type <em>A</em>′, but this lambda term is no longer
required to be linear.  Below some examples of non-linear lexical
meaning recipes.  The specification for the reflexive pronoun
‘himself’ is a pure combinator (a closed term): it
identifies the first and second argument of a binary relation.  The
terms for the relative pronoun ‘that’ or for the
determiners ‘a, some’ have an abstraction that binds two
occurrences of a variable, so as to compute the intersection of their
two (<em>e</em> → <em>t</em>) arguments (noun and verb phrase),
and to test the intersection for non-emptiness in the case of
‘some’.</p>

<!--pdf include
<table width="100%" cellpadding="+5" cellspacing="+5" align="center" style="font-size:18px;">
pdf include-->
<!--pdf exclude begin-->
<table width="100%" cellpadding="+5" cellspacing="+5" align="center">
<!--pdf exclude end-->

<tbody><tr>
<td></td>
<td><b>NL</b> type</td>
<td><b>LP</b> type</td>
<td>Lexical recipe</td>
</tr>

<tr>
<td>himself</td>
<td>((<em>np</em>\<em>s</em>)/<em>np</em>)\(<em>np</em>\<em>s</em>)</td>
<td>(<em>e</em>→<em>e</em>→<em>t</em>)→<em>e</em>→<em>t</em></td>
<td>λ<em>R</em>λ<em>x</em>.((<em>R</em> <em>x</em>) <em>x</em>)</td>
</tr>

<tr>
<td>a, some</td>
<td>(<em>s</em>/(<em>np</em>\<em>s</em>))/<em>n</em> </td>
<td>(<em>e</em>→<em>t</em>)→(<em>e</em>→<em>t</em>)→<em>t</em></td>
<td>λ<em>P</em>λ<em>Q</em>.(∃ λ<em>x</em>.((<em>P</em> <em>x</em>)∧(<em>Q</em> <em>x</em>)))</td>
</tr>

<tr>
<td>that</td>
<td>(<em>n</em>\<em>n</em>)/(<em>np</em>\<em>s</em>)</td>
<td>(<em>e</em>→<em>t</em>)→(<em>e</em>→<em>t</em>)→<em>e</em>→<em>t</em></td>
<td>λ<em>P</em>λ<em>Q</em>λ<em>x</em>.((<em>P</em> <em>x</em>)∧(<em>Q</em> <em>x</em>)))</td>
</tr>
</tbody></table>

<p>The interplay between lexical and derivational aspects of meaning
assembly is illustrated with the example below. Notice that the linear
proof term reflects the derivational history (modulo directionality);
after substitution of the lexical recipes and β conversion this
transparency is lost. The full encapsulation of lexical semantics is
one of the strong attractions of the categorial approach.</p>

<ul>

<li>string: ‘a boy hurt himself’</li>
    
<li><b>NL</b> derivation: (<em>x</em> :
(<em>s</em>/<em>np</em>\<em>s</em>)/<em>n</em>,<em>y</em>
: <em>n</em>),(<em>z</em> :
((<em>np</em>\<em>s</em>)/<em>np</em>,<em>w</em> :
((<em>np</em>\<em>s</em>)/<em>np</em>)\(<em>np</em>\<em>s</em>))
⊢ <em>M</em> : <em>s</em></li>
    
<li>corresponding <b>LP</b> proof term: (<em>M</em>)′ =
((<em>x</em>′ <em>y</em>′)
(<em>w</em>′ <em>z</em>′))</li>
    
<li>lexical insertion: <em>x</em>′ ↦
λ<em>P</em>λ<em>Q</em>.(∃
λ<em>x</em>.((<em>P</em>
 <em>x</em>)∧(<em>Q</em> <em>x</em>))),<em>y</em>′
↦ boy,<em>z</em>′ ↦ hurt,<em>w</em>′ ↦
λ<em>R</em>λ<em>x</em>.((<em>R</em> <em>x</em>) <em>x</em>)</li>
    
<li>final result after β conversion: (∃
 λ<em>x</em>.((boy <em>x</em>) ∧ (hurt <em>x</em> <em>x</em>)))</li>

</ul>

<h2><a name="ExtTypGra">3.   Extended typelogical grammars</a></h2>

<p>The strategy of combining resource-sensitivity at the level of
derivational semantics with non-linear lexical meaning assignments is
not a general solution for the expressive limitations of the syntactic
calculi originally conceived by Lambek. A type of problem that
necessitates extensions of the derivational machinery itself has to do
with the inability of (<b>N</b>)<b>L</b> to
establish <em>discontinuous</em> dependencies between grammatical
resources in a satisfactory way.  Such dependencies manifest
themselves in two situations.</p>

<p><b>Extraction</b>.  These are phenomena which generative grammar
refers to as ‘overt displacement’, as we find it
in <em>wh</em> ‘movement’ constructions.  Compare
the <em>wh</em> phrases ‘what — annoys Mary’ versus
‘what Mary put — there’, as they would occur in a
context ‘I know what annoys Mary’ or ‘I know what
Mary put there’.  The dash marks the spot where a noun phrase
would fit in a normal declarative sentence: ‘this record annoys
Mary’, ‘Mary put the record there’.  Assigning the
type <em>wh</em>/(<em>np</em>\<em>s</em>) to ‘what’, we
can derive <em>wh</em> for the first example, with a <em>np</em>
hypothesis in the left-peripheral subject position.  If this
hypothesis (the ‘gap’) occurs <em>internally</em>, as in
the second example, it is unreachable for (<b>N</b>)<b>L</b>: one
cannot derive <em>np</em>\<em>s</em> for ‘Mary — put
there’ (nor <em>s</em>/<em>np</em>, for that matter, which would
require a right-peripheral gap).</p>

<p><b>Infixation</b>. Typical examples would be cases of non-local
semantic construal, as in ‘Alice knows someone is
cheating’, with a wide-scope reading for ‘someone’:
‘there is a particular person <em>x</em> such that Alice
thinks <em>x</em> is cheating’. The quantifier phrase
‘someone’ occupies the structural position where in fact
the <em>np</em>-type hypothesis is needed, and realizes its semantic
effect at some higher sentential level. With a type-assignment
<em>s</em>/(<em>np</em>\<em>s</em>) for ‘someone’, there
is no derivation producing the non-local reading. In generative
grammar, one would call the non-local construal an instance of
‘covert’ movement.</p>  

<p>The extensions of the basic calculus
to be discussed in the sections below represent the strategies that
are being pursued to find principled solutions to these problems of
discontinuous dependencies.</p>

<h3><a name="MulSysStrCon">3.1   Multimodal systems, structural control</a></h3>

<p>The key idea of Multimodal Categorial Grammars, introduced in the
1990s, is simple: instead of a single family of residuated
type-forming operations, one considers multiple families, co-existing
in one logic. To discriminate between these families, type-forming
operations are decorated with a mode index
/<sub><em>i</em></sub>, ⊗<sub><em>i</em></sub>, \<sub><em>i</em></sub>;
likewise for the interpreting Merge
relations <em>R<sub>i</sub></em>. Each family has the
same <em>logical</em> rules (the residuation laws). But they can
differ in their structural properties. In particular, they can
<em>interact</em> in terms of structural rules that mix different
modes.  At the level of the interpretation, such interaction
principles introduce frame constraints, similar to the constraints
that went with the move from <b>NL</b> to <b>L</b> and <b>LP</b>.</p>

<p>For an example, let us return to the case of non-peripheral
extraction ‘(what) Mary put — there’.  Assume we
have two modes of composition: ⊗<sub><i>c</i></sub> and
⊗<sub><i>d</i></sub>.  Regular valency requirements are
expressed in terms of the slashes for the
⊗<sub><em>c</em></sub> product.  Discontinuous dependencies are
handled by structural postulates of <i>mixed</i> associativity and
commutativity, and an inclusion postulate expressing the relative
‘strength’ of ⊗<sub><em>c</em></sub> and
⊗<sub><em>d</em></sub>. The type-assignment to the pronoun
‘what’ provides access to these postulates, so that it can
establish a link with a non-subject <em>np</em> hypothesis, also in a
sentence-internal position.</p>
 
<ul>

<li>mixed associativity:
(<em>A</em>⊗<sub><em>c</em></sub> <em>B</em>)⊗<sub><em>d</em></sub> <em>C</em>
⊢ <em>A</em>⊗<sub><em>c</em></sub>(<em>B</em>
⊗<sub><em>d</em></sub> <em>C</em>)</li>
 
<li>mixed commutativity:
(<em>A</em>⊗<sub><em>c</em></sub> <em>B</em>)
⊗<sub><em>d</em></sub> <em>C</em> ⊢ (<em>A</em> ⊗<sub><em>d</em></sub>
<em>C</em>) ⊗<sub><em>c</em></sub> <em>B</em></li>
 
<li>inclusion: <em>A</em> ⊗<sub><em>d</em></sub> <em>B</em> ⊢
 <em>A</em> ⊗<sub><em>c</em></sub> <em>B</em></li>
 
<li>type for ‘what’ related to a non-subject <em>np</em>
hypothesis: <i>wh</i>/<sub><em>c</em></sub>(<em>s</em>/<sub><em>d</em></sub> <em>np</em>)</li>

</ul>

<p>We can now succesfully demonstrate that the phrase ‘Mary put
— there’ is of
type <em>s</em>/<sub><em>d</em></sub><em>np</em>, as shown in the
sketch of a derivation below. By means of the mixed interaction
postulates, the <em>np</em> hypothesis finds its way to the
non-peripheral position where it is required by the verb
‘put’. Once it has reached its home position, the
inclusion step switches from the structurally permissive mode <em>d</em> to the
<em>c</em> mode expressing regular subcategorization.</p>

<blockquote>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr><td align="center" colspan="6" style="border-bottom:1px solid black">⋮</td>
<td align="left" rowspan="6">
 <br />
 inclusion<br />
 mixed comm.<br />
 mixed assoc.<br />
 
</td></tr>

<tr><td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
<em>np</em>
⊗<sub><em>c</em></sub>
(((((<em>np</em>\<sub><em>c</em></sub><em>s</em>) /<sub><em>c</em></sub> <em>pp</em>) /<sub><em>c</em></sub> <em>np)</em>
⊗<sub><em>c</em></sub>
<em>np</em>)
⊗<sub><em>c</em></sub>
<em>pp</em>) ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
<em>np</em>
⊗<sub><em>c</em></sub>
(((((<em>np</em>\<sub><em>c</em></sub><em>s</em>) /<sub><em>c</em></sub> <em>pp</em>) /<sub><em>c</em></sub> <em>np</em>)
⊗<sub><em>d</em></sub>
<em>np</em>)
⊗<sub><em>c</em></sub>
<em>pp</em>) ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
<em>np</em>
⊗<sub><em>c</em></sub>
(((((<em>np</em>\<sub><em>c</em></sub><em>s</em>) /<sub><em>c</em></sub> <em>pp</em>) /<sub><em>c</em></sub> <em>np</em>)
⊗<sub><em>c</em></sub>
<em>np</em>)
⊗<sub><em>d</em></sub>
<em>np</em>) ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
(<em>np</em>
⊗<sub><em>c</em></sub>
((((<em>np</em> \<sub><em>c</em></sub> <em>s</em>) /<sub><em>c</em></sub> <em>pp</em>) /<sub><em>c</em></sub> <em>np</em>)
⊗<sub><em>c</em></sub>
<em>pp</em>))
⊗<sub><em>d</em></sub>
<em>np</em>) ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center"><em>np</em></td>
<td align="center">⊗<sub><em>c</em></sub></td>
<td align="center">(((<em>np</em> \<sub><em>c</em></sub> <em>s</em>) /<sub><em>c</em></sub> <em>pp</em>) /<sub><em>c</em></sub> <em>np</em>)</td>
<td align="center">⊗<sub><em>c</em></sub></td>
<td align="center"><em>pp</em>)</td>
<td align="center">⊢ <em>s</em> /<sub><em>d</em></sub> <em>np</em></td>
</tr>

<tr>
<td align="center" style="border-top:2px solid black;"><small>Mary</small></td>
<td></td>
<td align="center" style="border-top:2px solid black;"><small>put</small></td>
<td></td>
<td align="center" style="border-top:2px solid black;"><small>there</small></td>
<td></td>
</tr>
</tbody></table>
</blockquote>

<p>Through multimodal interaction principles, one avoids the
overgeneration that would result from <em>global</em> Associativity
and/or Commutativity options for a single composition operation
⊗. Undisciplined use of multimodality, however, could easily
lead to the introduction of construction-specific mode distinctions,
thus compromising one of the main attractions of the typelogical
approach.  The theory of structural control operators to be discussed
below offers a more principled solution to the type of problem
multimodal approaches address.</p>

<p><b>Control operators</b>.  Linear Logic, by dropping the structural
rules of Contraction and Weakening, splits the conjunction
‘&amp;’ of classical logic in a multiplicative and an
additive connective.  This move to a resource-sensitive logic can be
obtained without sacrificing expressivity: the unary modalities
‘!’,‘?’ allow one to reintroduce
Contraction/Weakening in a controlled form and thus to recover
classical logic from within the linear setting.</p>

<p>A similar strategy can be followed within typelogical grammar
(Moortgat 1996, Kurtonina 1995).  The type language is extended with a
pair of unary operators which we will write ◊,□; formulas
now are built out of atomic formulas <em>p</em> with the unary and
binary type-forming operations.  As with the binary vocabulary, we
start with a minimal logic for ◊,□; facilities for
structural control can then be added in the form of extra postulates.
The truth conditions below characterize the control operators ◊
and □ as inverse duals with respect to a binary accessibility
relation <em>R</em><sub>◊</sub>. This interpretation turns them
into a residuated pair, just like composition and the left and right
division operations.</p>

<ul>

<li>Formulas: <em>A</em>,<em>B</em> ::= <em>p</em> | ◊<em>A</em> |
 □<em>A</em> | <em>A</em>\<em>B</em> | <em>A</em>
 ⊗ <em>B</em> | <em>A</em>/<em>B</em></li>
    
<li>Interpretation:
  <ul>
  <li><em>x</em> ⊩◊<em>A</em> iff
  ∃<em>y</em>(<em>R</em><sub>◊</sub><em>xy</em>
  and <em>y</em> ⊩ <em>A</em>)</li>
  <li><em>x</em> ⊩□<em>A</em> iff
 ∀<em>y</em>(<em>R</em><sub>◊</sub><em>yx</em>
 implies <em>y</em> ⊩ <em>A</em>)</li>
  </ul>
  </li>    
<li>Residuation laws: ◊<em>A</em> ⊢ <em>B</em>
iff <em>A</em> ⊢ □<em>B</em></li>

</ul>

<p>We saw that in the minimal logic <b>NL</b> completeness with
respect to the frame semantics for composition and its residuals
doesn't impose restrictions on the interpretation of the merge
relation <em>R</em><sub>⊗</sub>. The same holds
for <em>R</em><sub>◊</sub> in the pure residuation logic of
◊,□. From the residuation laws, one easily derives
monotonicity of the control operators (<em>A</em> ⊢ <em>B</em>
implies ◊<em>A</em> ⊢ ◊<em>B</em> and □<em>A</em>
⊢ □<em>B</em>); their compositions satisfy
◊□<em>A</em> ⊢ <em>A</em> and <em>A</em> ⊢
□◊<em>A</em>. These properties can be put to use in refining
lexical type assignment so that <em>selectional dependencies</em> are
taken into account.  Compare the effect of an
assignment <em>A</em>/<em>B</em>
versus <em>A</em>/◊□<em>B</em>. The former will produce an
expression of type <em>A</em> in composition both with expressions of
type <em>B</em> and ◊□<em>B</em>, the latter only with the
more specific of these two, ◊□B. An expression typed as
□◊<em>B</em> will <em>resist</em> composition with
either <em>A</em>/<em>B</em> or <em>A</em>/◊□<em>B</em>.
Bernardi and Szabolcsi (2008) present accounts of licensing of
polarity-sensitive expressions and scope construal based on (a
generalization of) this modalisation strategy.</p>

<p>For sequent presentation of ◊,□, antecedent tree
structures now are built out of formulas by means of unary and binary
tree-building operations, (–) and (–,–).  The
residuation pattern then gives rise to the following left and right
introduction rules.  Cut elimination carries over straightforwardly to
the extended system, and with it decidability and the subformula
property.</p>

<blockquote>
<table cellpadding="+5" cellspacing="+5">
<tbody><tr>

<td><table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">Γ[(<em>A</em>)] ⇒ <em>B</em> </td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">Γ[◊<em>A</em>] ⇒ <em>B</em></td>
      </tr>
    </tbody></table>
  </td>
  <td valign="middle">◊<em>L</em></td>
  </tr></tbody></table>
</td>

<td><table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">Γ ⇒ <em>A</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">(Γ) ⇒ ◊<em>A</em></td>
      </tr>
    </tbody></table>
  </td>
  <td valign="middle">◊<em>R</em></td>
  </tr></tbody></table>
</td>

<td><table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">Γ[<em>A</em>] ⇒ <em>B</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">Γ[(□<em>A</em>)] ⇒ <em>B</em></td>
      </tr>
    </tbody></table>
  </td>
  <td valign="middle">□<em>L</em></td>
  </tr></tbody></table>
</td>

<td><table><tbody><tr>
  <td><table style="border-spacing:0px;"> 
      <tbody><tr> 
	<td style="padding:0px;border-bottom:1px solid black; text-align:center;">(Γ) ⇒ <em>A</em></td> 
      </tr> 

      <tr> 
	<td style="padding:0px; text-align:center;">Γ ⇒ □<em>A</em></td>
      </tr>
    </tbody></table>
  </td>
  <td valign="middle">□<em>R</em></td>
  </tr></tbody></table>
</td>

</tr>
</tbody></table>
</blockquote>

<p><b>Controlling structural resource management</b>.  Let us turn
then to the use of ◊,□ as control devices. The control can
take two forms: one is to <em>license</em> access to a structural
option that would be overgenerating if globally available; the second
is to occasionally <em>block</em> a structural capability of the
grammar. For the licensing type of control, compare the multimodal
treatment of extraction in terms of a distinction
⊗<sub><em>c</em></sub> vs ⊗<sub><em>d</em></sub>, with
the ◊-controlled version below, which relies on a single binary
composition operation, and narrows down the ‘movement’
potential entirely to ◊-marked resources.</p>

<ul>

<li>◊-controlled mixed associativity: (<em>A</em>
⊗ <em>B</em>) ⊗ ◊<em>C</em> ⊢ <em>A</em>
⊗ (<em>B</em> ⊗ ◊<em>C</em>)</li>

<li>◊-controlled mixed commutativity: (<em>A</em>
⊗ <em>B</em>) ⊗ ◊<em>C</em> ⊢
(<em>A</em> ⊗ ◊<em>C</em>) ⊗ <em>B</em></li>

<li>type for ‘what’ related to a non-subject <em>np</em>
hypothesis: <em>wh</em>/(<em>s</em>/◊□<em>np</em>)</li>

</ul>

<p>The derivation sketch below illustrates the interplay between
logical and structural reasoning. As long as the gap subformula
◊□<em>np</em> carries the licensing ◊, the structural
rules are applicable; as soon as it has found the appropriate
structural position where it is selected by the transitive verb, it
can be used as a regular <em>np</em>, given the type transition
◊□<em>np</em> ⊢ <em>np</em> which obviates the need
for the inclusion postulate of the earlier account.  Related analyses
of extraction based on the ‘!’ modality of Linear Logic
can be found in (Barry <em>et al</em>. 1991), with the dereliction
rule !<em>A</em> ⊢ <em>A</em> taking the place of
◊□<em>A</em> ⊢ <em>A</em> for gap introduction.</p>

<blockquote>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr><td align="center" colspan="6" style="border-bottom:1px solid black">⋮</td>
<td align="left" rowspan="6">
 <br />
 since ◊□<em>A</em> ⊢ <em>A</em><br />
 ◊ mixed comm.<br />
 ◊ mixed assoc.<br />
 
</td></tr>

<tr><td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
<em>np</em> ⊗ (((((<em>np</em>\<em>s</em>)/<em>pp</em>)/<em>np</em>)
  ⊗ <em>np</em>) ⊗ <em>pp</em>) ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
<em>np</em> ⊗ (((((<em>np</em>\<em>s</em>)/<em>pp</em>)/<em>np</em>) 
 ⊗ ◊□<em>np</em>) ⊗ <em>pp</em>) ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
<em>np</em> ⊗ (((((<em>np</em>\<em>s</em>)/<em>pp</em>)/<em>np</em>)
 ⊗ <em>pp</em>) ⊗ ◊□<em>np</em>) ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center" colspan="6" style="border-bottom:1px solid black"><span class="nw">
(<em>np</em> ⊗ ((((<em>np</em>\<em>s</em>)/<em>pp</em>)/<em>np</em>)
 ⊗<em>pp</em>)) ⊗ ◊□<em>np</em> ⊢ <em>s</em>
</span></td></tr>

<tr>
<td align="center"><em>np</em></td>
<td align="center">⊗ </td>
<td align="center">((((<em>np</em> \ <em>s</em>) / <em>pp</em>) / <em>np</em>)</td>
<td align="center">⊗</td>
<td align="center"><em>pp</em>)</td>
<td align="center">⊢ <em>s</em> / ◊□<em>np</em></td>
</tr>

<tr>
<td align="center" style="border-top:2px solid black;"><small>Mary</small></td>
<td></td>
<td align="center" style="border-top:2px solid black;"><small>put</small></td>
<td></td>
<td align="center" style="border-top:2px solid black;"><small>there</small></td>
<td></td>
</tr>
</tbody></table>
</blockquote>

<p>For the type of control where a structural transformation has to be
<em>blocked</em>, we can turn to extraction islands: phrases that do
not allow a gap, such as adjuncts. Compare ‘Mary fell asleep
during the concert’, with the adjunct phrase ‘during the
concert’ of type <em>s</em>\<em>s</em>, with the ill-formed
‘I know what Mary fell asleep during —’. To make the
adjunct inaccessible, one can assign ‘during’ the type
(□(<em>s</em>\<em>s</em>))/<em>np</em>: for the □ to be
eliminated, the whole adjunct phrase has to be enclosed by a
structural ◊, which then acts as a barrier for extraction. Without
such a barrier, the extraction postulates as formulated above would
allow the ill-formed sentence to be derived.  This strategy of
projecting island barriers from lexical type assignments was
introduced by Morrill; see the discussion of ‘structural
inhibition’ in Morrill 1994.</p>

<p>The two uses of the control operators illustrated here give rise to
a general theory of substructural communication relating the different
type logics in the categorial landscape. Let Source and Target be
neighbouring logics differing in some structural option, for
example <b>NL</b> versus <b>L</b>, or <b>L</b>
versus <b>LP</b>. Kurtonina and Moortgat (1997) establish a set of
embedding theorems of the following form:</p>

<ul>

<li>translation: μ : Source(/,⊗,\) →
Target(◊,□,/,⊗,\), such that</li>
    
<li><em>A</em> ⊢ <em>B</em> is derivable in the Source logic iff
μ(<em>A</em>) ⊢ μ(<em>B</em>) is derivable in the Target
logic</li>

</ul>

<p>In the case where the Target logic is more discriminating than the
Source, the translation implements control of the licensing
type. The other direction of communication obtains when the Target
system is less discriminating than the Source.  The modal decoration
effected by the translation in this case blocks the applicability of
structural rules.</p>

<p><b>Frame constraints, term assignment</b>.  The frame semantics for
the pure residuation logic does not impose restrictions on the
interpretation of the <em>R</em><sub>◊</sub>
and <em>R</em><sub>⊗</sub> relations.  In the case of extended
versions with modally controlled structural postulates, there is a
frame constraint for each structural postulate, and completeness holds
with respect to appropriately restricted frames.</p>

<blockquote>
<table cellpadding="0" cellspacing="0">
<tbody><tr>
<td align="center" valign="bottom">
  <table>
  <tbody><tr>
  <td><em>x</em></td><td></td><td></td><td></td><td><em>y</em></td><td></td><td><em>z</em></td>
  </tr>

  <tr>
    <td></td><td><font size="+2">\</font></td><td></td><td><font size="+2">/</font></td><td></td><td></td><td><font size="+2">|</font></td>
  </tr>

  <tr>
    <td></td><td></td><td><em>s</em></td><td></td><td></td><td></td><td><em>t</em></td>
   </tr>

  <tr>
    <td></td><td></td><td></td><td><font size="+2">\</font></td><td></td><td><font size="+2">/</font></td><td></td>
  </tr>

  <tr>
    <td></td><td></td><td></td><td></td><td><em>r</em></td><td></td><td></td>
  </tr>
  </tbody></table>
</td>

<td>   </td>

<td align="center">
 <table>
 <tbody><tr><td></td><td></td><td></td><td></td><td></td><td></td><td><em>z</em></td></tr>
 <tr><td></td><td></td><td></td><td></td><td></td><td></td><td><font size="+2">|</font></td></tr>
 <tr><td></td><td></td><td><em>y</em></td><td></td><td></td><td></td><td><em>t</em>′</td></tr>
 <tr><td></td><td></td><td></td><td><font size="+2">\</font></td><td></td><td><font size="+2">/</font></td><td></td></tr>
 <tr><td><em>x</em></td><td></td><td></td><td></td><td><em>s</em>′</td><td></td><td></td></tr>
 <tr><td></td><td><font size="+2">\</font></td><td></td><td><font size="+2">/</font></td><td></td><td></td><td></td></tr>
 <tr><td></td><td></td><td><em>r</em></td><td></td><td></td><td></td><td></td></tr>
 </tbody></table>
</td>

<td>   </td>

<td align="center">
 <table>
 <tbody><tr><td></td><td></td><td></td><td></td><td><em>z</em></td><td></td><td></td></tr>
 <tr><td></td><td></td><td></td><td></td><td><font size="+2">|</font></td><td></td><td></td></tr>
 <tr><td><em>x</em></td><td></td><td></td><td></td><td><em>t</em>′</td><td></td><td></td></tr>
 <tr><td></td><td><font size="+2">\</font></td><td></td><td><font size="+2">/</font></td><td></td><td></td><td></td></tr>
 <tr><td></td><td></td><td><em>s</em>′</td><td></td><td></td><td></td><td><em>y</em></td></tr>
 <tr><td></td><td></td><td></td><td><font size="+2">\</font></td><td></td><td><font size="+2">/</font></td><td></td></tr>
 <tr><td></td><td></td><td></td><td></td><td><em>r</em></td><td></td><td></td></tr>
 </tbody></table>
</td>
</tr>

<tr>
<td align="center">input</td>
<td>   </td>
<td align="center">output ◊ mixed ass.</td>
<td>   </td>
<td align="center">output ◊ mixed comm.</td>
</tr>
</tbody></table>
</blockquote>

<p>Depicting <em>R</em><sub>⊗</sub> with a branching
and <em>R</em><sub>◊</sub> with a non-branching node, in the case
of the modally controlled extraction postulates discussed here, we
have the constraint that for
all <em>x</em>,<em>y</em>,<em>z</em>,<em>r</em>,<em>s</em>,<em>t</em>
composed as in the input configuration below, there are alternative
internal points <em>s</em>′, <em>t</em>′ connecting
the root <em>r</em> to the leaves <em>x</em>,<em>y</em>,<em>z</em>.</p>

<p>For the mapping between the modally extended syntactic source
calculus and the semantic target calculus <b>LP</b>, we have two
options.  The first is to treat ◊,□ purely as syntactic
control devices.  One then sets (◊<em>A</em>)′ =
(□<em>A</em>)′ = <em>A</em>′, and the inference
rules affecting the modalities leave no trace in the <b>LP</b> term
associated with a derivation. The second is to actually provide
denotation domains for the new types, and to extend the term language
accordingly.  For the minimal logic of ◊,□, one can turn to
Wansing (2002), who develops a set-theoretic interpretation of minimal
temporal intuitionistic logic.  The temporal modalities of future
possibility and past necessity are indistinguishable from the control
operators ◊,□, prooftheoretically and as far as their
relational interpretation is concerned.  Morrill (1990) gives an
interpretation in intensional lambda calculus for a stronger S5
modality, which assumes a universal accessibility relation at the
level of the frame semantics.  Denotations for modalized
formulas <em>A</em>, in this setting, are functions from indices
(situations, reference points) to <em>A</em> denotations.</p>

<p><b>Discussion</b>. The control operators discussed here bear a
strong resemblance to the syntactic control features used in Stabler's
algebraic formulation of Minimalist Grammars (Stabler 1997, 1999).
These features, like ◊,□, come in matching pairs of a
licensor ‘+<em>f</em>’ and a licensee
‘−<em>f</em>’ that have to cancel for a derivation
to be successful; movement is triggered by the presence of an
appropriate licensor feature.  For an explicit comparison of the
mimimalist and the typelogical view on structural control, see Vermaat
(2004).</p>

<p>Whether one wants the full expressivity of modally controlled
structural rules is a matter of debate.  Vermaat (2006) takes the
strong position that the cross-linguistic possibilities for
displacement can be fully captured in terms of the right branch
extraction postulates discussed above, together with their mirror
images for extraction from left branches.  Under this view, which is
consistent with the radical lexicalism of the original categorial
systems, this postulate set is fixed at the level of Universal
Grammar, and variation is reduced to the language-specific lexica that
determine which of the available extraction patterns are
activated.</p>

<h3><a name="LogDis">3.2   The logic of discontinuity</a></h3>

<p>The discontinuous Lambek calculi that have been developed by
Morrill and co-workers (see Morrill <em>et al</em>. 2007, 2009) and
Chapter 6 of the forthcoming monograph (Morrill 2010) extend the
associative Lambek calculus <b>L</b>. We saw that <b>L</b> is the
logic of strings composed by concatenation.  The discontinuous calculi
enrich the ontology with a notion of <em>split</em> strings:
expressions consisting of detached parts, as in the idiom ‘take
— to task’. To build the phrase ‘take someone to
task’, one <em>wraps</em> the discontinuous expression around
its object. In this particular example, there is a single point of
discontinuity, but one can also think of cases with more than one
split point. This naturally leads to two notions of discontinuous
composition: a deterministic view, where wrapping targets a particular
split point, and a non-deterministic view where the targeted split
point is arbitrary. In the case of expressions with a single split
point, these two notions coincide.</p>

<p>The vocabulary of <b>DL</b> (Discontinuous Lambek Calculus)
consists of residuated families of unary and binary type-forming
operations.  A representative sample is given below.  For the binary
case, in addition to the concatenation product of <b>L</b> and the
residual slash operations, we have a discontinuous (wrapping) product
⊙, with residual infixation ↓ and extraction ↑
operations.  For the deterministic interpretation, the discontinuous
type-forming operations have an indexed form
↑<sub><em>k</em></sub>, ⊙<sub><em>k</em></sub>,
↓<sub><em>k</em></sub> explicitly referring to the <em>k</em>-th
split point of their interpretants.  The function of the unary
operations is to control the creation and removal of split points.  As
in the binary case, we have non-deterministic operations
(bridge <sup>∧</sup>, split <sup>∨</sup>) and indexed forms
(<sup>∧</sup><small><em>k</em></small>,<sup>∨</sup><small><em>k</em></small>)
with a deterministic interpretation.</p>

<blockquote>
<em>A</em>,<em>B</em> ::= … | <em>A</em> ⊙ <em>B</em>
| <em>A</em> ↓ <em>B</em> | <em>B</em> ↑ <em>A</em> |
<sup>∧</sup><em>A</em> | <sup>∨</sup><em>A</em>
</blockquote>

<p>On the model-theoretic side, an innovative feature of <b>DL</b> is
the move to a <em>multi-sorted</em> interpretation.  The key notion is
that of a graded algebra: a freely generated algebra
(<em>W</em>,·,1, ⎵) where the monoid
(<em>W</em>,·,1) of the interpretation of <b>L</b>* is
augmented with a distinguished generator ⎵ called the separator.
The <em>sort</em> of an expression <em>s</em>, σ(<em>s</em>), is
given by the number of occurrences of the separator in it. Expressions
of the nullary sort are the familiar strings for the language models
of <em>L</em>. Expressions of sort <em>n</em> &gt; 0 are split
strings, with <em>n</em> marked positions where other expressions can
be substituted.</p>

<p>Interpretation of types is now relativized to sorted
domains <em>W<sub>i</sub></em> = {<em>s</em> | σ(<em>s</em>)
= <em>i</em>} for <em>i</em> ≥ 0. Frames, accordingly, are sorted
structures
({<em>W<sub>i</sub></em>}<sub><em>i</em>∈<em>N</em></sub>,<em>R</em><sub><em>bridge</em></sub>,<em>R</em><sub><em>wrap</em></sub>,·,{<em>bridge<sub>k</sub></em>}<sub><em>k</em>∈<em>N</em>*</sub>,{<em>wrap<sub>k</sub></em>}<sub><em>k</em>∈<em>N</em>*</sub>)
with <em>n</em> + 1-ary relations for the <em>n</em>-ary type-forming
operations with a non-deterministic interpretation and <em>n</em>-ary
operations (functions) for the <em>n</em>-place deterministic
vocabulary items.  The operation · : <em>W<sub>i</sub></em>
× <em>W<sub>j</sub></em>
→ <em>W</em><sub><em>i</em>+<em>j</em></sub> here is the sorted
version of the concatenation operation of <b>L</b>.</p>

<blockquote>
<table cellpadding="+4" cellspacing="+4" style="border-collapse:collapse;">
<tbody><tr>
<td style="border-bottom:1px solid black;">relation/operation</td>
<td style="border-bottom:1px solid black;">interpretation</td>
</tr>

<tr>
<td nowrap="nowrap" valign="top"><em>R<sub>wrap</sub></em> ⊆ <em>W</em><sub><em>i</em>+1</sub>
 × <em>W<sub>j</sub></em> × <em>W</em><sub><em>i</em>+<em>j</em></sub></td>
<td align="left">the smallest relation
s.t. <em>R<sub>wrap</sub></em>(<em>u</em>⎵<em>w</em>,<em>v</em>,<em>uvw</em>)</td>
</tr>

<tr>
<td nowrap="nowrap" valign="top"><em>R</em><sub><em>bridge</em></sub>
⊆ <em>W</em><sub><em>i</em>+1</sub> × <em>W<sub>i</sub></em></td>
<td align="left">the smallest relation s.t. <em>R</em><sub><em>bridge</em></sub>(<em>u</em>⎵<em>v</em>,<em>uv</em>)</td>
</tr>

<tr>
<td valign="top"><em>wrap<sub>k</sub></em> : <em>W</em><sub><em>i</em>+1</sub>
 × <em>W<sub>j</sub></em> → <em>W</em><sub><em>i</em>+<em>j</em></sub></td>
<td><em>wrap<sub>k</sub></em>(<em>s</em>,<em>t</em>) is the result of
replacing the <em>k</em>-th separator in <em>s</em> by <em>t</em></td>
</tr>

<tr>
<td valign="top"><em>bridge<sub>k</sub></em> : <em>W</em><sub><em>i</em>+1</sub>
 → <em>W<sub>i</sub></em></td>
<td><em>bridge<sub>k</sub></em>(<em>s</em>) is the result of replacing
the <em>k</em>-th separator in <em>s</em> by 1</td>
</tr>

</tbody></table>
</blockquote>
   
<p> An interpretation for <b>DL</b> associates atomic types of
sort <em>i</em> to subsets of <em>W<sub>i</sub></em>. Interpretation
clauses for the new complex types are standard. In the light of the
illustrations below, we give the clauses for non-deterministic
bridge/split, and the wrapping family. The sort of the types can be
readily computed from the sort information for the interpreting
operations/relations.</p>
    
<ul>

<li><em>s</em> ⊩ <sup>∧</sup><em>A</em> iff
∃<em>t</em>(<em>R<sub>bridge</sub></em>(<em>t</em>,<em>s</em>)
and <em>t</em> ⊩ <em>A</em>)</li>
    
<li><em>t</em> ⊩ <sup>∨</sup><em>B</em> iff
∀<em>s</em>(<em>R<sub>bridge</sub></em>(<em>t</em>,<em>s</em>)
implies <em>s</em> ⊩ <em>B</em>)</li>
    
<li><em>s</em> ⊩ <em>A</em> ⊙ <em>B</em> iff
∃<em>s</em>′<em>s</em>″(<em>R<sub>wrap</sub></em>(<em>s</em>′,s″,<em>s</em>)
and <em>s</em>′ ⊩ <em>A</em> and <em>s</em>″ ⊩ <em>B</em>)</li>
    
<li><em>s</em>″ ⊩ <em>A</em> ↓ <em>C</em> iff ∀<em>s</em>′<em>s</em>(if <em>R<sub>wrap</sub></em>(<em>s</em>′,<em>s</em>″,<em>s</em>) and <em>s</em>′ ⊩ <em>A</em>, then <em>s</em> ⊩ <em>C</em>)</li>
    
<li><em>s</em>′ ⊩ <em>C</em> ↑ <em>B</em> iff
 ∀<em>s</em>″<em>s</em>(if <em>R<sub>wrap</sub></em>(<em>s</em>′,<em>s</em>″,<em>s</em>) and <em>s</em>″ ⊩ <em>B</em>, then <em>s</em> ⊩ <em>C</em>)</li>

</ul>

<p>On the proof-theoretic side, a cut-elimination theorem for a
sequent presentation of <b>DL</b> establishes decidability.
The <b>DL</b> sequent rules are shown to be sound with respect to the
intended interpretation; a completeness result so far has not been
obtained.  For the mapping from the syntactic source
calculus <b>DL</b> to the semantic target calculus <b>LP</b>, the
unary type-forming operations are considered inert: the inference
rules for these connectives, consequently, leave no trace in
the <b>LP</b> proof term associated with a derivation in the syntactic
source calculus. The continuous and discontinuous families, for the
rest, are treated exactly alike.  Specifically, the infixation and
extraction operations are mapped to <b>LP</b> function types, like the
slashes.</p>

<p><b>Illustration</b>.  <b>DL</b> has been successfully applied to a great
number of discontinuous dependencies, both of the overt and of the
covert type. The non-deterministic operations have been used to model
particle shift and complement alternation constructions.  The
deterministic operations of sort 1 (single split point) are used in
the analysis of non-peripheral extraction, discontinuous idioms,
gapping and ellipsis, quantifier scope construal, reflexivisation,
pied-piping and the Dutch cross- serial dependencies, among
others.</p>

<p>We illustrate the non-deterministic use of <b>DL</b> with English
particle shift, using labeled natural deduction format to display
derivations, with items <em>Form-Meaning:Type</em>. A verb-particle
combination ‘call — up’ can be lexically typed as
<sup>∨</sup>(<em>np</em>\<em>s</em>) ↑ <em>np</em> of sort 2, with an
internal split point, and a right-peripheral one. Elimination of the
non-deterministic extraction operation ↑ offers a choice as to
whether wrapping will affect the first or the second split point. The
first option is displayed below. The remaining separator is removed in
the elimination step for <sup>∨</sup>, with the continuous verb phrases
‘call Mary up’ or ‘call up Mary’ as
result.</p>

<blockquote>
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr>
<td align="center" style="border-bottom:1px solid black;">called ⋅ ⎵ ⋅ up ⋅ ⎵
−<em>phone</em> : <sup>∨</sup>(<em>np</em>\<em>s</em>)
↑ <em>np</em>    Mary − <em>m</em> : <em>np</em></td>
<td valign="middle" rowspan="2"><em> E</em>↑<br /> </td>
</tr>

<tr>
<td align="center">
  <table border="0" cellpadding="0" cellspacing="0">
  <tbody><tr>
  <td align="center" style="border-bottom:1px solid black;">called ⋅ Mary ⋅ up ⋅ ⎵
− (<em>phone</em> <em>m</em>) : <sup>∨</sup>(<em>np</em>\<em>s</em>)</td>
  <td valign="middle" rowspan="2"> <em>E</em><sup>∨</sup></td>
  </tr>

  <tr>
  <td align="center">called ⋅ Mary ⋅ up −
 (<em>phone</em> <em>m</em>) : <em>np</em>\<em>s</em></td>
  </tr>
  </tbody></table>
</td>
</tr>
</tbody></table>
</blockquote>

<p>For an example involving covert discontinuity, consider quantifier
scope construal.  <b>DL</b> provides a uniform type assignment to
generalized quantifier expressions such as ‘everyone’,
‘someone’: (<em>s</em> ↑ <em>np</em>)
↓ <em>s</em>.  In the syntactic source calculus, this type
assignment allows a quantifier phrase QP to occupy any position that
could be occupied by a regular non-quantificational noun phrase.
Semantically, the image of the ↑ Introduction rule at the level
of the semantic target calculus <b>LP</b> binds an <em>np</em> type
hypothesis at the position that was occupied by the QP (the <em>a</em>
− <em>x</em> : <em>np</em> premise, with <em>a</em>
and <em>x</em> structural and semantic variables for the <em>np</em>
hypothesis).  The image of the ↓ Elimination rule applies the
term representing the QP meaning to this abstract.  Scope ambiguities
arise from derivational ambiguity in the source
calculus <b>DL</b>. The derivation below results in a non-local
reading ‘there is a particular <em>x</em> such that Mary
thinks <em>x</em> left’.  Looking upward from the conclusion,
the last rule applied is ↓ Elimination, which means the QP takes
scope at the main clause level.  An alternative derivation, producing
the local scope reading, would have the / Elimination rule for
‘thinks’: (<em>np</em>\<em>s</em>)/<em>s</em> as the last
step.</p>

<!--pdf exclude begin-->
<blockquote>
<!--pdf exclude end-->
<!--pdf include
<small>
pdf include-->
<table border="0" cellpadding="0" cellspacing="0">
<tbody><tr>
<td></td>
<td align="center">
  <table border="0" cellpadding="0" cellspacing="0">
  <tbody><tr>
  <td align="center">
    <table border="0" cellpadding="0" cellspacing="0">
    <tbody><tr>
    <td align="center" style="border-bottom:1px solid black">…  <em>a</em> − <em>x</em> : <em>np</em>  …</td></tr>

    <tr>
    <td align="center">⋮</td></tr>
    </tbody></table>
  </td></tr>

  <tr>
  <td align="center" style="border-top:1px solid black;">Mary ⋅ thinks ⋅ <em>a</em> ⋅ left − ((<em>thinks</em> (<em>left</em> <em>x</em>)) <em>m</em>) : <em>s</em></td></tr>
  </tbody></table>
</td>
<td rowspan="5">
 <br />
 <br />
 ↑<em>I</em><br />
 ↓<em>E</em>
</td></tr>

<tr>
<td>someone − ∃ : (<em>s</em> ↑ <em>np</em>)
↓ <em>s</em> </td>
<td style="border-top:1px solid black;">Mary ⋅ thinks ⋅ ⎵
⋅ left − λ<em>x</em>.((<em>thinks</em>
(<em>left</em> <em>x</em>)) <em>m</em>) : <em>s</em> ↑ <em>np</em></td>
</tr>

<tr>
<td align="center" colspan="2" style="border-top:1px solid black;">Mary ⋅ thinks ⋅ someone ⋅ left −
(∃ λ<em>x</em>.((<em>thinks</em>
(<em>left</em> <em>x</em>)) <em>m</em>)) : <em>s</em></td>
</tr>

</tbody></table>
<!--pdf include
</small>
pdf include-->
<!--pdf exclude begin-->
</blockquote>
<!--pdf exclude end-->

<p><b>Discussion</b>.  The basis for the <b>DL</b> extensions is the
associative calculus <b>L</b>. As we saw above, a global insensitivity
to phrase structure is a source of overgeneration, unless blocked by
explicit island modalities.  In the development of <b>DL</b> nothing
seems to hinge on the associativity of the base system: it would seem
entirely feasible, in other words, to develop <b>DL</b> as an
extension of a non-associative basis, allowing for the representation
of constituent-structure information. In the interpreting frames, one
would then start from a graded <em>groupoid</em> rather than a
monoid. The fact that the <b>DL</b> framework can readily accommodate
a string or tree-based point of view testifies to the versatility of
the approach.</p>

<h3><a name="SymCatGra">3.3   Symmetric categorial grammar</a></h3>

<p>The extensions of the Syntactic Calculus that we studied in the
previous sections all obey an “intuitionistic”
restriction: in statements
<em>A</em><sub>1</sub>,…,<em>A<sub>n</sub></em>
⊢ <em>B</em> the antecedent can consist of multiple formulas
(configured as a ⊗ tree in the non-associative case, a list or
a multiset in the case of <b>L</b>, <b>LP</b>), the succedent is a
<em>single</em> formula.</p>

<p>In a remarkable paper antedating Linear Logic by five years,
Grishin (1983) presents a number of extensions of the Lambek calculus
with the common characteristic that derivability holds between
hypotheses <em>A</em><sub>1</sub>,…,<em>A<sub>n</sub></em>,
taken together by means of a multiplicative conjunction,
and <em>multiple</em>
conclusions <em>B</em><sub>1</sub>,…,<em>B<sub>m</sub></em>,
combined by means of a multiplicative disjunction. Linguistic
exploration of Grishin's ideas has started in recent years.  In this
section, we introduce a multiple-conclusion categorial system that is
already well-studied: <b>LG</b> (for Lambek-Grishin calculus).  We
present syntax and relational semantics of the system; in the next
section, we turn to its computational interpretation.</p>
                                                                                                         
<blockquote>
<table cellpadding="+7" cellspacing="+7" style="border:1px solid black;">
<tbody><tr>
<td>preorder laws</td>
<td align="center"><em>A</em> ⊢ <em>A</em></td>
<td align="center">
<table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>A</em>
 ⊢ <em>B</em>      <em>B</em> ⊢ <em>C</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>A</em> ⊢ <em>C</em></td>
</tr>
</tbody></table>
</td>
</tr>

<tr>
<td>(dual) residuation laws)</td>
<td align="center">
<table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:double;text-align:center;"><em>A</em>
 ⊢ <em>C</em>/<em>B</em></td> 
</tr> 

<tr> 
<td style="padding:0px;border-bottom:double;text-align:center;"><em>A</em>
 ⊗ <em>B</em> ⊢ <em>C</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>B</em> ⊢ <em>A</em>\<em>C</em></td>
</tr>
</tbody></table>
</td>

<td align="center">
<table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:double;text-align:center;"><em>B</em>
 ⦸ <em>C</em> ⊢ <em>A</em></td> 
</tr> 

<tr> 
<td style="padding:0px;border-bottom:double;text-align:center;"><em>C</em>
 ⊢ <em>B</em> ⊕ <em>A</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>C</em>
 ⊘ <em>A</em> ⊢ <em>B</em></td>
</tr>
</tbody></table>
</td>
</tr>

<tr>
<td>interaction principles</td>
<td align="center">
<table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>A</em>
 ⊗ <em>B</em> ⊢ <em>C</em> ⊕ <em>D</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>C</em>
 ⦸ <em>A</em> ⊢ <em>D</em> / <em>B</em></td>
</tr>
</tbody></table>
</td>
<td align="center">
<table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>A</em>
 ⊗ <em>B</em> ⊢ <em>C</em> ⊕ <em>D</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>B</em>
 ⊘ <em>D</em> ⊢ <em>A</em> \ <em>C</em></td>
</tr>
</tbody></table>
</td>
</tr>

<tr>
<td></td>
<td align="center">
 <table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>A</em>
 ⊗ <em>B</em> ⊢ <em>C</em> ⊕ <em>D</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>C</em> ⦸ <em>B</em>
 ⊢ <em>A</em> \ <em>D</em></td>
</tr>
</tbody></table>
</td>

<td align="center">
<table style="border-spacing:0px;"> 
<tbody><tr> 
<td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>A</em> ⊗ <em>B</em>
 ⊢ <em>C</em> ⊕ <em>D</em></td> 
</tr> 

<tr> 
<td style="padding:0px; text-align:center;"><em>A</em> ⊘ <em>D</em> ⊢ <em>C</em> / <em>B</em></td>
</tr>
</tbody></table>
</td>

</tr>
</tbody></table>
<div align="left">The Lambek-Grishin calculus <b>LG</b></div>
</blockquote>

<p><b>LG</b> consists of a symmetric version of the pure residuation
logic <b>NL</b> together with structure-preserving interaction
principles relating the conjunctive and disjunctive operations.  We
discuss these components in turn. For symmetry, the inventory of
type-forming operations is doubled: in addition to the <b>NL</b>
operations ⊗,\,/ (product, left and right division), there is a
second family ⊕,⊘,⦸: coproduct, right and left
difference.  The two families are related by an arrow reversal
symmetry δ, which translates type formulas according to the
table below.</p>

<blockquote>
<table>
<tbody><tr>
<td valign="middle">δ</td>
<td> </td>
<td align="center">
  <table style="border-spacing:0px;"> 
    <tbody><tr> 
      <td style="padding:0px;border-bottom:double;text-align:center;"><em>C</em>/<em>B</em>
      <em>A</em> ⊗ <em>B</em>       <em>A</em>\<em>C</em></td>
    </tr> 

    <tr> 
      <td style="padding:0px; text-align:center;"><em>B</em>
 ⦸ <em>C</em>    <em>B</em> ⊕ <em>A</em>
  <em>C</em> ⊘ <em>A</em></td>
    </tr>
  </tbody></table>
</td>
</tr>
</tbody></table>
</blockquote>

<p>At the level of derivability, we then have A ⊢ B iff
δ(B) ⊢ δ(A): for every theorem or rule of <b>NL</b>,
we also find its image under δ in <b>LG</b>. A note about the
notation: we read <em>B</em> ⦸ <em>A</em> as ‘<em>B</em>
from <em>A</em>’ and <em>A</em> ⊘ <em>B</em> as
‘<em>A</em> less <em>B</em>’, i.e., the quantity that is
subtracted is put under the circled (back)slash, just as we have the
denominator under the (back)slash in the case of left and right
division types. In a formulas-as-types spirit, we will feel free to
refer to the division operations also as implications, and to the
difference operations as coimplications.</p>

<p>Communication between the product and coproduct families requires
the addition of interaction principles to the (dual) residuation laws.
The principles above take the form of inference rules obtained from
the following recipe: from <em>A</em> ⊗ <em>B</em>
⊢ <em>C</em> ⊕ <em>D</em> in the premise, one selects a
product and a coproduct term; in the conclusion,
one <em>simultaneously</em> introduces the residual operations for the
remaining two terms. Using the (dual) residuation laws, one derives
the following patterns from the interaction principles.
Alternatively, one can take (P1)–(P4) as primitive postulates,
and obtain the interaction principles as derived inference rules,
using transitivity and the (dual) residuation laws.</p>

<blockquote>
<table cellpadding="+4" cellspacing="+4">
<tbody><tr>
<td>(P1)</td>
<td>(<em>A</em> ⦸ <em>B</em>) ⊗ <em>C</em>
⊢ <em>A</em> ⦸ (<em>B</em> ⊗ <em>C</em>)</td>
</tr>

<tr>
<td>(P2)</td>
<td><em>C</em> ⊗ (<em>A</em> ⦸ <em>B</em>)
⊢ <em>A</em> ⦸ (<em>C</em> ⊗ <em>B</em>)</td>
</tr>

<tr>
<td>(P3)</td>
<td><em>C</em> ⊗ (<em>B</em> ⊘ <em>A</em>) ⊢
(<em>C</em> ⊗ <em>B</em>) ⊘ <em>A</em></td>
</tr>

<tr>
<td>(P4)</td>
<td>(<em>B</em> ⊘ <em>A</em>) ⊗ <em>C</em> ⊢
(<em>B</em> ⊗ <em>C</em>) ⊘ <em>A</em></td>
</tr>

</tbody></table>
</blockquote>

<p>Derivability patterns of the form (P1)–(P4) have been called
linear distributivity principles — linear, because they do not
duplicate any of the terms involved. In <b>LG</b>, in addition to
being linear, they respect the word order and phrase structure
information encoded in the non-commutative, non-associative
type-forming operations.</p>

<p><b>Illustration</b>. The interaction principles of <b>LG</b>
provide a particularly direct way of capturing phenomena that depend
on <em>infixation</em> rather than concatenation.  The derivations
below (using monotonicity, and (dual) residuation steps) show how the
same pieces of information (the same premises) can be used either to
introduce an implication <em>B</em>\<em>C</em> on the left of the
turnstile, or a coimplication <em>B</em> ⊘ <em>C</em> on the
right.  The first option leads to a rule of Application — the
central composition operation of standard Lambek calculus. The second
option leads to a Co-Application variant.  Although these two rules
are derived from the same premises, there is an important difference
between them.  When the implication <em>B</em>\<em>C</em> composes
with its argument, it must stay external to <em>X</em>.  In the case
of the coimplication, when <em>X</em> is some product of
factors <em>A</em><sub>1</sub>,…,<em>A<sub>n</sub></em>, the
conditions for the interaction principles are met.  This means that
the coimplication <em>B</em> ⊘ <em>C</em> will be able to
descend into the phrase <em>X</em> and associate with any of the
constituent parts <em>A<sub>i</sub></em> into a formula (<em>B</em>
⊘ <em>C</em>) ⦸ <em>A<sub>i</sub></em>.</p>

<blockquote>
<table>
<tbody><tr>
<td>
  <table style="border-spacing:0px;"> 
    <tbody><tr> 
      <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>X</em>
 ⊢ <em>B</em>   <em>C</em> ⊢ <em>Y</em></td>
    </tr> 

    <tr> 
      <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>B</em>\<em>C</em>
 ⊢ <em>X</em>\<em>Y</em></td>
    </tr> 
    
    <tr> 
      <td style="padding:0px; text-align:center;"><em>X</em>
 ⊗ (<em>B</em>\<em>C</em>) ⊢ <em>Y</em></td>
    </tr>
  </tbody></table>
</td>
<td>   </td>
<td>
  <table style="border-spacing:0px;"> 
    <tbody><tr> 
      <td style="padding:0px;border-bottom:1px solid black;&#10;      text-align:center;"><em>X</em> ⊢ <em>B</em>
        <em>C</em> ⊢ <em>Y</em></td>
    </tr> 
    
    <tr> 
      <td style="padding:0px;border-bottom:1px solid black; text-align:center;"><em>X</em>
 ⊘ <em>Y</em> ⊢ <em>B</em> ⊘ <em>C</em></td>
    </tr> 

    <tr> 
      <td style="padding:0px; text-align:center;"><em>X</em>
 ⊢ (<em>B</em> ⊘ <em>C</em>) ⊕ <em>Y</em></td>
    </tr>
  </tbody></table>
</td>
</tr>
</tbody></table>
</blockquote>

<p>In general, an expression of type (<em>B</em> ⊘ <em>C</em>)
⦸ <em>A</em> behaves locally as an <em>A</em> within a context
of type B; it then acts as a function transforming <em>B</em>
into <em>C</em>. We illustrate with the example of non-local scope
construal for which we have seen the <b>DL</b> analysis in the previous
section. The key point is the lexical type assignment to the
generalized quantifier expression, instantiating (<em>B</em>
⊘ <em>C</em>) ⦸ <em>A</em> as (<em>s</em>
⊘ <em>s</em>) ⦸ <em>np</em>. The semantic interpretation
of this derivation will be discussed below.</p>

<blockquote>
<table cellpadding="0" cellspacing="0">
<tbody><tr><td align="center" colspan="8">  
 <em>np</em> ⊗ (((<em>np</em>\<em>s</em>)/<em>s</em>) ⊗ (<em>np</em> ⊗ (<em>np</em>\<em>s</em>)))
   ⊢ <em>s</em>     <em>s</em> ⊢ <em>s</em>
</td></tr>

<tr><td align="center" style="border-top:1px solid black;" colspan="8">
 <em>np</em> ⊗ (((<em>np</em>\<em>s</em>)/<em>s</em>) ⊗ (
<span style="border:1px solid black;"> <em>np</em> </span> ⊗ (<em>np</em>\<em>s</em>)))
 ⊢ 
<span style="border:1px solid black;"> (<em>s</em> ⊘ <em>s</em>) </span>  ⊕ <em>s</em>
</td></tr>

<tr><td align="center" style="border-top:1px solid black; border-bottom:1px solid black;" colspan="8">
⋮
</td></tr>

<tr>
<td align="center"><em>np</em></td>
<td align="center"> ⊗ </td>
<td align="center">(((<em>np</em>\<em>s</em>)/<em>s</em>)</td>
<td align="center"> ⊗ </td>
<td align="center"> ((
<span style="border:1px solid black;"> (<em>s</em> ⊘ <em>s</em>) </span> ⦸ 
<span style="border:1px solid black;"> <em>np</em> </span>)</td>
<td> ⊗ </td>
<td align="center">(<em>np</em>\<em>s</em>))</td>
<td align="center">) ⊢ <em>s</em></td>
</tr>

<tr>
<td align="center" style="border-top:2px solid black;">Alice</td>
<td></td>
<td align="center" style="border-top:2px solid black;">thinks</td>
<td></td>
<td align="center" style="border-top:2px solid black;">someone</td>
<td></td>
<td align="center" style="border-top:2px solid black;">left</td>
<td></td>
</tr>
</tbody></table>

</blockquote>

<p><b>Completeness, decidability</b>. Relational models for <b>LG</b>
are given in terms of two interpreting
relations: <em>R</em><sub>⊗</sub> for multiplicative
conjunction (merge, fusion), and <em>R</em><sub>⊕</sub> for
multiplicative disjunction (fission). The truth conditions for
co-product and the difference operations are given below.</p>

<ul>

<li><em>x</em> ⊩ <em>A</em> ⊕ <em>B</em> iff
 ∀<em>yz</em>(if <em>R</em><sub>⊕</sub><em>xyz</em>, then
 either <em>y</em> ⊩ <em>A</em> or <em>z</em> ⊩ <em>B</em>)</li>

<li><em>y</em> ⊩ <em>C</em> ⊘ <em>B</em> iff
 ∃<em>xz</em>(<em>R</em><sub>⊕</sub><em>xyz</em> and
 not-(<em>z</em> ⊩ <em>B</em>) and <em>x</em> ⊩ <em>C</em>)</li>

<li><em>z</em> ⊩ <em>A</em> ⦸ <em>C</em> iff
 ∃<em>xy</em>(<em>R</em><sub>⊕</sub><em>xyz</em>
 and not-(<em>y</em> ⊩ <em>A</em>) and <em>x</em> ⊩ <em>C</em>)</li>

</ul>

<p>Completeness of LG with respect to this interpretation is proved in
Kurtonina and Moortgat 2010.  The minimal symmetric system (without
interaction principles) has fission <em>R</em><sub>⊕</sub> and
merge <em>R</em><sub>⊗</sub> as distinct relations, without
imposing restrictions on their interpretation.  In the presense of the
interaction principles, their interpretation is related in terms of
frame constraints. The distributivity principle (<em>A</em>
⦸ <em>B</em>) ⊗ <em>C</em> ⊢ <em>A</em> ⦸
(<em>B</em> ⊗ <em>C</em>), for example, corresponds to the
constraint that for
every <em>x</em>,<em>y</em>,<em>z</em>,<em>w</em>,<em>v</em>, if we
have a configuration <em>R</em><sub>⊗</sub><em>xyz</em>
and <em>R</em><sub>⊕</sub><em>vwy</em>, then there exists an
alternative internal point <em>t</em> such
that <em>R</em><sub>⊕</sub><em>twx</em>
and <em>R</em><sub>⊗</sub><em>tvz</em>.  For decidability,
Moortgat (2009) gives a sequent presentation of <b>LG</b> in the
format of a Display Calculus allowing cut-free proof search.</p>

<p><b>Discussion</b>. <b>LG</b> is a weaker logic than <b>CNL</b>, the
Classical Non-Associative Lambek Calculus of de Groote and Lamarche
2002. In the latter system, like in classical linear logic, we have an
involutive negation and De Morgan dualities turning multiplicative
conjunction (times) and disjunction (par) into interexpressible
operations. With respect to the linguistic applications, one can
compare
<b>LG</b> with the Discontinuous Calculus of §3.2.  Whereas the
latter provides uniform analyses of both the extraction and the
infixation type of discontinuities, the distributivity principles
of <b>LG</b> are primarily addressing the infixation variety.  It may
be interesting in this respect to note that Grishin 1983 proposes a
second group of distributivity principles — converses of the
ones discussed above.  Whether these could play a role in the analysis
of overt displacement remains to be seen.  From a formal point of
view, each of the groups of distributivities is a conservative
extension of the basic symmetric calculus.  But the combination of the
two (i.e., the distributivity principles as <em>invertible</em> rules)
induces partial associativity/commutativity of the (co)product
operations, i.e., structure-preservation is lost.</p>

<h3><a name="FleIntConSem">3.4   Flexible interpretation, continuation semantics</a></h3>

<p>In the previous sections, the emphasis was on extending the
capabilities of the syntactic source calculi. In this section, we look
at developments that put more structure in the mapping between the
source and target calculi.</p>

<p>The syntax-semantics mapping, as discussed in §2.2,
is <em>rigid</em>: once we have determined its action on the atomic
types of the syntactic source calculus, everything is fixed. This
rigidity is problematic as soon as we have a class of syntactic
expressions with a non-uniform contribution to meaning assembly.  Noun
phrases are a case in point.  Mapping <em>np</em> to semantic
type <em>e</em> is appropriate for proper nouns and definite
descriptions whose denotations can be taken to be individuals.  But
for quantifying expressions like ‘someone’, ‘no
student’, an interpretation of type <em>e</em> is inadequate.
The rigid syntax-semantics mapping thus forces one to assign these
expressions a higher-order type in the syntax, for
example <em>s</em>/(<em>np</em>\<em>s</em>), so as to obtain a
semantic type with the right kind of denotation as the translation
image, (<em>e</em> → <em>t</em>) → <em>t</em>. If one wants
to avoid this semantically motivated complication of the syntax, one
could set <em>np</em>′ = (<em>e</em> → <em>t</em>)
→ <em>t</em>. But now the effect will be that simple transitive
verbs (<em>np</em>\<em>s</em>)/<em>np</em>, which one would like to
treat as binary relations <em>e</em> → <em>e</em>
→ <em>t</em> by default, are associated with third-order
interpretations instead.</p>

<p><b>Flexible interpretation</b>. An attractive alternative to this rigid
view of the syntax-semantics interface, is the flexible interpretation
of Hendriks 1993.  In this approach, a type of the syntactic source
calculus is associated with an infinite set of <b>LP</b> types: one of
these is the default semantic type associated with the syntactic
source type, the others are derived from this default by means
of <em>type shifting</em> laws. Similarly, a source calculus term is
translated into an infinite set of target semantic terms; one of these
is the <em>basic</em> translation (not necessarily of the default
semantic type for the relevant syntactic category), the others are
derived by the Curry-Howard term image of the typing shifting laws.
The type shifting principles used are (i) argument lowering, (ii)
value raising, and (iii) argument raising. The first two correspond to
valid type transitions of <b>NL</b>; argument raising is the crucial
principle that goes beyond the expressivity of the syntactic source
calculi, as we saw in §2.2.</p>

<p>As an example of flexible interpretation, compare ‘John loves
Mary’ and ‘Everyone loves someone’.  Syntactically,
all noun phrases are typed as <em>np</em>, and the verb as
(<em>np</em>\<em>s</em>)/<em>np</em>. ‘John’ and
‘Mary’ are interpreted with constants of type <em>e</em>,
‘loves’ with a constant of type <em>e</em>
→ <em>e</em> → <em>t</em>, the default semantic types for
the syntactic source types.  But ‘everyone’ and
‘someone’ are mapped to constants of type (<em>e</em>
→ <em>t</em>) → <em>t</em>. Interpreting the syntactic
derivation for ‘Everyone loves someone’, the semantic
types no longer match for simple function application: the verb
expects two <em>e</em> type arguments, but instead finds two arguments
of type (<em>e</em> → <em>t</em>) → <em>t</em>. There are
two ways of accommodating these higher-order arguments by means of
argument raising: depending on whether one raises first the object
argument and then the subject, or vice versa, one obtains two readings
for one and the same syntactic derivation: a reading corresponding to
the surface order, with ∀ taking scope over ∃, and a
reading with the inverted scope order. The correspondence between
syntax and semantics, under this flexible view, has become
a <em>relation</em>, rather than being <em>functionally</em>
determined by the translation homomorphism of §2.2.</p>

<p><b>Continuations</b>.  It has become clear in recent years, that
the core ideas of Hendriks' type-shifting account of derivational
ambiguity can be insightfully recast in terms of a
continuation-passing-style interpretation (CPS), as developed within
computer science, and the different <em>evaluation strategies</em>
available for such interpretation. The use of continuations in natural
language semantics has been pioneered by de Groote (2001b) and Barker
(2002). In the theory of programming languages, a continuation is a
representation of the control state, i.e., the future of the
computation to be performed. By adding the control state as an
explicit parameter in the interpretation, it becomes possible for a
program to manipulate its continuation, and thus to express control
constructs that would otherwise be unavailable.  Technically,
continuation semantics makes use of a designated response type for the
ultimate result of a computation; a continuation for an expression of
type <em>A</em> is a function that takes an <em>A</em> value to the
response type. In the application to natural language semantics, the
response type is generally identified with the type of truth
values <em>t</em>, i.e., the type assigned to complete sentences.</p>

<p>Barker 2004 shows how to obtain the type-shifting scope ambiguity
for ‘Everyone loves someone’ in terms of a
continuation-passing-style translation.  The key idea is that in
mapping the source calculus to the semantic target calculus, all types
are parameterized with an extra continuation argument.  We use a new
translation function (·)* for this, which composes the mapping
from syntactic to semantic types with the continuization.  A source
language type <em>A</em> is now associated with a so-called
<em>computation</em> in the target language, i.e., a function acting
on its own continuation: <em>A</em>* = (<em>A</em>′
→ <em>t</em>) → <em>t</em>.</p>

<p>At the level of <em>proofs</em>, given the above interpretation of
types, the task is to find an <b>LP</b> proof for the sequent below,
the image of the \ and / elimination rules
for <em>A</em>, <em>A</em>\<em>B</em> ⊢ <em>B</em>
and <em>B</em>/<em>A</em>, <em>A</em> ⊢ <em>B</em>.
Whereas in the source calculus there is only one way of putting
together an <em>A</em>\<em>B</em> (or <em>B</em>/<em>A</em>) function
with its
<em>A</em> argument, in the target calculus there is a choice as to
the <em>evaluation order</em>: do we want to first evaluate the
translation image of the argument, then that of the function, or the
other way around.  We write ·<sup><em>v</em></sup> for the
first option (call-by-value) and ·<sup><em>n</em></sup> for the
second (call-by-name).  In the target language, <em>m</em>
and <em>n</em> are variables of type <em>A</em>′
→ <em>B</em>′ and <em>A</em>′
respectively; <em>k</em> is the resulting <em>B</em>′ →
<em>t</em> continuation.</p>

<ul>

<li><b>LP</b> translation of Application:<br />(<em>A</em>′
→ <em>t</em>) → <em>t</em>, ((<em>A</em>′
→ <em>B</em>′) → <em>t</em>) → <em>t</em> ⊢
(<em>B</em>′ → <em>t</em>) → <em>t</em></li>
    
<li>call-by-value solution:<br />(<em>M</em>
⊲ <em>N</em>)<sup><em>v</em></sup> = (<em>N</em>
⊳ <em>M</em>)<sup><em>v</em></sup> =
λ<em>k</em>.(<em>N</em><sup><em>v</em></sup>
λ<em>n</em>.(<em>M</em><sup><em>v</em></sup> λ<em>m</em>.(<em>k</em> (<em>m</em>
<em>n</em>))))</li>
    
<li>call-by-name solution:<br />(<em>N</em>
⊳ <em>M</em>)<sup><em>n</em></sup> = (<em>M</em>
⊲ <em>N</em>)<sup><em>n</em></sup> =
λ<em>k</em>.(<em>M</em><sup><em>n</em></sup>
λ<em>m</em>.(<em>N</em><sup><em>n</em></sup>
 λ<em>n</em>.(<em>k</em> (<em>m</em> <em>n</em>))))</li>

</ul>

<p>The continuation-passing-style interpretation, like the
type-shifting approach, makes it possible to assign syntactic
type <em>np</em> both to proper names and to quantificational noun
phrases: in the target calculus, the translation of <em>np</em> has
the appropriate semantic type (<em>e</em> → <em>t</em>)
→ <em>t</em>.  But the lifting strategy is now generalized to all
source types: a transitive verb (<em>np</em>\<em>s</em>)/<em>np</em>)
is mapped to ((<em>e</em> → <em>e</em> → <em>t</em>)
→ <em>t</em>) → <em>t</em>, etc.  For the translation of
lexical constants of type <em>A</em>, the default recipe is
λ<em>k</em>.(<em>k</em> <b>c</b>), <b>c</b> a non-logical
constant of type <em>A</em>′.  The default translation simply
passes the semantic value for these lexical items to the continuation
parameter <em>k</em>. But quantificational noun phrases effectively
exploit the continuation parameter: they take <em>scope</em> over
their continuation, leading to lexical recipes
λ<em>k</em>.(∀
λ<em>x</em>.(<em>k</em> <em>x</em>)),
λ<em>k</em>.(∃
λ<em>x</em>.(<em>k</em> <em>x</em>)) for ‘everyone’
and ‘someone’. The choice between the evaluation
strategies, in combination with these lexical recipes, then results in
different interpretations for a <em>single</em> derivation in the
source calculus <em>M</em> = (everyone⊳(loves⊲someone)),
with ·<sup><em>v</em></sup> producing the surface scope
construal, and ·<sup><em>n</em></sup> the inverted scope
reading.</p>

<blockquote>
<table>
<tbody><tr>
<td><em>M</em><sup><em>v</em></sup></td>
<td> = </td>
<td>λ<em>k</em>.(∀ λ<em>x</em>.(∃
 λ<em>y</em>.(<em>k</em> ((<em>loves</em> <em>y</em>) <em>x</em>)))</td>
</tr>

<tr>
<td><em>M</em><sup><em>n</em></sup></td>
<td> = </td>
<td>λ<em>k</em>.(∃ λ<em>y</em>.(∀
 λ<em>x</em>.(<em>k</em> ((<em>loves</em> <em>y</em>) <em>x</em>)))</td>
</tr>
</tbody></table>
</blockquote>

<p>The above example only uses the slash Elimination of <b>NL</b> in
the syntactic source calculus.  An important motivation for the
introduction of continuations is that they make it possible to give a
constructive interpretation to classical (as opposed to
intuitionistic) logic; see Sørensen and Urzyczyn 2006 for
discussion. It will be no surprise, then, that also the
multiple-conclusion symmetric categorial grammar <b>LG</b> has a
natural interpretation in the continuation-passing-style (Bernardi and
Moortgat 2010, Moortgat 2009).  In the example we gave above, source
types <em>A</em> are provided with a single continuation parameter. In
the CPS translation for <b>LG</b> types, the continuization of
syntactic source types is performed recursively.  Let us
write <em>V</em>(<em>A</em>) for target language values of
type <em>A</em>, <em>K</em>(<em>A</em>) for continuations, i.e.,
functions <em>V</em>(<em>A</em>) → <em>R</em>,
and <em>C</em>(<em>A</em>) for computations, i.e.,
functions <em>K</em>(<em>A</em>) → <em>R</em>, where <em>R</em>
is the response type.  For an <b>LG</b> syntactic source
type <em>A</em>, the call-by-value CPS translation yields an <b>LP</b>
value <em>V</em>(<em>A</em>) as follows.  <em>V</em>(<em>p</em>)
= <em>p</em> for atomic types,</p>

<ul>

<li>implications: <em>V</em>(<em>A</em>\<em>B</em>)
= <em>V</em>(<em>B</em>/<em>A</em>) = <em>K</em>(<em>B</em>)
→ <em>K</em>(<em>A</em>)</li>
    
<li>coimplications (dual to implications): <em>V</em>(<em>A</em>
⊘ <em>B</em>) = <em>V</em>(<em>B</em> ⦸ <em>A</em>)
= <em>K</em>(<em>A</em>\<em>B</em>)</li>

</ul>

<p>At the level of proofs (and the terms in Curry-Howard
correspondence with them), the CPS translation turns the
multiple-conclusion source derivations into
single-conclusion <b>LP</b> derivations. The translation respects the
invariant below. An active output formula <em>A</em> (marked off by
the vertical bar in the box below) maps to a computation
<em>C</em>(<em>A</em>), an active input formula to a
continuation <em>K</em>(<em>A</em>). A cut then is interpreted as the
application of <em>C</em>(<em>A</em>) to <em>K</em>(<em>A</em>).</p>

<blockquote>
<table style="border:1px solid black; border-spacing: 0.5em 1em;">
<tbody><tr align="center">
<td>source: <b>LG</b><sub>/,\,⊘,⦸</sub></td>
<td> →<sub>CPS</sub> </td>
<td>target: <b>LP</b><sub>→</sub></td>
</tr>

<tr align="center">
<td><em>X</em> ⊢ <em>A</em> | <em>Y</em></td>
<td></td>
<td><em>V</em>(<em>X</em>),<em>K</em>(<em>Y</em>)
 ⊢ <em>C</em>(<em>A</em>)</td>
</tr>
    
<tr align="center">
<td><em>X</em> | <em>A</em> ⊢ <em>Y</em></td>
<td></td>
<td><em>V</em>(<em>X</em>),<em>K</em>(<em>Y</em>) ⊢ <em>K</em>(<em>A</em>) </td>
</tr>

<tr align="center">
<td><em>X</em> ⊢ <em>Y</em></td>
<td></td>
<td><em>V</em>(<em>X</em>),<em>K</em>(<em>Y</em>) ⊢ <em>R</em></td>
</tr>

</tbody></table>
</blockquote>

<p><b>Discussion</b>.  Continuation-based approaches are now available
for a number of recalcitrant phenomena that would seem to defy a
compositional treatment.  Examples, at the sentential level, include
the treatment of in situ scope construal and <em>wh</em> questions of
Shan and Barker 2006, where crossover and superiority violations are
explained in terms of a preference of the human processor for a
left-to-right evaluation strategy; donkey anaphora (Barker and Shan
2008)); quantifier scope ambiguities for the call-by-value and
call-by-name interpretation of <b>LG</b> are investigated in Bernardi and
Moortgat 2010. At the discourse level, de Groote (2006) gives a
type-theoretic analysis of dynamic phenomena, modeling propositions as
functions over a sentence's left and right context (continuation).</p>

<h2><a name="ProNetPro">4.  Proof nets and processing</a></h2>

<p>In the previous sections, we have seen how the different categorial
calculi can be presented as proof systems with a sequent-based
decision procedure.  Naive proof search in these systems, however, is
inefficient from a computational point of view: in general, there will
be multiple ways to construct alternative proofs that differ only in
the order of the application of the inference rules, but produce the
same interpretation (the <b>LP</b> term associated with a derivation
under the interpretation homomorphism). This issue of
‘spurious’ ambiguity can be tackled by the introduction of
normal form derivations (as in Hepple 1990, Hendriks 1993), compare
focused proof search regimes in linear logic), combined with the use
of chart-based parsing methods, as in Hepple 1999, Capelletti 2007. An
alternative for categorial ‘parsing-as-deduction’ is to
leave the sequent calculus for what it is, and to switch to
a <em>proof net</em> approach.  Proof nets, originally developed in
the context of linear logic, use a representation of derivations that
is inherently free of ‘spurious ambiguity’, i.e., the issue
of irrelevant rule orderings simply doesn't arise. For a general
treatment of proof nets, we refer to the entry on 
 <a href="../logic-linear/">linear Logic</a>.
 Below we comment on aspects that are specifically addressing issues in
categorial grammar.</p>

<p><b>Proof nets for the Lambek calculi</b>. Proof nets for the
associative calculus <b>L</b> and the commutative variant <b>LP</b>
have been studied initially by Roorda (1991, 1992). To capture the
‘intuitionistic’ nature of these systems, one works with
formulas with <em>polarities</em>: input (‘given’)
polarity for antecedent occurrences of a formula, versus output
(‘to prove’) polarity for succedent
occurrences. Correctness criteria identify proof nets among a wider
class of graphs: to the criteria of acyclicity and connectedness,
applicable for linear logic in general, one adds <em>planarity</em> to
capture word order sensitivity: in the proof nets for <b>L</b>, axioms
links cannot cross.  With respect to the syntax-semantics interface,
de Groote and Retoré (1996) show that the lambda terms in
Curry-Howard correspondence with <b>LP</b> derivations in the semantic
target calculus can be read off from a proof net by specifying a set
of ‘travel instructions’ for traversing a net; these
instructions then correspond step-by-step with the construction of the
associated lambda term.</p>

<p><b>Incremental processing</b>. Proof nets, considered statically as
graphs satisfying certain correctness criteria, remove spurious
choices relating to the order of rule applications in sequent calculi:
in this respect, they represent a purely ‘declarative’
view on categorial deductions. Johnson (1998) and Morrill (2000) have
pointed out that an alternative, ‘procedural’ view on the
actual process of constructing a net makes perfect sense as well, and
offers an attractive perspective on performance phenomena. Under this
interpretation, a net is built in a left-to-right incremental fashion
by establishing possible linkings between the input/output literals of
the partial proof nets associated with lexical items as they occur in
real time. This suggests a simple complexity measure on an incremental
traversal, given by the number of unresolved dependencies between
literals.  This complexity measure correlates nicely with a number of
well-attested processing issues, such as the difficulty of center
embedding, garden path effects, attachment preferences, and
preferred scope construals in ambiguous constructions.</p>

<p><b>First-order quantifiers</b>.  The planarity condition singles
out the non-commutative proof nets for <b>L</b> among the <b>LP</b>
nets. To deal with the more structured categorial calculi discussed
here, the correctness criteria have to be refined. One strategy of
doing this is via a translation into MILL1 (first-order multiplicative
intuitionistic linear logic) where one has proof nets with extra links
for existential and universal quantification over first order
variables. One can then use these variables in a way very similar to
the use of position arguments in Definite Clause Grammars as used in
logic programming. Moot and Piazza (2001) work out such translations
for <b>L</b> and <b>NL</b>. For the concatenation operations
of <b>L</b>, one replaces the proposition letters (atomic formulas) by
two-place terms, marking beginning and end of a continuous string. For
non-associative <b>NL</b>, one adds an extra variable to keep track of
the nesting depth of subformulas. For wrapping operations in the
simple discontinuous calculus <b>DL</b> (allowing a single split
point), Morrill and Fadda (2008) use four-place predicates.  In
general, we find a correlation here between the syntactic expressivity
of the calculi and the number of variables needed to encode their
structural resource management.</p>

<p><b>Nets and rewriting</b>.  The multimodal and symmetric calculi of
§3.1 and §3.3 pose a challenge to the proof net methods as
originally developed for linear logic.  In these systems we typically
find <em>one-way</em> structural rules, such as the extraction postulates for
overt displacement, or the Grishin distributivity laws in the case of
<b>LG</b>: these one-way rules naturally suggest a notion of graph
<em>rewriting</em>. A completely general proof net framework for the
extended Lambek calculi, with a correctness criterion based on
rewriting, has been developed in Moot and Puite 2002 and Moot
2007.</p>

<p>The basic building block for the Moot-Puite nets is a generalized
notion of a link, accommodating connectives of any arity.  A link is
determined by its type (tensor or cotensor), its premises (a sequence
<em>P</em><sub>1</sub>,…,<em>P<sub>n</sub></em>, 0
≤ <em>n</em>), its conclusions (a
sequence <em>C</em><sub>1</sub>,…,<em>C<sub>m</sub></em>, 0
≤ <em>m</em>), and its main formula (which can be empty, in the
case of a neutral link, or one of the <em>P<sub>i</sub></em>
or <em>C<sub>i</sub></em>). A <em>proof structure</em> is a set of
links over a finite set of formulas such that every formula is at most
once the premise of a link and at most once the conclusion.  Formulas
which are not the conclusion of any link are the <em>hypotheses</em>
of the proof structures, whereas the formulas which are not the
premise of any link are the conclusions.  An <em>axiom formula</em> is
a formula which is not the main formula of any
link.  <em>Abstract</em> proof structures are obtained by erasing all
formulas on the internal nodes. A <em>proof net</em> is a proof
structure for which the abstract proof structure converts to a tensor
tree — a rooted tree in the case of the intuitionistic systems,
possibly an unrooted tree in the case of symmetric <b>LG</b>.  Proofs
nets, so defined, can then be show to be the graphs that correspond to
valid derivations.</p>

<p>The rewriting steps transforming a candidate abstract proof
structure into a proof net are of two kinds.  The <em>logical
contractions</em> correspond to identities <em>A</em>
⊢ <em>A</em>, for complex formulas <em>A</em>; they reduce a
configuration of a matching tensor and cotensor link to a point.
The <em>structural conversions</em> perform an internal rewiring of a
proof structure with
hypotheses <em>H</em><sub>1</sub>,…,<em>H<sub>n</sub></em> and
conclusions <em>C</em><sub>1</sub>,…,<em>C<sub>m</sub></em> to
a structure with some permutation of the <em>H<sub>i</sub></em> as
hypotheses and some permutation of the <em>C<sub>i</sub></em> as
conclusions. Copying and deletion, in other words, are ruled out. As
an example, the rewriting corresponding to one of the Grishin
interaction principles discussed in §3.3, allowing us to
infer <em>C</em> ⦸ <em>A</em> ⊢ <em>D</em>/<em>B</em>
from <em>A</em> ⊗ <em>B</em> ⊢ <em>C</em>
⊕ <em>D</em>. Hypotheses <em>A</em>, <em>B</em> are on the left,
conclusions <em>C</em>, <em>D</em> on the right.  The net
representation brings out in a particularly clear way that these
principles are structure-preserving: they leave the order of
hypotheses and conclusions untouched.</p>

<blockquote>
<img src="net.png" alt="ABCD net" />
</blockquote>

<h2><a name="RecCapCom">5.   Recognizing capacity, complexity</a></h2>

<p>Reflecting on the above, one can say that the extensions of the
syntactic calculus reviewed here are motivated by the desire to find a
proper balance between <em>expressivity</em> and <em>computational
tractability</em>: on the side of expressivity, an adequate
typelogical framework should be able to recognize patterns beyond the
strictly context-free; ideally, one would like to keep such
expressivity compatible with a polynomial derivability problem.  Among
contemporary ‘lexicalized’ grammar formalisms, there is a
remarkable convergence on systems that meet these requirements: the
so-called ‘mildly context-sensitive’ systems (Joshi,
Vijay-Shanker, and Weir 1991).  Where can we situate the typelogical
systems discussed here with respect to recognizing capacity and
complexity?</p>

<p>The minimal system in the typelogical hierarchy <b>NL</b> has a
polynomial recognition problem (see de Groote 1999, and Capelletti
2007 for actual parsing algorithms), but it is strictly context-free
(Kandulski 1988).  Extensions with global structural rules are
unsatisfactory, both on the expressivity and on the complexity front.
As for <b>L</b>, Pentus (1993b, 2006) shows that it remains strictly
context-free, whereas the addition of global associativity makes the
derivability problem NP complete.  NP completeness already holds for
the product-free fragment of <b>L</b> (Savateev 2009).  Also
for <b>LP</b>, i.e., multiplicative intuitionistic linear logic, we
have NP completeness (Kanovich 1994).  With regard to recognizing
capacity, van Benthem (1995) shows that <b>LP</b> recognizes all
permutation closures of context-free languages: a class which is too
wide from the <em>syntactic</em> point of view.  As the logic of
<em>meaning</em> assembly, <b>LP</b> is a core component of the
typelogical inventory.  But as we saw in the discussion of the
syntax-semantics interface, we can restrict attention to the
sublanguage of <b>LP</b> that forms the image of derivations in
syntactic calculi making interesting claims about word order and
phrase structure.</p>

<p>The situation of the multimodal and symmetric extensions is more
intricate. Expressivity here is directly related to the kind of
restrictions one imposes on structural resource management. At one end
of the spectrum, multimodality without structural rules does not lead
us beyond context-free recognition: Jäger (2003) shows that the
pure residuation logic for <em>n</em>-ary families of type-forming
operations stays strictly context-free. If one requires structural
rules to be <em>resource-sensitive</em> (no copying or deletion) and,
for the unary modalities, <em>non-expanding</em>, one obtains the full
expressivity of context-sensitive grammars, and the PSPACE complexity
that goes with it (Moot 2002). (PSPACE consists of those problems 
solvable using some polynomial amount of memory space.  See the entry on 
 <a href="../computability/">computability and complexity</a>.
 If one imposes no restrictions on structural rules (specifically, if
one allows copying and deletion operations), unsurprisingly, one
obtains the expressivity of unrestricted rewriting systems (Carpenter
1999). A <em>controlled</em> use of copying is used in the analysis of
anaphora resolution in Jäger 2005.</p>

<p>The symmetric calculus <b>LG</b> <em>without</em> interaction
principles is context-free, as shown in Bastenhof 2010.  For the
system <em>with</em> the interaction principles of §3.3, Melissen
(2009) shows that all languages which are the intersection of a
context- free language and the permutation closure of a context-free
language are recognizable in <b>LG</b>. In this class, we find
generalized forms of MIX (the language consisting of an equal number
of symbols <em>a</em>, <em>b</em>, <em>c</em>, in any order), with
equal multiplicity of <em>k</em> alphabet symbols in any order, and
counting dependencies
 <em>a</em><span class="supsub"><em>n</em><span class="lower">1</span></span>…<em>a</em><span class="supsub"><em>n</em><span class="lower"><em>k</em></span></span>
for any number <em>k</em> of alphabet symbols.  Patterns of this type
are recognized by Range Concatenation Grammars and Global Index
Grammars; a comparison with these formalisms then might be useful to
fix the upper bound of the recognizing capacity of <b>LG</b>, which is as yet
unknown.</p>

<p>With respect to computational complexity, Moot (2008) establishes a
correspondence between Lexicalized Tree Adjoining Grammars on the one
hand, and categorial grammars with the multimodal extraction
postulates of §3.1 and a restricted set of <b>LG</b> grammars on
the other. For these grammars he obtains a polynomial parsability
result via a translation into Hyperedge Replacement Grammars.  In the
case of <b>LG</b>, the restriction requires the coimplications
⊘,⦸ to occur in matching pairs in lexical type
assignments.  The lexicon of the generalized MIX construction of
Melissen 2009, and the type assignment used for quantifier phrases in
the analysis of scope construal in Bernardi and Moortgat 2010, do not
respect this restriction. For the general case of <b>LG</b> with the
interaction principles of §3.3, Bransen (2010) establishes
NP-completeness.  The position of the discontinuous calculi of
§3.2 in this hierarchy has to be determined: they
recognize more than the context-free languages, but it is not clear
whether they stay within the mildly context-sensitive family.</p>

<h2><a name="RelApp">6.   Related approaches</a></h2>

<p>Typelogical grammars as discussed here share a number of
characteristics with a number of related formal grammar frameworks
where types also play a key role.  We briefly discuss some of these
relatives, and provide pointers to the literature for readers who want
to explore the connections further.</p>

<p><b>Combinatory Categorial Grammar</b> (CCG).  The CCG framework
(see Steedman 2000 for a comprehensive presentation) takes its name
from the Combinatory Logic of Curry and Feys.  Whereas the design of
typelogical grammars follows a logical agenda (sequent calculus,
soundness, completeness, etc), grammars in the CCG tradition are
presented as finite sets of <em>rules</em> for type change and type
combination.  This rule inventory will typically include function
application schemata, type lifting, and functional composition, both
of the kind valid within associative <b>L</b> (composing implicational
types with the same directionality), and of the mixed kind (composing
functors with different directionality).</p>

<p>The development of CCG and Lambek style typelogical grammars in the
1980s initially followed separate courses. More recently, there are
signs of convergence. An important factor has been the introduction of
the typelogical technique of multimodal control to fine-tune the
applicability of the CCG combinatory schemata (Baldridge 2002, Kruijff
and Baldridge 2003).  Multimodal typelogical grammar, from this
perspective, plays the role of the underlying general logic which one
uses to establish the validity of the CCG combinatory schemata,
whereas the particular choice of primitive combinators is motivated by
computational efficiency considerations (Hoyt and Baldridge 2008). In
terms of generative capacity, CCG is a member of the mildly context
sensitive family of grammar formalisms. Polynomial parsability is
obtained by imposing a bound on the functional composition
schemata.</p>

<p><b>Pregroup grammars</b>.  Pregroups are an algebraic version of
<em>compact</em> bilinear logic (Lambek 1993) obtained by collapsing
the tensor and cotensor operations.  Pregroup grammars were introduced
in Lambek 1999 as a simplification of the original Syntactic
Calculus <b>L</b>. They have since been used to build computational
fragments for a great variety of languages by Lambek and co-workers.
A pregroup is a partially ordered monoid in which each
element <em>a</em> has a left and a right adjoint, <em>a<sup>l</sup></em>,
<em>a<sup>r</sup></em>, satisfying <em>a<sup>l</sup>a</em> → 1
→ <em>aa<sup>l</sup></em> and <em>aa<sup>r</sup></em> → 1
→ <em>a<sup>r</sup>a</em>, respectively.  Type assignment takes
the form of associating a word with one or more elements from the free
pregroup generated by a partially ordered set of basic types.  For the
connection with categorial type formulas, one can use the translations
<em>a</em>/<em>b</em> = <em>ab<sup>l</sup></em>
and <em>b</em>\<em>a</em> = <em>b<sup>r</sup>a</em>. Parsing, in the
pregroup setting, is extremely straightforward. Lambek (1999) proves
that one only has to perform the contractions
replacing <em>a<sup>l</sup>a</em> and <em>a<sup>l</sup>a</em> by the
multiplicative unit 1.  This is essentially a check for
well-bracketing — an operation that can be entrusted to a
pushdown automaton. The expansions 1 → <em>aa<sup>l</sup></em>
and 1 → <em>a<sup>r</sup>a</em> are needed to prove equations
like (<em>ab</em>)<sup><em>l</em></sup>
= <em>b<sup>l</sup>a<sup>l</sup></em>.  We have used the latter to
obtain the pregroup version of the higher-order relative pronoun type
(<em>n</em>\<em>n</em>)/(<em>s</em>/<em>np</em>) in the example
below.</p>

<table cellpadding="+3" cellspacing="+3">
<tbody><tr align="center">
<td></td>
<td>book</td>
<td>that</td>
<td>Carroll</td>
<td>wrote</td>
<td></td>
</tr>

<tr align="center">
<td nowrap="nowrap" align="right">type assignment in <b>L</b> :</td>
<td><em>n</em></td>
<td>(<em>n</em>\<em>n</em>)/(<em>s</em>/<em>np</em>)</td>
<td><em>np</em></td>
<td>(<em>np</em>\<em>s</em>)/<em>np</em></td>
<td></td>
</tr>

<tr align="center">
<td nowrap="nowrap" align="right">pregroup type assignment :</td>
<td><em>n</em></td>
<td><em>n<sup>r</sup></em> <em>n</em> <em>np<sup>ll</sup></em> <em>s<sup>l</sup></em></td>
<td><em>np</em></td>
<td><em>np<sup>r</sup></em> <em>s</em> <em>np<sup>l</sup></em></td>
<td>→ <em>n</em></td>
</tr>
</tbody></table>

<p>Compact bilinear logic is not a conservative extension of the
original Syntactic Calculus.  Every sequent derivable in <b>L</b> has
a translation derivable in the corresponding pregroup, but the
converse is not true: the pregroup image of the types (<em>a</em>
⊗ <em>b</em>)/<em>c</em> and <em>a</em> ⊗
(<em>b</em>/<em>c</em>), for example,
is <em>a b c<sup>l</sup></em>, but these two types are
not interderivable in <b>L</b>.</p>

<p>With respect to generative capacity, Buszkowski (2001) shows that
the pregroup grammars are equivalent to context-free grammars. They
share, in other words, the expressive limitations of the original
categorial grammars. To overcome these limitations different
strategies have been pursued, including lexical rules (metarules),
derivational constraints, controlled forms of commutativity, and
products of pregroups. The <em>Studia Logica</em> special issue
(Buszkowski and Preller 2007) and the monograph Lambek 2008 give a
good picture of current research.</p>

<p><b>Abstract Categorial Grammar</b>.  The ACG framework (de Groote
2001a) is a meta-theory of compositional grammar architectures. ACGs
are built on higher-order linear signatures Σ =
(<em>A</em>,<em>C</em>,τ), where <em>A</em> and <em>C</em> are
finite sets of atomic types and constants respectively, and τ a
function assigning each constant a linear implicative type over
<em>A</em>. Given a source signature Σ and a target signature
Σ′, an interpretation is a mapping form Σ to
Σ′ given by a pair of functions: η mapping the type
atoms of Σ to linear implicative types of Σ′ and
θ mapping the constants of Σ to well-typed linear lambda
terms of Σ′ in a way that is compatible with the mapping
on types. Using the terminology of compiler theory, one refers to the
source and target signatures as the <em>abstract</em> vocabulary and the
<em>concrete</em> vocabulary, respectively, and to the interpretive
mapping as the <em>lexicon</em>. An ACG is then obtained by specifying
an atomic type of Σ as the distinguished type of the
grammar.</p>

<p>In the ACG setting, one can model the syntax-semantics interface in
terms of the abstract versus object vocabulary distinction. But one
can also obtain <em>surface form</em> as the result of an interpretive
mapping, using the canonical λ term encodings of strings and
trees and operations on them. ACG has given rise to an interesting
complexity hierarchy for rewriting grammar formalisms encoded as ACGs:
context-free grammars, tree-adjoining grammars, etc.; see for example
de Groote and Pogodalla 2004.  Expressive power of these formalisms is
measured in terms of the maximal order of the constants in the
abstract vocabulary and of the object types interpreting the atomic
abstract types. The study of ACG encodings of typelogical systems
proper has started with Retoré and Salvati 2010; these authors
present an ACG construction for (product-free) <b>NL</b>.</p>

<p>It will be clear from this description that the ACG architecture is
closely related to the compositional interpretation for categorial
type logics as discussed in this artile. A key difference is the
nature of the ‘abstract syntax’, i.e. the source calculus
from which interpretations are homomorphically derived. In the case of
the standard Lambek systems and the extended systems discussed in
§3 above, the abstract syntax is a <em>directional</em> type
logic; in the case of ACG, one finds <b>LP</b> and the linear lambda
calculus both at the source and at the target end.  The debate as to
whether structural properties of language have to be accounted for at
the level of the abstract syntax has a long history, starting with
Curry 1961; see Muskens 2007 for discussion.  The typelogical view
accounts for word order universals at the level of its <em>logical
constants</em>, i.e., the type-forming operations, and the laws that
govern them.  The ACG view is more liberal in this respect: the
derivation of surface form can be specified on a word-by-word
basis. Whether this more relaxed connection between abstract syntax
and surface realisation is desirable, is a matter of debate. An
attractive feature of ACG that has not been investigated
systematically within the typelogical setting is the connection
between expressivity and order restrictions on the source constants
and on the interpretive mapping.</p>

</div>

<div id="bibliography">

<h2><a name="Bib">Bibliography</a></h2>

<p>
Note: In addition to the regular text references, the bibliography
contains some items that can place the entry in a broader
context.</p>

<ul>

<li>For general logical and mathematical background, see Galatos
et al. 2007, Restall 2000, Sørensen and
Urzyczyn 2006.</li>

<li>For monographs, collections and survey articles on typelogical
grammar, see Buszkowski 1997, Buszkowski et al. 1988,
Carpenter 1998, Jäger 2005, Moortgat 1988, 1997,
Morrill 1994, 2010, Oehrle et al. 1988, van
Benthem 1995.</li>
</ul>

<ul class="hanging">

<li>Baldridge, J. (2002).
<em>Lexically Specified Derivational Control in Combinatory Categorial
Grammar</em>.  Ph. D. thesis, University of Edinburgh.</li>

<li>Barker, C. (2004).  Continuations in natural language.  In
H. Thielecke (Ed.), <em>CW'04: Proceedings of the 4th ACM SIGPLAN
continuations workshop</em>, Tech. Rep. CSR-04-1, School of Computer
Science, University of Birmingham, pp. 1–11.</li>

<li>–––. (2002).  Continuations and the nature of
quantification. <em>Natural language semantics</em>, 10: 211–242.</li>

<li>Barker, C. and C. Shan (2006).  Types as graphs: Continuations in
type logical grammar. <em>Journal of Logic, Language and Information</em>, 15(4):
331–370.</li>

<li>–––. (2008).  Donkey anaphora is in-scope
binding. <em>Semantics and Pragmatics</em>, 1(1): 1–46.</li>

<li>Barry, G., M. Hepple, N. Leslie, and G. Morrill (1991).  Proof
figures and structural operators for categorial grammar.
In <em>Proceedings of the 5th conference on European chapter of the
Association for Computational Linguistics</em>, 
Association for Computational Linguistics, pp. 198–203.</li>

<li>Bastenhof, A. (2010). Tableaux for the Lambek-Grishin calculus. CoRR abs/1009.3238.
To appear in <em>Proceedings ESSLLI 2010 Student
Session</em>. Copenhagen.</li>

<li>Bernardi, R. and M. Moortgat (2010).  Continuation semantics for
the Lambek-Grishin calculus.
<em>Information and Computation</em>, 208(5): 394–416.</li>

<li>Bernardi, R. and A. Szabolcsi (2008).  Optionality, Scope, and
Licensing: An Application of Partially Ordered Categories.
<em>Journal of Logic, Language and Information</em>, 17(3):
237–283.</li>

<li>Bransen, J. (2010). The Lambek-Grishin calculus is NP-complete. To
appear in <em>Proceedings 15th Conference on Formal Grammar</em>,
Copenhagen. CoRR abs/1005.4697.</li>

<li>Buszkowski, W. (2001).  Lambek grammars based on pregroups.  In
P. de Groote, G. Morrill, and C. Retoré (Eds.), <em>Logical
Aspects of Computational Linguistics</em>, <em>Lecture Notes in
Artificial Intelligence</em> (Volume 2099), Berlin: Springer, pp.
95–109.</li>

<li>–––. (1997).  Mathematical linguistics and proof
theory.  In J. van Benthem and A. ter Meulen (Eds.), <em>Handbook of
Logic and Language</em> (Chapter 12), Amsterdam: Elsevier, and
Cambridge, MA: MIT Press, pp. 683–736.</li>

<li>Buszkowski, W. and G. Penn (1990).  Categorial grammars determined
from linguistic data by unification.
<em>Studia Logica</em>, 49(4): 431–454.</li>

<li>Buszkowski, W. and A. Preller (2007).  Editorial introduction
special issue on pregroup grammars.
<em>Studia Logica</em>, 87(2): 139–144.</li>

<li>Buszkowski, W., W. Marciszewski, and J. van Benthem (Eds.) (1988).
<em>Categorial Grammar</em>.  Amsterdam: John Benjamins.</li>

<li>Capelletti, M. (2007).
<em>Parsing with structure-preserving categorial grammars</em>.
Ph. D. thesis, Utrecht Institute of Linguistics OTS, Utrecht
University.</li>

<li>Carpenter, B. (1999).  The Turing-completeness of multimodal
categorial grammars.  In J. Gerbrandy, M. Marx, M. de Rijke, and
Y. Venema (Eds.), <em>JFAK. Essays Dedicated to Johan van Benthem on
the Occasion of his 50th Birthday</em>. Amsterdam: Amsterdam
University Press.</li>

<li>–––. (1998).
<em>Type-logical Semantics</em>.  Cambridge, MA: MIT Press.</li>

<li>Curry, H. B. (1961).  Some logical aspects of grammatical
structure.  In R. Jacobson (Ed.), <em>Structure of Language and its
Mathematical Aspects</em>,  <em>Proceedings of the
Symposia in Applied Mathematics</em> (Volume XII),  American
Mathematical Society, pp.  56–68.</li>

<li>de Groote, P. (2006).  Towards a Montagovian account of dynamics.
In <em>Proceedings SALT 16</em>. CLC Publications.</li>

<li>–––. (2001a).  Towards abstract categorial
grammars.  In <em>Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics</em>, Association
for Computational Linguistics, pp.  252–259.</li>

<li>–––. (2001b).  Type raising, continuations, and classical
logic.  In M. S. R. van Rooy (Ed.), <em>Proceedings of the Thirteenth
Amsterdam Colloquium</em>, Amsterdam: ILLC (Universiteit van
Amsterdam), pp.  97–101.</li>

<li>–––. (1999).  The non-associative Lambek
calculus with product in polynomial time.  In N. V. Murray
(Ed.), <em>Automated Reasoning With Analytic Tableaux and Related
Methods</em>, <em>Lecture Notes in Artificial Intelligence</em>
(Volume 1617), Berlin: Springer, pp.  128–139.</li>

<li>de Groote, P. and F. Lamarche (2002).  Classical non-associative
Lambek calculus.
<em>Studia Logica</em>, 71(3): 355–388.</li>

<li>de Groote, P. and S. Pogodalla (2004).  On the Expressive Power of
Abstract Categorial Grammars: Representing Context-Free Formalisms.
<em>Journal of Logic, Language and Information</em>, 13(4):
421–438.</li>

<li>de Groote, P. and C. Retoré (1996).  On the semantic
readings of proof nets.  In G.-J. Kruijff, G. Morrill, and D. Oehrle
(Eds.), <em>Proceedings 2nd Formal Grammar Conference</em>, Prague,
pp.  57–70.</li>

<li>Došen, K. (1992).  A brief survey of frames for the Lambek
calculus. <em>Mathematical Logic Quarterly</em>, 38(1): 179–187.</li>

<li>Galatos, N., P. Jipsen, T. Kowalski, and H. Ono (2007).
<em>Residuated Lattices: An Algebraic Glimpse at Substructural Logics,
Studies in Logic and the Foundations of Mathematics (Volume 151)</em>,
Amsterdam: Elsevier.</li>

<li>Girard, J.-Y. (1987).  Linear logic.
<em>Theoretical Computer Science</em>, 50: 1–102.</li>

<li>Grishin, V. (1983).  On a generalization of the Ajdukiewicz-Lambek
system.  In A. Mikhailov (Ed.), <em>Studies in Nonclassical Logics and
Formal Systems</em>, Moscow: Nauka, pp.  315–334.  [English
translation in Abrusci and Casadio (eds.) New Perspectives in Logic
and Formal Linguistics. Bulzoni, Rome, 2002].</li>

<li>Hendriks, H. (1993).
<em>Studied Flexibility. Categories and Types in Syntax and
Semantics</em>.  Ph. D. thesis, ILLC, University of Amsterdam.</li>

<li>Hepple, M. (1999).  An Earley-style predictive chart parsing
method for Lambek grammars.  In <em>Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics</em>,
Association for Computational Linguistics, pp.  465–472.</li>

<li>–––. (1990).  Normal form theorem proving for the Lambek
calculus.  In <em>Papers presented to the 13th International
Conference on Computational Linguistics</em>, Helsinki, pp.
173–178.</li>

<li>Hoyt, F. and J. Baldridge (2008).  A logical basis for the D
combinator and normal form in CCG.  In <em>Proceedings of ACL-08:
HLT</em>, Association for Computational Linguistics, pp.
326–334.</li>

<li>Jäger, G. (2005).
<em>Anaphora And Type Logical Grammar</em>.  Berlin: Springer.</li>

<li>–––. (2004). Residuation, Structural Rules and Context Freeness.
<em>Journal of Logic, Language and Information</em>, 13: 47–59.</li>

<li>Johnson, M. (1998).  Proof nets and the complexity of processing
center-embedded constructions.
<em>Journal of Logic, Language and Information</em>, 7(4):
433–447.</li>

<li>Joshi, A. K., K. Vijay-Shanker, and D. Weir (1991).  The
convergence of mildly context-sensitive grammar formalisms.  In
P. Sells, S. M. Shieber, and T. Wasow (Eds.), <em>Foundational Issues
in Natural Language Processing</em>, Cambridge, MA:
MIT Press,  pp.  31–81.</li>

<li>Kanazawa, M. (1998).
<em>Learnable classes of categorial grammars</em>.  Stanford: CSLI
Publications.</li>

<li>Kandulski, M. (1988).  The equivalence of nonassociative Lambek
categorial grammars and context-free grammars.
<em>Zeitschrift für mathematische Logik und Grundlagen der
Mathematik</em>, 34: 41–52.</li>

<li>Kanovich, M. (1994).  The Complexity of Horn Fragments of Linear
Logic. <em>Annals of Pure and Applied Logic</em>, 69(2-3): 195–241.</li>

<li>Kruijff, G.-J. and J. Baldridge (2003).  Multi-modal combinatory
categorial grammar.  In <em>Proceedings of the 10th Conference of the
European Chapter of the Association for Computational
Linguistics</em>, Association for Computational Linguistics, pp.
211–218.</li>

<li>Kurtonina, N. (1995).
<em>Frames and Labels. A Modal Analysis of Categorial Inference</em>.
Ph. D. thesis, OTS Utrecht, ILLC Amsterdam.</li>

<li>Kurtonina, N. and M. Moortgat (2010).  Relational semantics for
the Lambek-Grishin calculus.  In C. Ebert, G. Jäger, and
J. Michaelis (Eds.), <em>The Mathematics of Language. Proceedings of
the 10th and 11th Biennial Conference</em>, Lecture Notes in Computer
Science (Volume 6149). Berlin: Springer, pp. 210–222.</li>

<li>––– (1997).  Structural control.  In
P. Blackburn and M. de Rijke (Eds.), <em>Specifying Syntactic
Structures</em>,  Stanford: CSLI Publications, pp.  75–113.</li>

<li>Lambek, J. (2008).
<em>From word to sentence. A computational algebraic approach to
grammar</em>.  Polimetrica.</li>

<li>–––. (1999).  Type grammar revisited.  In
A. Lecomte, F. Lamarche, and G. Perrier (Eds.), <em>Logical Aspects of
Computational Linguistics</em>, <em>Lecture Notes in Artificial
Intelligence</em> (Volume 1582), Berlin: Springer, pp.
1–27.</li>

<li>–––. (1993).  From categorial to bilinear logic.  In
K. Došen and P. Schröder-Heister (Ed.), <em>Substructural Logics</em>.
Oxford University Press.</li>

<li>–––. (1961).  On the calculus of syntactic
types.  In R. Jacobson (Ed.), <em>Structure of Language and its
Mathematical Aspects</em>, <em>Proceedings of the Symposia in Applied
Mathematics</em> (Volume XII), American Mathematical Society, pp.
166–178.</li>

<li>–––. (1958).  The mathematics of sentence structure.
<em>American Mathematical Monthly</em>, 65: 154–170.</li>

<li>Melissen, M. (2009).  The generative capacity of the
Lambek-Grishin calculus: A new lower bound.  In P. de Groote
(Ed.), <em>Proceedings 14th Conference on Formal Grammar</em>, 
Lecture Notes in Computer Science (Volume 5591), Berlin: Springer.</li>

<li>Moortgat, M. (2009).  Symmetric categorial grammar.
<em>Journal of Philosophical Logic</em>, 8(6),
681–710.</li>

<li>–––. (1997).  Categorial type logics.  In J. van Benthem
and A. ter Meulen (Eds.), <em>Handbook of Logic and Language</em>
(Chapter 2),  Amsterdam: Elsevier, pp. 93–177.  (Second edition, revised and
updated: Elsevier Insights Series, 2010).</li>

<li>–––. (1996).  Multimodal linguistic inference.
<em>Journal of Logic, Language and Information</em>, 5(3–4):
349–385.</li>

<li>–––. (1988).
<em>Categorial Investigations. Logical and Linguistic Aspects of the
Lambek calculus</em>.  Berlin: De Gruyter.</li>

<li>Moot, R. (2008).  Lambek grammars, tree adjoining grammars and
hyperedge replacement grammars.  In <em>Proceedings of TAG+9, The 9th
International Workshop on Tree Adjoining Grammars and Related
Formalisms</em>, Tübingen, pp.  65–72.</li>

<li>–––. (2007).  Proof nets for display logic.
<em>CoRR</em>, abs/0711.2444.</li>

<li>–––. (2002).
<em>Proof Nets for Linguistic Analysis</em>.  Ph. D. thesis, Utrecht
Institute of Linguistics OTS, Utrecht University.</li>

<li>Moot, R. and M. Piazza (2001).  Linguistic Applications of First
Order Intuitionistic Linear Logic. <em>Journal of Logic, Language and
Information</em>, 10(2): 211–232.</li>

<li>Moot, R. and Q. Puite (2002).  Proof Nets for the Multimodal
Lambek Calculus.
<em>Studia Logica</em>, 71(3): 415–442.</li>

<li>Morrill, G. (2010).
<em>Categorial Grammar: Logical Syntax, Semantics, and
Processing</em>.  Oxford: Oxford University Press.</li>

<li>–––. (2000).  Incremental processing and acceptability.
<em>Computational linguistics</em>, 26(3): 319–338.</li>

<li>–––. (1994).
<em>Type Logical Grammar: Categorial Logic of Signs</em>.  Dordrecht:
Kluwer Academic Publishers.</li>

<li>–––. (1990).  Intensionality and boundedness.
<em>Linguistics and Philosophy</em>, 13(6): 699–726.</li>

<li>Morrill, G. and M. Fadda (2008).  Proof nets for basic
discontinuous Lambek calculus.
<em>Journal of Logic and Computation</em>, 18(2): 239–256.</li>

<li>Morrill, G., M. Fadda, and O. Valentin (2007).  Nondeterministic
discontinuous Lambek calculus.  In <em>Proceedings of the Seventh
International Workshop on Computational Semantics (IWCS7)</em>,
Tilburg.</li>

<li>Morrill, G., O. Valentin, and M. Fadda (2009).  Dutch grammar and
processing: A case study in TLG.  In P. Bosch, D. Gabelaia, and
J. Lang (eds.), <em>Logic, Language, and Computation: 7th
International Tbilisi Symposium on Logic, Language, and
Computation</em>, Tbilisi, Georgia, October 1-5, 2007 (Revised
Selected Papers), Lecture Notes in Artificial Intelligence (Volume
5422), Berlin: Springer, pp.  272–286.</li>

<li>Muskens, R. (2007).  Separating syntax and combinatorics in
categorial grammar. <em>Research on Language &amp;
Computation</em>, 5(3): 267–285.</li>

<li>Oehrle, R. T., E. Bach, and D. Wheeler (Eds.) (1988).
<em>Categorial Grammars and Natural Language Structures</em>, Studies
in Linguistics and Philosophy (Number 32). Dordrecht: Reidel.</li>

<li>Pentus, M. (1993b).  Lambek grammars are context free.
In <em>Proceedings of the 8th Annual IEEE</em> Symposium on Logic in
Computer Science},  IEEE Computer Society Press, pp.  429–433.</li>

<li>–––. (2006).  Lambek calculus is NP-complete.
<em>Theoretical Computer Science</em>, 357: 186–201.</li>

<li>–––. (1995).  Models for the Lambek calculus.
<em>Annals of Pure and Applied Logic</em>, 75(1–2),
179–213.</li>

<li>Restall, G. (2000). <em>An Introduction to Substructural
Logics</em>.  London: Routledge.</li>

<li>Retoré, C. and S. Salvati (2010).  A faithful
representation of non-associative Lambek grammars in Abstract
Categorial Grammars.
<em>Journal of Logic, Language and Information</em>, 19(2).  Special
issue on New Directions in Type Theoretic Grammars.</li>

<li>Roorda, D. (1992).  Proof Nets for Lambek calculus.
<em>Journal of Logic and Computation</em>, 2(2): 211–231.</li>

<li>Savateev, Y. (2009).  Product-free Lambek Calculus is NP-complete.
In S. Artemov and A. Nerode (Eds.), <em>Proceedings of the 2009
International Symposium on Logical Foundations of Computer
Science</em>, Lecture Notes in Computer Science (Volume 5407), Berlin: Springer, pp.
380–394.</li>

<li>Shan, C. and C. Barker (2006).  Explaining Crossover and
Superiority as Left-to-right Evaluation.
<em>Linguistics and Philosophy</em>, 29(1): 91–134.</li>

<li>Sørensen, M. H. and P. Urzyczyn (2006).
<em>Lectures on the Curry-Howard Isomorphism</em>,
<em>Studies in Logic and the Foundations of Mathematics</em> (Volume
149),  Amsterdam: Elsevier.</li>

<li>Stabler, E. (1999).  Remnant movement and complexity.  In
G. Bouma, E. Hinrichs, G.-J. Kruijff, and R. T. Oehrle (Eds.),
<em>Constraints and Resources in Natural Language Syntax and
Semantics</em>, Stanford: CSLI, pp.  299–326.</li>

<li>–––. (1997).  Derivational minimalism.  In
C. Retoré (Ed.), <em>Logical Aspects of Computational
Linguistics</em>, <em>Lecture Notes in Artificial Intelligence</em>
(Volume 1328), Berlin: Springer, pp.  68–95.</li>

<li>Steedman, M. (2000).
<em>The Syntactic Process</em>.  Cambridge, MA: MIT Press.</li>

<li>van Benthem, J. (1995).
<em>Language in Action: Categories, Lambdas and Dynamic Logic</em>.
Cambridge, MA: MIT Press.</li>

<li>–––. (1983).  The semantics of variety in categorial
grammar.  Technical Report 83-29, Simon Fraser University.  Revised
version in W. Buszkowski <em>et al</em>. (1988).</li>

<li>Vermaat, W. (2006).
<em>The logic of variation. A cross-linguistic account of wh-question
formation</em>.  Ph. D. thesis, Utrecht Institute of Linguistics OTS,
Utrecht University.</li>

<li>–––. (2004).  The minimalist move operation in a deductive
perspective.
<em>Research on Language &amp; Computation</em>, 2(1), 69–85.</li>

<li>Wansing, H. (2002).  Sequent systems for modal logics.  In
D. Gabbay and F. Guenthner (Eds.), <em>Handbook of Philosophical
Logic</em> (Volume 8), Dordrecht: Kluwer Academic Publishers, pp.
61–145.</li>

<li>–––. (1992).  Formulas-as-types for a hierarchy
of sublogics of intuitionistic propositional logic.  In D. Pearce and
H. Wansing (Eds.), <em>Nonclassical Logics and Information
Processing</em>, Lecture Notes in Computer Science (Volume 619),
Berlin: Springer, pp.  125–145.</li>

</ul>

</div>

<div id="academic-tools">

<h2><a id="Aca">Academic Tools</a></h2>

<blockquote>
<table>
<tbody><tr><td valign="top"><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=typelogical-grammar&amp;archive=win2019" target="other">How to cite this entry</a>.</td>
</tr>
<tr><td valign="top"><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/typelogical-grammar/" target="other">Preview the PDF version of this entry</a> at the <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>
<tr><td valign="top"><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=typelogical-grammar&amp;redirect=True" target="other">Look up this entry topic</a> at the <a href="https://www.inphoproject.org/" target="other">Indiana Philosophy Ontology Project</a> (InPhO).</td>
</tr>
<tr><td valign="top"><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="http://philpapers.org/sep/typelogical-grammar/" target="other">Enhanced bibliography for this entry</a> at <a href="http://philpapers.org" target="other">PhilPapers</a>, with links to its database.</td>
</tr>
</tbody></table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2><a name="Oth">Other Internet Resources</a></h2>

<ul>

<li><a href="http://www.labri.fr/perso/moot/grail3.html" target="other">Moot's typelogical theorem prover</a></li>

<li><a href="http://symcg.pbworks.com" target="other">Symmetric Categorial Grammar</a></li>

<!--LINK CHECKER COMMENTED OUT (Sun Jun 11 12:28:49 PDT 2017)

<li><a href="http://www.loria.fr/equipes/calligramme/acg/" target="other">Abstract Categorial Grammar</a></li>

LINK CHECKER-->

<li><a href="http://groups.inf.ed.ac.uk/ccg/" target="other">Combinatory Categorial Grammar</a></li>

</ul>

</div>

<div id="related-entries">

<h2><a name="Rel">Related Entries</a></h2>

<p>

 <a href="../compositionality/">compositionality</a> |
 <a href="../logic-linear/">logic: linear</a> |
 <a href="../logic-modal/">logic: modal</a> |
 <a href="../logic-relevance/">logic: relevance</a> |
 <a href="../logic-substructural/">logic: substructural</a> |
 <a href="../type-theory/">type theory</a>

</p>

</div>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright © 2010</a> by

<br />
Michael Moortgat
&lt;<a href="mailto:Michael%2eMoortgat%40phil%2euu%2enl"><em>Michael<abbr title=" dot ">.</abbr>Moortgat<abbr title=" at ">@</abbr>phil<abbr title=" dot ">.</abbr>uu<abbr title=" dot ">.</abbr>nl</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
     <div id="article-banner-content">
  <a href="../../fundraising/">
  Open access to the SEP is made possible by a world-wide funding initiative.<br />
  Please Read How You Can Help Keep the Encyclopedia Free</a>
 </div>


</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">New in this Archive</a></li>
            
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../../../archives/">Archives <i class="icon-external-link"></i></a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            
            <li><a href="../../../../contact.html">Contact <i class="icon-external-link"></i></a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../../../support/">Support the SEP</a></li>
            <li><a href="../../../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
                <div class="btn-group">
<a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/"><span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span><span class="mirror-source">CSLI, Stanford University</span></a>          <ul class="dropdown-menu">
            <li><a href="https://stanford.library.sydney.edu.au/archives/win2019/entries/typelogical-grammar/"><span class="flag flag-australia"></span> Australia <span class="mirror-source">Library, University of Sydney</span></a>           </li>
            <li><a href="https://seop.illc.uva.nl/archives/win2019/entries/typelogical-grammar/"><span class="flag flag-netherlands"></span> Netherlands <span class="mirror-source">ILLC, University of Amsterdam</span></a>           </li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p class="csli-logo"><a href="https://www-csli.stanford.edu/"><img src="../../symbols/SU_csli.png" width="355" alt="Stanford Center for the Study of Language and Information" /></a></p>
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright © 2016</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Center for the Study of Language and Information (CSLI), Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>





</body></html>
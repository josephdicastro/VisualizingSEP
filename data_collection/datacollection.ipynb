{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd \n",
    "import time\n",
    "import pymongo\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from  tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#import local libraries\n",
    "import functions_mongo as mdb\n",
    "import funtions_webscraping as web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INIT GLOBAL VARIABLES#####\n",
    "\n",
    "#init Mongo\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "#create database\n",
    "db = client.visualizing_sep\n",
    "\n",
    "#create collection to hold SEP TOC\n",
    "toc_collection = db.sep_win2019_toc\n",
    "# entries_collection = db.all_entries\n",
    "entries_collection = db.all_entries_updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRAPE TOC FROM SEP AND SAVE TO MONGO ###\n",
    "\n",
    "print(\"Starting: SEP TOC Scrape\\n\")\n",
    "\n",
    "#TOC to scrape\n",
    "search_url = \"https://plato.stanford.edu/archives/win2019/contents.html\"\n",
    "\n",
    "#url to generate absolute paths from\n",
    "base_url = \"https://plato.stanford.edu/archives/win2019/\"\n",
    "\n",
    "#current TOC edition\n",
    "toc_edition = \"Winter 2019\"\n",
    "\n",
    "#hold all individual TOC entries \n",
    "toc_list = []\n",
    "\n",
    "#scrape selected webpage and save HTML file\n",
    "soup = web.scrape_web_page(search_url, \"html_files/win2019_toc.html\")\n",
    "\n",
    "#get all links with an HREF attribute in the content page\n",
    "links_in_toc = soup.find(id=\"content\").findAll(\"a\", href=True)\n",
    "\n",
    "#we will write to these two texts files during our TOC scrape. \n",
    "#file_tocList logs the object names and links to be stored in mongo\n",
    "#file_links logs each link that BeautifulSoup identified as valid\n",
    "file_toclist = open(\"html_files/sep_toc_list.txt\",\"w\",encoding=\"UTF-8\")\n",
    "file_links = open(\"html_files/sep_toc_links.txt\", \"w\", encoding=\"UTF-8\")\n",
    "\n",
    "for link in links_in_toc:\n",
    "    link_text = link.get_text()\n",
    "    link_href = link[\"href\"]\n",
    "\n",
    "    #some links are internal links to other parts of the TOC page, so we exclude those from scraping\n",
    "    if link_href != '#pagetopright':\n",
    "        #create dictionary to append to mongo\n",
    "        entry = {\"base_url\":base_url, \"link_url\":link_href}\n",
    "\n",
    "        #some entries are duplicated in the TOC, so we only store unique links.\n",
    "        if entry not in toc_list:\n",
    "            toc_list.append(entry)\n",
    "            #log results\n",
    "            file_toclist.write(f\"{link_text}:{link_href}\\n\")\n",
    "            file_links.write(f\"{link}\\n\")\n",
    "\n",
    "# #insert into Mongo\n",
    "toc_collection.insert_many(toc_list)\n",
    "\n",
    "\n",
    "print(\"Completed: SEP TOC Scrape\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOWNLOAD ALL PAGES FROM SEP, AND STORE THEM LOCALLY###\n",
    "# Completed 1/30/2020. This took 7 hours, so don't do it again!\n",
    "\n",
    "#get all page links stored in mongo\n",
    "pages = toc_collection.find().sort('link_url')\n",
    "\n",
    "#cast mongo cursor into python list for iteration.\n",
    "pages_list = list(pages)\n",
    "\n",
    "#download all pages from TOC\n",
    "for page in tqdm(pages_list, desc=\"Processing\"):\n",
    "\n",
    "    base_url = page['base_url']\n",
    "    link_url = page['link_url']\n",
    "\n",
    "    #absolute path to the file to download from SEP\n",
    "    page_url = f\"{base_url}{link_url}\"\n",
    "    \n",
    "    #link_urls are all stored with the following structure: \"entries/<pagename>.html\"\n",
    "    #We want to remove \"entires/\" remove all paths from page name, and \n",
    "    # store the file locally with just the individual page name\n",
    "    slash_pos = link_url.rfind(\"/\",0,len(link_url)-1)\n",
    "    page_name = link_url[slash_pos+1:len(link_url)-1]\n",
    "\n",
    "    #local path to save file into \n",
    "    html_save_as = f\"html_files/sep/{page_name}.html\"\n",
    "\n",
    "    try:\n",
    "        #this is commented out, so that this notebook cell isn't accidently activated \n",
    "        #and local files overwritten\n",
    "        #soup = web.scrape_web_page(page_url, html_save_as)\n",
    "        \n",
    "    except:\n",
    "        print(f\"Scraped failed @ {page_name}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Processing:: 100%|██████████| 3/3 [00:00<00:00, 23.87it/s]\n[{'copyright':<p>\n<a href=\"../../info.html#c\">Copyright © 2014</a> by\n\n<br/>\nBrigitte Sassen\n&lt;<a href=\"mailto:sassenb%40mcmaster%2eca\"><em>sassenb<abbr title=\" at \">@</abbr>mcmaster<abbr title=\" dot \">.</abbr>ca</em></a>&gt;\n    </p>,\n'first_paragraph':'  In Germany, the eighteenth century was the age of '\n'enlightenment, the age, that is, that called for the '\n'independence of reason. Although the ethos of this age '\n'found its clearest (and certainly its most famous) '\n'articulation towards the end of the century with '\n'Immanuel Kant and his critical philosophy, he was not '\n'the first to issue this call. Instead, that task fell to '\n'Christian Thomasius (Thomas) at the end of the '\n'seventeenth century. It was then taken up and further '\n'developed in a theological (pietist) direction by a '\n'number of minor figures, the Thomasians, and reissued in '\n'a rationalist direction in the early and middle part of '\n'the eighteenth century by Christian Wolff and his '\n'followers. The development of their position(s) as well '\n'as their philosophical (dis)agreements took place by and '\n'large at the University of Halle and against the context '\n'of pietism.',\n'inpho_link':'https://www.inphoproject.org/entity?sep=18thGerman-preKant&redirect=True',\n'page_url':'/entries/18thGerman-preKant/',\n'pubdate':'First published Sun Mar 10, 2002; substantive revision Mon Jul '\n'28, 2014',\n'related':[{'link': '/entries/hobbes/', 'text': 'Hobbes, Thomas'},\n{'link': '/entries/kant/', 'text': 'Kant, Immanuel'},\n{'link':'/entries/leibniz/',\n'text':'Leibniz, Gottfried Wilhelm'},\n{'link': '/entries/locke/', 'text': 'Locke, John'}],\n'title':'18th Century German Philosophy Prior to Kant'},\n{'copyright':<p>\n<a href=\"../../info.html#c\">Copyright © 2017</a> by\n\n<br/>\nIgor Douven\n&lt;<a href=\"mailto:igor%2edouven%40paris-sorbonne%2efr\"><em>igor<abbr title=\" dot \">.</abbr>douven<abbr title=\" at \">@</abbr>paris-sorbonne<abbr title=\" dot \">.</abbr>fr</em></a>&gt;\n    </p>,\n'first_paragraph':' In the philosophical literature, the term “abduction” '\n'is used in two related but different senses. In both '\n'senses, the term refers to some form of explanatory '\n'reasoning. However, in the historically first sense, it '\n'refers to the place of explanatory reasoning in '\n'generating hypotheses, while in the sense in which it is '\n'used most frequently in the modern literature it refers '\n'to the place of explanatory reasoning in justifying '\n'hypotheses. In the latter sense, abduction is also often '\n'called “Inference to the Best Explanation.”',\n'inpho_link':'https://www.inphoproject.org/entity?sep=abduction&redirect=True',\n'page_url':'/entries/abduction/',\n'pubdate':'First published Wed Mar 9, 2011; substantive revision Fri Apr '\n'28, 2017',\n'related':[{'link':'/entries/epistemology-bayesian/',\n'text':'epistemology: Bayesian'},\n{'link':'/entries/induction-problem/',\n'text':'induction: problem of'},\n{'link': '/entries/peirce/', 'text': 'Peirce, Charles Sanders'},\n{'link':'/entries/scientific-explanation/',\n'text':'scientific explanation'},\n{'link':'/entries/scientific-realism/',\n'text':'scientific realism'},\n{'link': '/entries/simplicity/', 'text': 'simplicity'},\n{'link': '/entries/skepticism/', 'text': 'skepticism'},\n{'link':'/entries/scientific-underdetermination/',\n'text':'underdetermination, of scientific theories'}],\n'title':'Abduction'},\n{'copyright':<p>\n<a href=\"../../info.html#c\">Copyright © 2018</a> by\n\n<br/>\n<a href=\"http://individual.utoronto.ca/pking/\" target=\"other\">Peter King</a>\n<br/>\n<a href=\"http://www.brooklyn.cuny.edu/web/academics/faculty/faculty_profile.jsp?faculty=641\" target=\"other\">Andrew Arlig</a>\n&lt;<a href=\"mailto:aarlig%40brooklyn%2ecuny%2eedu\"><em>aarlig<abbr title=\" at \">@</abbr>brooklyn<abbr title=\" dot \">.</abbr>cuny<abbr title=\" dot \">.</abbr>edu</em></a>&gt;\n    </p>,\n'first_paragraph':' Peter Abelard (1079–21 April 1142) [‘Abailard’ or '\n'‘Abaelard’ or ‘Habalaarz’ and so on] was the pre-eminent '\n'philosopher and theologian of the twelfth century. The '\n'teacher of his generation, he was also famous as a poet '\n'and a musician. Prior to the recovery of Aristotle, he '\n'brought the native Latin tradition in philosophy to its '\n'highest pitch. His genius was evident in all he did. He '\n'is, arguably, the greatest logician of the Middle Ages '\n'and is equally famous as the first great nominalist '\n'philosopher. He championed the use of reason in matters '\n'of faith (he was the first to use ‘theology’ in its '\n'modern sense), and his systematic treatment of religious '\n'doctrines are as remarkable for their philosophical '\n'penetration and subtlety as they are for their audacity. '\n'Abelard seemed larger than life to his contemporaries: '\n'his quick wit, sharp tongue, perfect memory, and '\n'boundless arrogance made him unbeatable in debate—he was '\n'said by supporter and detractor alike never to have lost '\n'an argument—and the force of his personality impressed '\n'itself vividly on all with whom he came into contact. '\n'His luckless affair with Héloïse made him a tragic '\n'figure of romance, and his conflict with Bernard of '\n'Clairvaux over reason and religion made him the hero of '\n'the Enlightenment. For all his colourful life, though, '\n'his philosophical achievements are the cornerstone of '\n'his fame.',\n'inpho_link':'https://www.inphoproject.org/entity?sep=abelard&redirect=True',\n'page_url':'/entries/abelard/',\n'pubdate':'First published Tue Aug 3, 2004; substantive revision Wed Aug 8, '\n'2018',\n'related':[{'link':'/entries/aristotle-logic/',\n'text':'Aristotle, General Topics: logic'},\n{'link':'/entries/mereology-medieval/',\n'text':'mereology: medieval'},\n{'link':'/entries/relations-medieval/',\n'text':'relations: medieval theories of'},\n{'link':'/entries/medieval-syllogism/',\n'text':'syllogism: medieval theories of'},\n{'link':'/entries/universals-medieval/',\n'text':'universals: the medieval problem of'},\n{'link':'/entries/william-champeaux/',\n'text':'William of Champeaux'}],\n'title':'Peter Abelard'}]\n"
    }
   ],
   "source": [
    "### PARSING LOCAL SEP FILES ###\n",
    "\n",
    "sep_searchpath = Path.cwd() / 'html_files/sep'\n",
    "sep_files = list(Path(sep_searchpath).rglob('*.html'))\n",
    "sep_files = sep_files[0:3]\n",
    "\n",
    "#list to hold all page objects\n",
    "page_data = []\n",
    "\n",
    "for sep_file in tqdm(sep_files, \"Processing:\"):\n",
    "    file_name = sep_file.name\n",
    "    current_page = \"/entries/\" + file_name.replace(\".html\",\"\").strip() + \"/\"\n",
    "    file_to_read = open(str(sep_file),'r', encoding='UTF-8').read()\n",
    "    \n",
    "    soup = BeautifulSoup(file_to_read, 'lxml')\n",
    "\n",
    "    title = soup.find(id=\"aueditable\").find(\"h1\").get_text()\n",
    "    pubdate = soup.find(id=\"pubinfo\").get_text()\n",
    "    inpho = soup.find(href=re.compile('^https://www.inphoproject.org/'))[\"href\"]\n",
    "    related_entries = soup.find(id=\"related-entries\").find_all(\"a\", href=True)\n",
    "    copyright_html = soup.find(id=\"article-copyright\").find(\"p\")\n",
    "\n",
    "    first_paragraph = soup.find(id=\"preamble\").find(\"p\").get_text()\n",
    "    first_paragraph = first_paragraph.replace('\\n',' ').replace('\\r', ' ')\n",
    "\n",
    "    #list to hold related links from page\n",
    "    related_pages = []\n",
    "    for entry in related_entries:\n",
    "        sep_link = entry[\"href\"].replace(\"..\",\"/entries\").strip()\n",
    "        sep_text = entry.get_text()\n",
    "        rel = { \"link\": sep_link, \"text\": sep_text}\n",
    "        \n",
    "        related_pages.append(rel)\n",
    "\n",
    "\n",
    "    page_object = { \"page_url\": current_page,\n",
    "                    \"title\": title,\n",
    "                    \"pubdate\": pubdate,\n",
    "                    \"inpho_link\":inpho,\n",
    "                    \"related\":related_pages,\n",
    "                    \"copyright\":copyright_html,\n",
    "                    \"first_paragraph\":first_paragraph}\n",
    "    \n",
    "    page_data.append(page_object)\n",
    "\n",
    "#this line got deleted somehow, so it must be tested before its run again\n",
    "# entries_collection.insert_many(page_data)\n",
    "pprint(page_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Processing:: 100%|██████████| 1671/1671 [12:17<00:00,  2.27it/s]\n"
    }
   ],
   "source": [
    "### get InPhO API URL from the SEP InPhO URL field ###\n",
    "# Completed 2/12/2020. \n",
    "\n",
    "#get all entries stored in mongo\n",
    "sep_entries = list(entries_collection.find().sort('page_url'))\n",
    "\n",
    "#loop through sep_entries\n",
    "for entry in tqdm(sep_entries, desc=\"Processing:\"):\n",
    "    #get mongoDB ID\n",
    "    entry_id = entry[\"_id\"]\n",
    "    #get InPhO URL from entry \n",
    "    inpho_url = entry[\"inpho_link\"]\n",
    "    #open request from InPhO URL and convert mongo URL into proper API call\n",
    "    r = requests.get(inpho_url)\n",
    "    api_url = r.url\n",
    "    api_url = api_url.replace(\"https://www.inphoproject.org/\",\\\n",
    "                              \"https://inpho.cogs.indiana.edu/\")\\\n",
    "                     .replace(\"html\",\"json\")\n",
    "    #update mongo\n",
    "    entries_collection.update_one({\"_id\":entry_id},{\"$set\": {\"inpho_api\": api_url}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Processing:: 100%|██████████| 95/95 [00:00<00:00, 1642.07it/s]\n"
    }
   ],
   "source": [
    "### Update all Taxonomy InPhO API links with proper JSON formatting ###\n",
    "# Completed 2/12/2020. \n",
    "\n",
    "#find all InPhO API URLS that are a taxonomy\n",
    "taxonomy_entries = list(entries_collection.find({'inpho_api': {\"$regex\":'taxo'}}))\n",
    "\n",
    "#loop through entries\n",
    "for entry in tqdm(taxonomy_entries, desc=\"Processing:\"):\n",
    "    #get mongoDB ID\n",
    "    entry_id = entry[\"_id\"]\n",
    "    #add .json file handler to each entry\n",
    "    inpho_api = f\"{entry['inpho_api']}.json\"\n",
    "    #update Mongo\n",
    "    entries_collection.update_one({\"_id\":entry_id},{\"$set\": {\"inpho_api\": inpho_api}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "https://inpho.cogs.indiana.edu/taxonomy/2355.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2265.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2444.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2203.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2398.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2392.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2221.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2302.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2372.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2296.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2284.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2212.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2241.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2417.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2301.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2207.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2185.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2434.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2341.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2292.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2199.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2300.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2422.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2378.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2379.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2389.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2374.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2369.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2220.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2219.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2357.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2432.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2368.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2246.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2443.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2214.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2198.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2419.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2440.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2435.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2201.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2217.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2394.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2423.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2288.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2332.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2287.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2416.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2406.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2418.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2410.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2407.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2186.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2409.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2236.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2408.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2299.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2309.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2316.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2393.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2248.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2350.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2264.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2438.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2358.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2401.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2189.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2436.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2457.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2425.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2215.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2283.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2437.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2234.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2415.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2354.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2320.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2297.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2227.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2187.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2279.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2360.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2383.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2442.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2445.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2452.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2448.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2286.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2333.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2412.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2391.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2213.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2223.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2240.json\nhttps://inpho.cogs.indiana.edu/taxonomy/2413.json\n"
    }
   ],
   "source": [
    "taxonomy_entries = list(entries_collection.find({'inpho_api': {\"$regex\":'taxo'}}))\n",
    "for entry in taxonomy_entries:\n",
    "    print(entry[\"inpho_api\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Processing: 100%|██████████| 1671/1671 [00:00<00:00, 186490.39it/s]\n"
    }
   ],
   "source": [
    "### CREATE JSON FROM MONGODB ###\n",
    "# First Completed 2/10/2020. \n",
    "# This code has been updated to get the number of links and the type of article\n",
    "\n",
    "#get all entries stored in mongo\n",
    "sep_entries = list(entries_collection.find().sort('title'))\n",
    "\n",
    "#init empty lists \n",
    "nodes_list = []\n",
    "links_list = []\n",
    "\n",
    "#loop through all entries\n",
    "for entry in tqdm(sep_entries, desc=\"Processing\"):\n",
    "    if \"thinker\" in entry[\"inpho_api\"]:\n",
    "        entry_type = \"thinker\"\n",
    "    else:\n",
    "        entry_type = \"idea\"\n",
    "\n",
    "    node_object = { \n",
    "        \"id\": entry[\"page_url\"], \n",
    "        \"title\": entry[\"title\"],\n",
    "        \"num_links\": len(entry[\"related\"]),\n",
    "        \"entry_type\": entry_type\n",
    "        }\n",
    "\n",
    "    nodes_list.append(node_object)\n",
    "\n",
    "    for link in entry[\"related\"]:\n",
    "        doc = entries_collection.find({\"page_\"})\n",
    "        links_list.append({\"source\":entry[\"page_url\"], \"target\":link[\"link\"]})\n",
    "\n",
    "network_object = {\"nodes\": nodes_list, \"links\":links_list}\n",
    "\n",
    "with open('network1.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(network_object,f,ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Processing: 100%|██████████| 1671/1671 [00:06<00:00, 263.81it/s]\n"
    }
   ],
   "source": [
    "### CREATE JSON of thinkers FROM MONGODB ###\n",
    "# First Completed 2/10/2020. \n",
    "# This code has been updated to get the number of links and the type of article\n",
    "\n",
    "#get all entries stored in mongo\n",
    "sep_entries = list(entries_collection.find().sort('title'))\n",
    "\n",
    "#init empty lists \n",
    "nodes_list = []\n",
    "links_list = []\n",
    "\n",
    "#loop through all entries\n",
    "for entry in tqdm(sep_entries, desc=\"Processing\"):\n",
    "    if \"thinker\" in entry[\"inpho_api\"]:\n",
    "\n",
    "        node_object = { \n",
    "            \"id\": entry[\"page_url\"], \n",
    "            \"title\": entry[\"title\"],\n",
    "            \"num_links\": len(entry[\"related\"]),\n",
    "            \"entry_type\": entry[\"inpho_api\"]\n",
    "            }\n",
    "\n",
    "        nodes_list.append(node_object)\n",
    "\n",
    "        for link in entry[\"related\"]:\n",
    "            if entries_collection.find_one({ \"$and\": [{'page_url':link[\"link\"]},\n",
    "                                       {'inpho_api': {'$regex':'thinker'}}]}):  \n",
    "                links_list.append({\"source\":entry[\"page_url\"], \"target\":link[\"link\"]})\n",
    "\n",
    "network_object = {\"nodes\": nodes_list, \"links\":links_list}\n",
    "\n",
    "with open('network_thinkers.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(network_object,f,ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "yes\n"
    }
   ],
   "source": [
    "if entries_collection.find_one({ \"$and\": [{'page_url':'/entries/bodin/'},\n",
    "                                       {'inpho_api': {'$regex':'thinker'}}]}):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}